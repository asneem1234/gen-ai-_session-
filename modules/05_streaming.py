"""
05 - Streaming Concepts
=======================

This module demonstrates streaming responses from the AI model.
Students will learn:
- What is streaming and why it matters
- Implementing streaming responses
- Real-time token generation
- User experience improvements
- Handling streaming errors

Teaching Points:
- Streaming provides immediate feedback to users
- Better UX for long responses
- Token-by-token generation
- Essential for chat applications
- Reduces perceived latency
"""

import os
from dotenv import load_dotenv
import google.generativeai as genai
import time
import sys

# Setup
load_dotenv()
genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))


# ============================================================================
# SECTION 1: Understanding Streaming
# ============================================================================

def streaming_concepts():
    """
    Explain what streaming is and why it's important
    """
    print("\n" + "=" * 60)
    print("SECTION 1: Understanding Streaming")
    print("=" * 60)
    
    explanation = """
    üåä WHAT IS STREAMING?
    
    NON-STREAMING (Traditional):
    ----------------------------
    1. User sends prompt
    2. AI processes entire response
    3. User waits...
    4. Complete response appears all at once
    
    ‚è±Ô∏è Problem: Long wait time before seeing anything
    
    STREAMING:
    ----------
    1. User sends prompt
    2. AI starts generating
    3. Tokens appear in real-time as generated
    4. User sees response building up
    
    ‚úÖ Benefit: Immediate feedback, better UX
    
    
    üìä COMPARISON:
    
    Prompt: "Write a 500-word essay about AI"
    
    Non-Streaming:
    ‚Ä¢ User waits 15 seconds
    ‚Ä¢ Entire essay appears at once
    ‚Ä¢ Perception: "Is it working?"
    
    Streaming:
    ‚Ä¢ Text starts appearing after 0.5 seconds
    ‚Ä¢ Words flow continuously
    ‚Ä¢ Perception: "It's working! I can start reading!"
    
    
    üí° WHEN TO USE STREAMING:
    
    ‚úÖ Use streaming for:
       ‚Ä¢ Interactive chat applications
       ‚Ä¢ Long-form content generation
       ‚Ä¢ Real-time user interfaces
       ‚Ä¢ Progressive web apps
    
    ‚ùå Don't need streaming for:
       ‚Ä¢ Batch processing
       ‚Ä¢ Background jobs
       ‚Ä¢ Very short responses
       ‚Ä¢ Data processing pipelines
    
    
    ‚öôÔ∏è HOW IT WORKS:
    
    1. Instead of: response = model.generate_content(prompt)
    2. Use: response = model.generate_content(prompt, stream=True)
    3. Iterate: for chunk in response: print(chunk.text)
    """
    
    print(explanation)


# ============================================================================
# SECTION 2: Basic Streaming Example
# ============================================================================

def basic_streaming_example():
    """
    Simple streaming demonstration
    """
    print("\n" + "=" * 60)
    print("SECTION 2: Basic Streaming Example")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    prompt = "Write a short paragraph about the benefits of renewable energy."
    
    print(f"\nüìù Prompt: {prompt}\n")
    print("ü§ñ AI Response (streaming):")
    print("-" * 60)
    
    # Enable streaming with stream=True
    response = model.generate_content(prompt, stream=True)
    
    # Iterate through chunks as they arrive
    for chunk in response:
        print(chunk.text, end='', flush=True)
    
    print("\n" + "-" * 60)
    print("‚úÖ Streaming complete!")


# ============================================================================
# SECTION 3: Streaming vs Non-Streaming Comparison
# ============================================================================

def compare_streaming_vs_non_streaming():
    """
    Side-by-side comparison of both approaches
    """
    print("\n" + "=" * 60)
    print("SECTION 3: Streaming vs Non-Streaming Comparison")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    prompt = "Explain quantum computing in simple terms. Keep it brief."
    
    # Non-Streaming
    print("\n1Ô∏è‚É£ NON-STREAMING Approach:")
    print("-" * 60)
    print("‚è≥ Waiting for complete response...\n")
    
    start_time = time.time()
    response = model.generate_content(prompt, stream=False)
    end_time = time.time()
    
    print("ü§ñ Response:")
    print(response.text)
    print(f"\n‚è±Ô∏è  Time to first output: {end_time - start_time:.2f} seconds")
    print("üìä User experience: Wait ‚Üí Complete text appears")
    
    # Streaming
    print("\n\n2Ô∏è‚É£ STREAMING Approach:")
    print("-" * 60)
    print("ü§ñ Response:\n")
    
    start_time = time.time()
    first_chunk_time = None
    
    response_stream = model.generate_content(prompt, stream=True)
    
    for i, chunk in enumerate(response_stream):
        if i == 0:
            first_chunk_time = time.time() - start_time
        print(chunk.text, end='', flush=True)
        time.sleep(0.05)  # Simulate reading time for demo
    
    end_time = time.time()
    
    print(f"\n\n‚è±Ô∏è  Time to first output: {first_chunk_time:.2f} seconds")
    print(f"‚è±Ô∏è  Total time: {end_time - start_time:.2f} seconds")
    print("üìä User experience: Immediate start ‚Üí Progressive display")


# ============================================================================
# SECTION 4: Token-by-Token Streaming
# ============================================================================

def token_by_token_streaming():
    """
    Demonstrate granular token streaming
    """
    print("\n" + "=" * 60)
    print("SECTION 4: Token-by-Token Streaming")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    prompt = "List 5 programming languages and one-word descriptions."
    
    print(f"\nüìù Prompt: {prompt}\n")
    print("ü§ñ Streaming Response:")
    print("-" * 60)
    
    # Track chunks
    chunk_count = 0
    total_text = ""
    
    response = model.generate_content(prompt, stream=True)
    
    for chunk in response:
        chunk_count += 1
        chunk_text = chunk.text
        total_text += chunk_text
        
        # Display with visual indicator
        print(chunk_text, end='', flush=True)
        
        # Small delay to visualize streaming
        time.sleep(0.02)
    
    print("\n" + "-" * 60)
    print(f"üìä Statistics:")
    print(f"   ‚Ä¢ Total chunks: {chunk_count}")
    print(f"   ‚Ä¢ Total characters: {len(total_text)}")
    print(f"   ‚Ä¢ Average chunk size: {len(total_text)/chunk_count:.1f} chars")


# ============================================================================
# SECTION 5: Interactive Streaming Chat
# ============================================================================

def interactive_streaming_chat():
    """
    Interactive chat with streaming responses
    """
    print("\n" + "=" * 60)
    print("SECTION 5: Interactive Streaming Chat")
    print("=" * 60)
    print("\nType your messages and see streaming responses!")
    print("Type 'quit' to exit\n")
    
    model = genai.GenerativeModel('gemini-pro')
    
    conversation_count = 0
    
    while True:
        user_input = input("You: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("üëã Goodbye!")
            break
            
        if not user_input:
            continue
        
        conversation_count += 1
        
        print("ü§ñ AI: ", end='', flush=True)
        
        try:
            # Stream the response
            response = model.generate_content(user_input, stream=True)
            
            for chunk in response:
                print(chunk.text, end='', flush=True)
            
            print("\n")
            
            if conversation_count == 1:
                print("üí° Notice how the response appears word-by-word!\n")
                
        except Exception as e:
            print(f"\n‚ùå Error: {e}\n")


# ============================================================================
# SECTION 6: Progress Indicators with Streaming
# ============================================================================

def streaming_with_progress():
    """
    Streaming with visual progress indicators
    """
    print("\n" + "=" * 60)
    print("SECTION 6: Streaming with Progress Indicators")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    prompt = "Write a three-paragraph story about a robot learning to paint."
    
    print(f"\nüìù Prompt: {prompt}\n")
    print("ü§ñ Generating story with progress indicator:")
    print("=" * 60)
    
    # Method 1: Character counter
    print("\n[Method 1: Character Counter]\n")
    
    char_count = 0
    response = model.generate_content(prompt, stream=True)
    
    for chunk in response:
        chunk_text = chunk.text
        print(chunk_text, end='', flush=True)
        char_count += len(chunk_text)
        
        # Update counter in status line (would use different approach in GUI)
        if char_count % 50 == 0:
            sys.stdout.write(f' [{char_count} chars]')
            sys.stdout.flush()
    
    print(f"\n\n‚úÖ Complete! Total: {char_count} characters")
    
    # Method 2: Typing effect
    print("\n" + "=" * 60)
    print("[Method 2: Typing Effect]\n")
    
    prompt2 = "Describe a sunset in 2 sentences."
    response2 = model.generate_content(prompt2, stream=True)
    
    for chunk in response2:
        for char in chunk.text:
            print(char, end='', flush=True)
            time.sleep(0.03)  # Typing effect
    
    print("\n")


# ============================================================================
# SECTION 7: Error Handling in Streaming
# ============================================================================

def streaming_error_handling():
    """
    Properly handle errors in streaming responses
    """
    print("\n" + "=" * 60)
    print("SECTION 7: Error Handling in Streaming")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    print("\n‚úÖ BEST PRACTICE: Always use try-except with streaming\n")
    
    code_example = """
def safe_streaming_response(model, prompt):
    try:
        response = model.generate_content(prompt, stream=True)
        
        accumulated_text = ""
        
        for chunk in response:
            try:
                text = chunk.text
                accumulated_text += text
                print(text, end='', flush=True)
                
            except ValueError as e:
                # Handle blocked content or safety filters
                print(f"\\n‚ö†Ô∏è  Content blocked: {e}")
                break
                
            except Exception as e:
                # Handle other chunk-level errors
                print(f"\\n‚ùå Chunk error: {e}")
                continue
        
        return accumulated_text
        
    except Exception as e:
        print(f"‚ùå Stream error: {e}")
        return None
"""
    
    print("CODE EXAMPLE:")
    print("-" * 60)
    print(code_example)
    
    # Demonstrate safe implementation
    print("\n\nDEMONSTRATION:")
    print("-" * 60)
    
    def safe_streaming_response(model, prompt):
        try:
            response = model.generate_content(prompt, stream=True)
            
            accumulated_text = ""
            
            for chunk in response:
                try:
                    text = chunk.text
                    accumulated_text += text
                    print(text, end='', flush=True)
                    
                except ValueError as e:
                    print(f"\n‚ö†Ô∏è  Content blocked: {e}")
                    break
                    
                except Exception as e:
                    print(f"\n‚ùå Chunk error: {e}")
                    continue
            
            return accumulated_text
            
        except Exception as e:
            print(f"‚ùå Stream error: {e}")
            return None
    
    test_prompt = "Explain why error handling is important in production code."
    print(f"\nPrompt: {test_prompt}\n")
    safe_streaming_response(model, test_prompt)
    print("\n")


# ============================================================================
# SECTION 8: Practical Streaming Applications
# ============================================================================

def practical_streaming_applications():
    """
    Real-world uses of streaming
    """
    print("\n" + "=" * 60)
    print("SECTION 8: Practical Streaming Applications")
    print("=" * 60)
    
    applications = """
    üöÄ REAL-WORLD USE CASES:
    
    1. üí¨ CHAT APPLICATIONS
       ‚Ä¢ Instant messaging feel
       ‚Ä¢ Better than "typing..." indicators
       ‚Ä¢ User can start reading while AI generates
       
       Implementation:
       - WebSocket for real-time communication
       - Send chunks as they arrive
       - Update UI incrementally
    
    2. üìù CONTENT GENERATION TOOLS
       ‚Ä¢ Blog post writers
       ‚Ä¢ Code generators
       ‚Ä¢ Email composers
       
       Benefit:
       - Users can edit early parts while rest generates
       - Faster perceived performance
       - Can interrupt if going wrong direction
    
    3. üéì EDUCATIONAL PLATFORMS
       ‚Ä¢ Tutoring chatbots
       ‚Ä¢ Interactive learning
       ‚Ä¢ Real-time explanations
       
       Benefit:
       - More engaging for students
       - Natural conversation flow
       - Immediate feedback
    
    4. üõ†Ô∏è CODE ASSISTANTS
       ‚Ä¢ IDE integrations
       ‚Ä¢ Code completion
       ‚Ä¢ Debugging help
       
       Benefit:
       - See code as it's generated
       - Can accept/reject parts
       - Faster development workflow
    
    5. üìû CUSTOMER SUPPORT
       ‚Ä¢ Chatbots
       ‚Ä¢ FAQ assistants
       ‚Ä¢ Ticket resolution
       
       Benefit:
       - Natural conversation
       - Reduces waiting anxiety
       - Professional appearance
    
    
    üíª IMPLEMENTATION PATTERNS:
    
    Pattern 1: Web Application
    --------------------------
    Backend (Python):
        for chunk in response:
            yield f"data: {chunk.text}\\n\\n"
    
    Frontend (JavaScript):
        const eventSource = new EventSource('/stream');
        eventSource.onmessage = (event) => {
            displayText += event.data;
            updateUI(displayText);
        };
    
    Pattern 2: Desktop App
    ----------------------
    Use threading:
        def stream_response(prompt, callback):
            response = model.generate_content(prompt, stream=True)
            for chunk in response:
                callback(chunk.text)
    
    Pattern 3: Mobile App
    ---------------------
    Use async/await:
        async def stream_to_ui(prompt):
            response = model.generate_content(prompt, stream=True)
            for chunk in response:
                await update_ui(chunk.text)
    """
    
    print(applications)


# ============================================================================
# SECTION 9: Performance Considerations
# ============================================================================

def streaming_performance():
    """
    Performance tips for streaming
    """
    print("\n" + "=" * 60)
    print("SECTION 9: Performance Considerations")
    print("=" * 60)
    
    tips = """
    ‚ö° OPTIMIZATION TIPS:
    
    1. BUFFER MANAGEMENT
       ‚ùå Don't: Update UI for every single character
       ‚úÖ Do: Buffer small chunks, update in reasonable intervals
       
       Example:
       buffer = ""
       for chunk in response:
           buffer += chunk.text
           if len(buffer) > 50:  # Update every 50 chars
               update_ui(buffer)
               buffer = ""
    
    2. NETWORK EFFICIENCY
       ‚Ä¢ Use HTTP/2 for multiplexing
       ‚Ä¢ Enable compression
       ‚Ä¢ Keep connections alive
       ‚Ä¢ Use CDN for static assets
    
    3. UI RESPONSIVENESS
       ‚Ä¢ Use virtual scrolling for long outputs
       ‚Ä¢ Debounce updates if needed
       ‚Ä¢ Don't block main thread
       ‚Ä¢ Show loading states
    
    4. ERROR RECOVERY
       ‚Ä¢ Implement retry logic
       ‚Ä¢ Save partial responses
       ‚Ä¢ Graceful degradation
       ‚Ä¢ User-friendly error messages
    
    5. RESOURCE MANAGEMENT
       ‚Ä¢ Close streams properly
       ‚Ä¢ Clean up event listeners
       ‚Ä¢ Monitor memory usage
       ‚Ä¢ Implement timeouts
    
    
    üìä MEASURING PERFORMANCE:
    
    Key Metrics:
    ‚Ä¢ Time to first byte (TTFB)
    ‚Ä¢ Tokens per second
    ‚Ä¢ Total generation time
    ‚Ä¢ Network latency
    ‚Ä¢ UI responsiveness
    
    
    üîç DEBUGGING:
    
    Common Issues:
    1. Choppy streaming ‚Üí Network buffering
    2. Delayed start ‚Üí Cold start/model loading
    3. Missing chunks ‚Üí Error handling needed
    4. Memory leaks ‚Üí Clean up properly
    """
    
    print(tips)


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """
    Main function with menu
    """
    print("\n")
    print("üéì " + "=" * 58 + " üéì")
    print("    GENERATIVE AI SESSION - MODULE 5: STREAMING CONCEPTS")
    print("üéì " + "=" * 58 + " üéì")
    
    menu = """
    Choose a section to run:
    
    1. Understanding Streaming
    2. Basic Streaming Example
    3. Streaming vs Non-Streaming Comparison
    4. Token-by-Token Streaming
    5. Interactive Streaming Chat
    6. Streaming with Progress Indicators
    7. Error Handling in Streaming
    8. Practical Applications
    9. Performance Considerations
    
    all - Run all (except interactive)
    quit - Exit
    
    """
    
    while True:
        print(menu)
        choice = input("Your choice: ").strip().lower()
        
        if choice in ['quit', 'q', 'exit']:
            print("üëã Goodbye!")
            break
        elif choice == '1':
            streaming_concepts()
        elif choice == '2':
            basic_streaming_example()
        elif choice == '3':
            compare_streaming_vs_non_streaming()
        elif choice == '4':
            token_by_token_streaming()
        elif choice == '5':
            interactive_streaming_chat()
        elif choice == '6':
            streaming_with_progress()
        elif choice == '7':
            streaming_error_handling()
        elif choice == '8':
            practical_streaming_applications()
        elif choice == '9':
            streaming_performance()
        elif choice == 'all':
            streaming_concepts()
            basic_streaming_example()
            compare_streaming_vs_non_streaming()
            token_by_token_streaming()
            streaming_with_progress()
            streaming_error_handling()
            practical_streaming_applications()
            streaming_performance()
            print("\n‚úÖ All sections completed!")
            print("üí° Try section 5 separately for interactive chat")
            break
        else:
            print("‚ö†Ô∏è  Invalid choice. Please try again.")


if __name__ == "__main__":
    main()
    
    # Teaching Questions:
    # 1. Why is streaming important for user experience?
    # 2. When would you NOT use streaming?
    # 3. How would you implement streaming in a web app?
