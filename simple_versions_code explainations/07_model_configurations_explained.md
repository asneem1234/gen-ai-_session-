# 07 Model Configurations - Complete Code Explanation

## ğŸ“‹ Overview
This code demonstrates how to control AI behavior through configuration parameters. Temperature, max tokens, and top-p sampling allow you to fine-tune responses for different use cases - from deterministic factual answers to creative storytelling. Understanding these settings is essential for building reliable AI applications.

---

## ğŸ’» Complete Code with Line-by-Line Explanation

```python
import os
# WHY: Import operating system module
# WHAT IT DOES: Provides access to environment variables
# WHEN TO USE: Required for reading API keys securely

from dotenv import load_dotenv
# WHY: Import function to load .env file
# WHAT IT DOES: Reads environment variables from .env
# SECURITY: Keeps sensitive information out of code

import google.generativeai as genai
# WHY: Import Google Generative AI library
# WHAT IT DOES: Provides tools for AI operations
# CAPABILITY: Supports extensive configuration options

load_dotenv()
# WHY: Load environment variables at module level
# WHAT IT DOES: Makes GOOGLE_API_KEY accessible
# TIMING: Called immediately before configuration

genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
# WHY: Authenticate with Gemini API
# WHAT IT DOES: Sets up API key for all requests
# REQUIRED: Must be done before any AI operations

def temperature_control():
    # WHY: Demonstrate temperature parameter effects
    # WHAT IT DOES: Shows how temperature controls randomness
    # USE CASE: Choosing between consistent vs creative responses
    # RANGE: 0.0 to 2.0 (typically 0.0-1.0 recommended)
    
    prompt = "Complete this sentence: The future of AI is"
    # WHY: Use open-ended prompt for comparison
    # WHAT IT DOES: Creates sentence requiring completion
    # PURPOSE: Same prompt with different temps shows variation
    # OPEN-ENDED: Allows AI to demonstrate creativity vs consistency
    
    print("Low temperature (0.1) - Deterministic:")
    # WHY: Label the low temperature test
    # WHAT IT DOES: Indicates deterministic mode
    # EXPECTATION: Consistent, predictable responses
    
    model = genai.GenerativeModel('gemini-2.0-flash', 
                                   generation_config={'temperature': 0.1})
    # WHY: Create model with low temperature
    # WHAT IT DOES: Initializes model with specific config
    # PARAMETER: generation_config dictionary
    #   - temperature: 0.1 (very low)
    # EFFECT: AI chooses most probable tokens
    # BEHAVIOR: Responses will be similar each time
    # USE CASE: Factual answers, code generation, consistency
    
    response = model.generate_content(prompt)
    # WHY: Generate content with low temperature
    # WHAT IT DOES: Gets deterministic response
    # EXPECTED: Predictable, safe completion
    
    print(f"{response.text}\n")
    # WHY: Display the deterministic response
    # WHAT IT DOES: Shows consistent output
    # TYPICAL: "promising" or "bright" (common words)
    
    print("High temperature (1.5) - Creative:")
    # WHY: Label the high temperature test
    # WHAT IT DOES: Indicates creative mode
    # EXPECTATION: Varied, creative responses
    
    model = genai.GenerativeModel('gemini-2.0-flash',
                                   generation_config={'temperature': 1.5})
    # WHY: Create model with high temperature
    # WHAT IT DOES: Initializes model with creative config
    # PARAMETER: temperature: 1.5 (high)
    # EFFECT: AI explores less probable tokens
    # BEHAVIOR: Responses vary significantly each run
    # USE CASE: Creative writing, brainstorming, variety
    # WARNING: Very high temps (>1.0) can produce nonsense
    
    response = model.generate_content(prompt)
    # WHY: Generate content with high temperature
    # WHAT IT DOES: Gets creative, varied response
    # EXPECTED: Unexpected, imaginative completion
    
    print(f"{response.text}")
    # WHY: Display the creative response
    # WHAT IT DOES: Shows varied output
    # TYPICAL: Unusual or creative word choices

def max_tokens_control():
    # WHY: Demonstrate output length control
    # WHAT IT DOES: Shows how to limit response length
    # USE CASE: Controlling costs, UI constraints, summaries
    # TOKEN: ~4 characters = 1 token (rough estimate)
    
    prompt = "Explain quantum computing"
    # WHY: Use complex topic for length demo
    # WHAT IT DOES: Creates prompt that could generate long response
    # PURPOSE: Shows clear difference between length limits
    # TOPIC: Quantum computing has much to explain
    
    print("Short response (50 tokens):")
    # WHY: Label the short response test
    # WHAT IT DOES: Indicates length limit
    # ~50 tokens â‰ˆ 200 characters â‰ˆ 2-3 sentences
    
    model = genai.GenerativeModel('gemini-2.0-flash',
                                   generation_config={'max_output_tokens': 50})
    # WHY: Create model with token limit
    # WHAT IT DOES: Initializes model with length constraint
    # PARAMETER: max_output_tokens: 50
    # EFFECT: Response stops at ~50 tokens
    # BEHAVIOR: May cut off mid-sentence if limit reached
    # USE CASE: Summaries, short answers, cost control
    # COST: Fewer tokens = lower cost
    
    response = model.generate_content(prompt)
    # WHY: Generate short response
    # WHAT IT DOES: Gets truncated explanation
    # EXPECTED: Brief, possibly incomplete answer
    
    print(f"{response.text}\n")
    # WHY: Display the short response
    # WHAT IT DOES: Shows limited output
    # RESULT: Concise explanation or summary
    
    print("Longer response (200 tokens):")
    # WHY: Label the longer response test
    # WHAT IT DOES: Indicates higher limit
    # ~200 tokens â‰ˆ 800 characters â‰ˆ 8-10 sentences
    
    model = genai.GenerativeModel('gemini-2.0-flash',
                                   generation_config={'max_output_tokens': 200})
    # WHY: Create model with higher token limit
    # WHAT IT DOES: Allows longer, detailed response
    # PARAMETER: max_output_tokens: 200
    # EFFECT: Response can be up to ~200 tokens
    # BEHAVIOR: More comprehensive explanation
    # USE CASE: Detailed answers, articles, explanations
    # TRADEOFF: More tokens = higher cost
    
    response = model.generate_content(prompt)
    # WHY: Generate longer response
    # WHAT IT DOES: Gets detailed explanation
    # EXPECTED: Complete, thorough answer
    
    print(f"{response.text}")
    # WHY: Display the longer response
    # WHAT IT DOES: Shows extended output
    # RESULT: Comprehensive quantum computing explanation

def top_p_sampling():
    # WHY: Demonstrate nucleus sampling (top-p)
    # WHAT IT DOES: Shows probability mass-based token selection
    # USE CASE: Balancing quality and diversity
    # RANGE: 0.0 to 1.0
    
    prompt = "Tell me an interesting fact"
    # WHY: Use fact-based prompt for diversity demo
    # WHAT IT DOES: Creates open-ended request
    # PURPOSE: Shows how top-p affects fact selection
    # VARIETY: Many possible interesting facts exist
    
    print("Top-p = 0.5 (focused):")
    # WHY: Label the focused sampling test
    # WHAT IT DOES: Indicates narrow token selection
    # 0.5 = Consider tokens in top 50% probability mass
    
    model = genai.GenerativeModel('gemini-2.0-flash',
                                   generation_config={'top_p': 0.5})
    # WHY: Create model with nucleus sampling
    # WHAT IT DOES: Initializes model with probability threshold
    # PARAMETER: top_p: 0.5
    # EFFECT: Only considers most probable tokens (50% mass)
    # BEHAVIOR: More focused, safer responses
    # USE CASE: Factual content, professional writing
    # HOW IT WORKS:
    #   1. Sort tokens by probability
    #   2. Sum probabilities until reaching 0.5
    #   3. Only sample from those tokens
    
    response = model.generate_content(prompt)
    # WHY: Generate with focused sampling
    # WHAT IT DOES: Gets response from high-probability tokens
    # EXPECTED: Common, well-known fact
    
    print(f"{response.text}\n")
    # WHY: Display focused response
    # WHAT IT DOES: Shows safer, common output
    # TYPICAL: Well-known scientific facts
    
    print("Top-p = 0.95 (diverse):")
    # WHY: Label the diverse sampling test
    # WHAT IT DOES: Indicates broader token selection
    # 0.95 = Consider tokens in top 95% probability mass
    
    model = genai.GenerativeModel('gemini-2.0-flash',
                                   generation_config={'top_p': 0.95})
    # WHY: Create model with wider sampling
    # WHAT IT DOES: Allows more diverse token choices
    # PARAMETER: top_p: 0.95
    # EFFECT: Considers wider range of tokens (95% mass)
    # BEHAVIOR: More diverse, interesting responses
    # USE CASE: Creative writing, variety, exploration
    # BALANCE: High enough for diversity, not so high for nonsense
    
    response = model.generate_content(prompt)
    # WHY: Generate with diverse sampling
    # WHAT IT DOES: Gets response from broader token set
    # EXPECTED: More unique or unexpected fact
    
    print(f"{response.text}")
    # WHY: Display diverse response
    # WHAT IT DOES: Shows more varied output
    # TYPICAL: Less common but interesting facts

def combined_settings():
    # WHY: Demonstrate using multiple parameters together
    # WHAT IT DOES: Shows balanced configuration
    # USE CASE: Real-world applications need multiple controls
    # GOAL: Achieve specific quality/creativity/length balance
    
    config = {
        'temperature': 0.7,
        'max_output_tokens': 100,
        'top_p': 0.9
    }
    # WHY: Define configuration dictionary
    # WHAT IT DOES: Specifies multiple parameters at once
    # PARAMETERS:
    #   - temperature: 0.7 (moderate creativity)
    #   - max_output_tokens: 100 (medium length)
    #   - top_p: 0.9 (good diversity)
    # BALANCE: Reasonable creativity + quality + length
    # USE CASE: General-purpose content generation
    # EXPLANATION:
    #   - 0.7 temp: Creative but not wild
    #   - 100 tokens: ~400 chars, decent detail
    #   - 0.9 top-p: Diverse but not random
    
    model = genai.GenerativeModel('gemini-2.0-flash', generation_config=config)
    # WHY: Create model with combined configuration
    # WHAT IT DOES: Applies all parameters simultaneously
    # PARAMETER: Pass entire config dictionary
    # EFFECT: All settings work together
    # RESULT: Balanced behavior across all dimensions
    
    response = model.generate_content("Write a creative tagline for an AI company")
    # WHY: Use creative prompt to show balanced config
    # WHAT IT DOES: Requests creative but focused content
    # DEMONSTRATES: Config produces quality creative output
    # TAGLINE: Short, creative, memorable (perfect test)
    
    print(f"Response: {response.text}")
    # WHY: Display the balanced response
    # WHAT IT DOES: Shows result of combined settings
    # EXPECTED: Creative but sensible tagline

if __name__ == "__main__":
    # WHY: Check if script is run directly
    # WHAT IT DOES: Only executes demos when main program
    # BEST PRACTICE: Prevents auto-execution on import
    
    print("1. Temperature Control")
    # WHY: Label the temperature demo
    # WHAT IT DOES: Prints section header
    # UX: Helps user track demo progress
    
    temperature_control()
    # WHY: Execute temperature comparison demo
    # WHAT IT DOES: Shows low vs high temperature effects
    # DEMONSTRATES: Randomness control
    
    print("\n2. Max Tokens Control")
    # WHY: Label the tokens demo
    # WHAT IT DOES: Prints header with spacing
    # UX: Separates demos visually
    
    max_tokens_control()
    # WHY: Execute token limit demo
    # WHAT IT DOES: Shows short vs long responses
    # DEMONSTRATES: Length control
    
    print("\n3. Top-p Sampling")
    # WHY: Label the top-p demo
    # WHAT IT DOES: Prints header
    # INDICATES: Nucleus sampling test
    
    top_p_sampling()
    # WHY: Execute top-p comparison demo
    # WHAT IT DOES: Shows focused vs diverse sampling
    # DEMONSTRATES: Diversity control
    
    print("\n4. Combined Settings")
    # WHY: Label the combined demo
    # WHAT IT DOES: Prints header
    # INDICATES: Multiple parameters together
    
    combined_settings()
    # WHY: Execute combined configuration demo
    # WHAT IT DOES: Shows balanced parameter usage
    # DEMONSTRATES: Real-world configuration
```

---

## ğŸ”„ Code Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        START PROGRAM                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. IMPORT & CONFIGURE                                              â”‚
â”‚     â”œâ”€ Import: os, dotenv, genai                                   â”‚
â”‚     â”œâ”€ Load .env file                                              â”‚
â”‚     â””â”€ Configure API key                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. DEFINE CONFIGURATION FUNCTIONS                                  â”‚
â”‚     â”œâ”€ temperature_control()                                       â”‚
â”‚     â”œâ”€ max_tokens_control()                                        â”‚
â”‚     â”œâ”€ top_p_sampling()                                            â”‚
â”‚     â””â”€ combined_settings()                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. DEMO 1: Temperature Control                                     â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ Prompt: "Complete: The future of AI is..."                  â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ LOW TEMPERATURE (0.1):                                      â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Create model with temp=0.1                              â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Token Selection Process:                                â”‚
â”‚     â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚     â”‚   â”‚   â”‚ Possible next tokens:               â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ "promising" â†’ 80% probability âœ“     â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ "bright"    â†’ 15% probability       â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ "uncertain" â†’ 3% probability        â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ "purple"    â†’ 2% probability        â”‚                â”‚
â”‚     â”‚   â”‚   â”‚                                     â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ Low temp: Always picks "promising"  â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ (highest probability)               â”‚                â”‚
â”‚     â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â””â”€ Output: "promising, with continued advances..."         â”‚
â”‚     â”‚       â””â”€ CONSISTENT (run again = same result)                â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ HIGH TEMPERATURE (1.5):                                     â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Create model with temp=1.5                              â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Token Selection Process:                                â”‚
â”‚     â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚     â”‚   â”‚   â”‚ Same token probabilities:           â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ "promising" â†’ 80%  â† Maybe         â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ "bright"    â†’ 15%  â† Maybe         â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ "uncertain" â†’ 3%   â† Possible!     â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ "purple"    â†’ 2%   â† Even this!    â”‚                â”‚
â”‚     â”‚   â”‚   â”‚                                     â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ High temp: Explores lower probs     â”‚                â”‚
â”‚     â”‚   â”‚   â”‚ (more randomness)                   â”‚                â”‚
â”‚     â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â””â”€ Output: "uncertain yet fascinating, unfolding..."       â”‚
â”‚     â”‚       â””â”€ VARIABLE (run again = different result)             â”‚
â”‚     â”‚                                                               â”‚
â”‚     â””â”€ Temperature Scale:                                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚         â”‚ 0.0 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• 2.0   â”‚            â”‚
â”‚         â”‚  â†‘                    â†‘              â†‘     â”‚            â”‚
â”‚         â”‚  â”‚                    â”‚              â”‚     â”‚            â”‚
â”‚         â”‚ Deterministic    Balanced      Chaotic     â”‚            â”‚
â”‚         â”‚ (facts, code)    (general)   (creative)    â”‚            â”‚
â”‚         â”‚                                            â”‚            â”‚
â”‚         â”‚ 0.1: Almost no randomness                  â”‚            â”‚
â”‚         â”‚ 0.7: Good balance (recommended)            â”‚            â”‚
â”‚         â”‚ 1.5: High creativity (can be nonsensical) â”‚            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. DEMO 2: Max Tokens Control                                      â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ Prompt: "Explain quantum computing"                         â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ SHORT (50 tokens):                                          â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Model config: max_output_tokens=50                      â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Generation Process:                                     â”‚
â”‚     â”‚   â”‚   Token 1: "Quantum"                                     â”‚
â”‚     â”‚   â”‚   Token 2: "computing"                                   â”‚
â”‚     â”‚   â”‚   Token 3: "uses"                                        â”‚
â”‚     â”‚   â”‚   ...                                                    â”‚
â”‚     â”‚   â”‚   Token 48: "information"                                â”‚
â”‚     â”‚   â”‚   Token 49: "processing"                                 â”‚
â”‚     â”‚   â”‚   Token 50: "." â†’ STOP! âœ‹ (limit reached)               â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â””â”€ Output: "Quantum computing uses quantum mechanics       â”‚
â”‚     â”‚       principles like superposition and entanglement..."     â”‚
â”‚     â”‚       â””â”€ ~200 characters, brief explanation                  â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ LONG (200 tokens):                                          â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Model config: max_output_tokens=200                     â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Generation continues to 200 tokens                      â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â””â”€ Output: "Quantum computing uses quantum mechanics...    â”‚
â”‚     â”‚       Unlike classical computers that use bits... Qubits     â”‚
â”‚     â”‚       can exist in superposition... This enables parallel    â”‚
â”‚     â”‚       processing... Applications include cryptography..."    â”‚
â”‚     â”‚       â””â”€ ~800 characters, detailed explanation               â”‚
â”‚     â”‚                                                               â”‚
â”‚     â””â”€ Token Length Comparison:                                    â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚         â”‚ 50 tokens  â–ˆâ–ˆâ–ˆâ–ˆ (~2-3 sentences)        â”‚               â”‚
â”‚         â”‚ 100 tokens â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (~5-6 sentences)    â”‚               â”‚
â”‚         â”‚ 200 tokens â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (~10-12)    â”‚               â”‚
â”‚         â”‚ 500 tokens â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚               â”‚
â”‚         â”‚                                         â”‚               â”‚
â”‚         â”‚ Rule of thumb: 1 token â‰ˆ 4 characters   â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. DEMO 3: Top-p Sampling (Nucleus Sampling)                       â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ Prompt: "Tell me an interesting fact"                       â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ FOCUSED (top_p=0.5):                                        â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Model config: top_p=0.5                                 â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Token Selection Process:                                â”‚
â”‚     â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚     â”‚   â”‚   â”‚ All possible tokens (sorted by prob):  â”‚            â”‚
â”‚     â”‚   â”‚   â”‚                                         â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Token A: 30% âœ“ â”                       â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Token B: 15% âœ“ â”‚ Sum = 50%             â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Token C: 5%  âœ“ â”˜ (top-p threshold)     â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Token D: 3%  âœ— (below threshold)       â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Token E: 2%  âœ—                          â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ ...more...                              â”‚            â”‚
â”‚     â”‚   â”‚   â”‚                                         â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Only A, B, C considered for selection   â”‚            â”‚
â”‚     â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â””â”€ Output: "The human brain contains approximately         â”‚
â”‚     â”‚       86 billion neurons." (common fact)                     â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ DIVERSE (top_p=0.95):                                       â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Model config: top_p=0.95                                â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Token Selection Process:                                â”‚
â”‚     â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚     â”‚   â”‚   â”‚ All possible tokens:                    â”‚            â”‚
â”‚     â”‚   â”‚   â”‚                                         â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Token A: 30% âœ“ â”                       â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Token B: 15% âœ“ â”‚                       â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Token C: 5%  âœ“ â”‚                       â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Token D: 3%  âœ“ â”‚ Sum = 95%             â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Token E: 2%  âœ“ â”‚ (top-p threshold)     â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ ...many more âœ“ â”˜                       â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Token Z: 0.1% âœ— (below threshold)      â”‚            â”‚
â”‚     â”‚   â”‚   â”‚                                         â”‚            â”‚
â”‚     â”‚   â”‚   â”‚ Many tokens considered for selection    â”‚            â”‚
â”‚     â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â””â”€ Output: "Octopuses have three hearts and blue           â”‚
â”‚     â”‚       blood." (less common, more interesting)                â”‚
â”‚     â”‚                                                               â”‚
â”‚     â””â”€ Top-p Scale:                                                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚         â”‚ 0.0 â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• 1.0 â”‚               â”‚
â”‚         â”‚  â†‘            â†‘            â†‘         â†‘   â”‚               â”‚
â”‚         â”‚ Impossible  Very       Balanced   Almost â”‚               â”‚
â”‚         â”‚           Focused                   All  â”‚               â”‚
â”‚         â”‚                                          â”‚               â”‚
â”‚         â”‚ 0.5: Only top 50% prob mass (safe)      â”‚               â”‚
â”‚         â”‚ 0.9: Top 90% prob mass (recommended)    â”‚               â”‚
â”‚         â”‚ 0.95: Top 95% prob mass (diverse)       â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. DEMO 4: Combined Settings                                       â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ Configuration Dictionary:                                   â”‚
â”‚     â”‚   {                                                           â”‚
â”‚     â”‚     'temperature': 0.7,      â† Moderate creativity           â”‚
â”‚     â”‚     'max_output_tokens': 100, â† Medium length                â”‚
â”‚     â”‚     'top_p': 0.9              â† Good diversity               â”‚
â”‚     â”‚   }                                                           â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ Prompt: "Write a creative tagline for an AI company"        â”‚
â”‚     â”‚                                                               â”‚
â”‚     â”œâ”€ How Parameters Work Together:                               â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Step 1: Top-p filters tokens (top 90% mass)             â”‚
â”‚     â”‚   â”‚   â””â”€ Removes very unlikely words                         â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Step 2: Temperature adjusts probabilities               â”‚
â”‚     â”‚   â”‚   â””â”€ Moderate randomness within filtered set             â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â”œâ”€ Step 3: Generate tokens one by one                      â”‚
â”‚     â”‚   â”‚   â””â”€ "Empowering" "tomorrow," "today."                   â”‚
â”‚     â”‚   â”‚                                                           â”‚
â”‚     â”‚   â””â”€ Step 4: Stop at max_output_tokens (100)                 â”‚
â”‚     â”‚       â””â”€ Or natural completion, whichever first              â”‚
â”‚     â”‚                                                               â”‚
â”‚     â””â”€ Output: "Empowering tomorrow, today: Where human            â”‚
â”‚         imagination meets artificial intelligence to create         â”‚
â”‚         infinite possibilities."                                    â”‚
â”‚         â””â”€ Creative âœ“, Concise âœ“, Quality âœ“                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PROGRAM COMPLETE âœ…                             â”‚
â”‚  All configuration demonstrations complete!                         â”‚
â”‚  - Temperature: Controls randomness/creativity                      â”‚
â”‚  - Max tokens: Controls response length                             â”‚
â”‚  - Top-p: Controls diversity via probability filtering              â”‚
â”‚  - Combined: Balanced configuration for real use                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Sample Output

### âœ… Complete Successful Execution

```
1. Temperature Control
Low temperature (0.1) - Deterministic:
promising, with continued advances in machine learning, natural language processing, and responsible AI development transforming industries and society.

High temperature (1.5) - Creative:
uncertain yet endlessly fascinating, a kaleidoscope of potential where silicon dreams dance with quantum whispers, perhaps transcending our wildest imaginings or maybe just making really good pizza recommendations.

2. Max Tokens Control
Short response (50 tokens):
Quantum computing uses quantum mechanics principles like superposition and entanglement to process information. Unlike classical computers using bits (0 or 1), quantum computers use qubits that can be both simultaneously, enabling parallel processing for complex problems.

Longer response (200 tokens):
Quantum computing uses quantum mechanics principles to process information in fundamentally different ways than classical computers. While traditional computers use bits representing either 0 or 1, quantum computers use quantum bits (qubits) that can exist in superpositionâ€”simultaneously representing 0 and 1. This property, along with quantum entanglement (where qubits become correlated), allows quantum computers to perform many calculations in parallel. Quantum computing shows promise for solving complex problems in cryptography, drug discovery, financial modeling, and optimization tasks that would take classical computers impractically long times. However, qubits are fragile and require extreme conditions like near-absolute zero temperatures. Current quantum computers are still limited in size and error-prone, but ongoing research continues advancing this revolutionary technology.

3. Top-p Sampling
Top-p = 0.5 (focused):
The human brain contains approximately 86 billion neurons, each capable of forming thousands of connections with other neurons, creating an incredibly complex network that enables thought, memory, and consciousness.

Top-p = 0.95 (diverse):
Octopuses have three hearts: two pump blood to the gills, while the third pumps it to the rest of the body. Even more fascinating, when an octopus swims, the heart that delivers blood to the body actually stops beating!

4. Combined Settings
Response: Empowering tomorrow, today: Where human imagination meets artificial intelligence to create infinite possibilities for a smarter, more connected world.
```

---

## ğŸ¯ Key Concepts Explained

### 1. **Temperature: Randomness Control**

```python
# Temperature affects token probability distribution

# Low temperature (0.1)
# Probabilities: [80%, 15%, 3%, 2%]
# After temp: [95%, 4%, 0.5%, 0.5%] â† More peaked
# Effect: Almost always picks highest probability

# High temperature (1.5)
# Probabilities: [80%, 15%, 3%, 2%]
# After temp: [40%, 25%, 20%, 15%] â† More flattened
# Effect: More likely to pick lower probability options
```

**Use Cases:**
- **0.0-0.3:** Code generation, factual answers, consistency needed
- **0.4-0.7:** General purpose, balanced creativity
- **0.8-1.0:** Creative writing, brainstorming
- **1.0+:** Experimental, very creative (can be nonsensical)

### 2. **Max Output Tokens: Length Control**

```python
# Token counting examples
"Hello" = 1 token
"Hello, world!" = 4 tokens
"The quick brown fox" = 4 tokens

# Rough conversion: 1 token â‰ˆ 4 characters (English)

# Setting limits
max_output_tokens=50   # ~200 chars, 2-3 sentences
max_output_tokens=100  # ~400 chars, 5-6 sentences
max_output_tokens=500  # ~2000 chars, paragraph
max_output_tokens=2000 # ~8000 chars, article
```

**Use Cases:**
- **50-100:** Short answers, UI constraints, cost control
- **100-300:** Standard responses, chatbot replies
- **500-1000:** Detailed explanations, articles
- **1000+:** Long-form content, documentation

### 3. **Top-p (Nucleus Sampling): Diversity Control**

```python
# How top-p works:
# 1. Sort all possible tokens by probability
# 2. Sum probabilities until reaching top_p threshold
# 3. Only sample from those tokens

# Example with top_p=0.8:
tokens = [
    ("the", 0.5),    # Cumulative: 0.5 âœ“
    ("a", 0.2),      # Cumulative: 0.7 âœ“
    ("this", 0.15),  # Cumulative: 0.85 âœ— (exceeds 0.8)
    ("that", 0.1),   # Not considered
    # ... rest ignored
]
# Only "the" and "a" considered for selection
```

**Use Cases:**
- **0.5-0.7:** Focused, consistent output
- **0.8-0.9:** Balanced quality and diversity (recommended)
- **0.9-0.95:** More creative, diverse responses
- **0.95-1.0:** Maximum diversity (can be unpredictable)

### 4. **Temperature vs Top-p**

```python
# Often used together!

# Temperature: Adjusts probability distribution
# Top-p: Filters which tokens can be selected

# Recommended combinations:
config_creative = {'temperature': 0.9, 'top_p': 0.95}
config_balanced = {'temperature': 0.7, 'top_p': 0.9}
config_focused = {'temperature': 0.3, 'top_p': 0.8}
config_deterministic = {'temperature': 0.1, 'top_p': 0.5}
```

---

## ğŸš€ What Happens Behind the Scenes

### Token Generation Process with Configuration:

```
1. MODEL RECEIVES PROMPT
   "Complete: The future of AI is"

2. PREDICT NEXT TOKEN PROBABILITIES
   All possible tokens:
   "promising" â†’ 0.35
   "bright"    â†’ 0.20
   "uncertain" â†’ 0.10
   "exciting"  â†’ 0.08
   ... (thousands more)

3. APPLY TOP-P FILTER (if set)
   top_p=0.8
   â”œâ”€ "promising" 0.35 (cumulative: 0.35) âœ“
   â”œâ”€ "bright"    0.20 (cumulative: 0.55) âœ“
   â”œâ”€ "uncertain" 0.10 (cumulative: 0.65) âœ“
   â”œâ”€ "exciting"  0.08 (cumulative: 0.73) âœ“
   â””â”€ Next token 0.07 (cumulative: 0.80) â†’ STOP
   
   Remaining tokens considered: 5

4. APPLY TEMPERATURE
   temperature=0.7
   â”œâ”€ Adjusts probabilities
   â””â”€ "promising": 0.35 â†’ 0.40 (slightly increased)

5. SAMPLE TOKEN
   Select one token based on adjusted probabilities
   Selected: "promising"

6. CHECK MAX_TOKENS
   Current tokens: 8
   max_output_tokens: 100
   â”œâ”€ Continue? YES (8 < 100)
   â””â”€ Add "promising" to output

7. REPEAT STEPS 2-6
   Until:
   - Reach max_output_tokens, OR
   - Generate stop token (natural end), OR
   - Hit context limit
```

---

## ğŸ“ Common Use Cases

### Use Case 1: Code Generation (Deterministic)
```python
def generate_code():
    config = {
        'temperature': 0.1,     # Very consistent
        'max_output_tokens': 500,
        'top_p': 0.5            # Safe choices only
    }
    model = genai.GenerativeModel('gemini-2.0-flash', generation_config=config)
    response = model.generate_content("Write a Python function to sort a list")
    return response.text
```

### Use Case 2: Creative Writing (High Creativity)
```python
def creative_story():
    config = {
        'temperature': 0.9,     # Very creative
        'max_output_tokens': 1000,
        'top_p': 0.95           # Maximum diversity
    }
    model = genai.GenerativeModel('gemini-2.0-flash', generation_config=config)
    response = model.generate_content("Write a sci-fi short story about AI")
    return response.text
```

### Use Case 3: Chatbot (Balanced)
```python
def chatbot_response(message):
    config = {
        'temperature': 0.7,     # Friendly variety
        'max_output_tokens': 150,  # Concise replies
        'top_p': 0.9            # Good quality
    }
    model = genai.GenerativeModel('gemini-2.0-flash', generation_config=config)
    response = model.generate_content(message)
    return response.text
```

### Use Case 4: Summaries (Short & Focused)
```python
def summarize_text(text):
    config = {
        'temperature': 0.3,     # Consistent summaries
        'max_output_tokens': 100,  # Brief output
        'top_p': 0.8            # Focused
    }
    model = genai.GenerativeModel('gemini-2.0-flash', generation_config=config)
    response = model.generate_content(f"Summarize: {text}")
    return response.text
```

---

## ğŸ’¡ Best Practices

### 1. **Start with Defaults, Then Tune**
```python
# Start simple
model = genai.GenerativeModel('gemini-2.0-flash')

# Test output quality
# Then adjust if needed

# If too random â†’ lower temperature
# If too repetitive â†’ raise temperature
# If too long â†’ add max_output_tokens
# If too generic â†’ raise top_p
```

### 2. **Don't Set Extreme Values**
```python
# BAD: Too extreme
config = {'temperature': 2.0, 'top_p': 1.0}  # Nonsense
config = {'temperature': 0.0, 'top_p': 0.1}  # Too rigid

# GOOD: Reasonable ranges
config = {'temperature': 0.7, 'top_p': 0.9}  # Balanced
```

### 3. **Consider Use Case**
```python
# Different tasks need different configs
configs = {
    'factual': {'temperature': 0.2, 'top_p': 0.7},
    'creative': {'temperature': 0.9, 'top_p': 0.95},
    'code': {'temperature': 0.1, 'top_p': 0.5},
    'chat': {'temperature': 0.7, 'top_p': 0.9}
}
```

### 4. **Monitor Token Usage**
```python
# Check token count for cost control
response = model.generate_content(prompt)
# Note: Actual token count available in response metadata
print(f"Tokens used: {response.usage_metadata.total_token_count}")
```

---

## âš ï¸ Important Notes

1. **Default Values:** If not specified, Gemini uses sensible defaults
2. **Temperature Range:** 0.0-2.0 (stick to 0.0-1.0 for quality)
3. **Top-p Range:** 0.0-1.0 (0.9 is common default)
4. **Max Tokens:** Response can be shorter than limit (natural ending)
5. **Token Counting:** Approximate (4 chars â‰ˆ 1 token for English)
6. **Configuration Scope:** Settings apply per model instance
7. **Cost Impact:** More tokens = higher cost
8. **Quality vs Creativity:** Always a tradeoff

---

## ğŸ”§ Advanced: Configuration Comparison Table

| Use Case | Temperature | Max Tokens | Top-p | Why |
|----------|-------------|------------|-------|-----|
| **Code Generation** | 0.1-0.2 | 500-1000 | 0.5-0.7 | Need consistency, correctness |
| **Factual Q&A** | 0.2-0.4 | 100-300 | 0.7-0.8 | Accurate, focused answers |
| **Chatbot** | 0.6-0.8 | 100-200 | 0.9 | Natural, varied responses |
| **Creative Writing** | 0.8-1.0 | 1000+ | 0.95 | Maximum creativity, diversity |
| **Summaries** | 0.3-0.5 | 50-150 | 0.8 | Consistent, concise output |
| **Brainstorming** | 0.9-1.2 | 200-500 | 0.95 | Diverse ideas, unexpected |
| **Translation** | 0.3 | 500+ | 0.8 | Accurate, consistent |
| **Classification** | 0.1 | 10-50 | 0.5 | Deterministic labels |

---

## ğŸ”— Prerequisites

1. âœ… Completed previous lessons (01-06)
2. âœ… Understanding of probability concepts (helpful)
3. âœ… Basic understanding of token-based language models

---

## ğŸ“ Learning Outcomes

After understanding this code, you should know:

- âœ… How temperature controls randomness and creativity
- âœ… How max_output_tokens limits response length
- âœ… How top-p sampling filters token selection
- âœ… When to use different configuration settings
- âœ… How to combine parameters for specific use cases
- âœ… The tradeoffs between quality, creativity, and cost
- âœ… How to tune configurations for your application
- âœ… Best practices for different content types

---

## ğŸ”œ Next Steps

1. Move to `08_system_instructions.py` to learn about system prompts
2. Experiment with different temperature values for your use case
3. Build a configuration selector based on task type
4. Create A/B tests comparing different settings
5. Monitor token usage to optimize costs

---

**âš™ï¸ Excellent!** You now understand how to fine-tune AI behavior through configuration parameters!
