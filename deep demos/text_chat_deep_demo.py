"""
Deep Demo: Text Chat - Behind The Scenes
=========================================

This demo shows EXACTLY what happens when you call Gemini:
- Token conversion
- Prompt structure
- Processing steps
- Response generation
- Token usage

Perfect for showing students "what's under the hood"
"""

import os
from dotenv import load_dotenv
import google.generativeai as genai
import time

# Setup
load_dotenv()
genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))


def print_section(title):
    """Print a fancy section header"""
    print("\n" + "="*70)
    print(f"  {title}")
    print("="*70 + "\n")


def simulate_tokenization(text):
    """
    Simulate tokenization (show approximate tokens)
    Real tokenization is more complex, this is for educational purposes
    """
    # Rough approximation: ~4 characters = 1 token
    words = text.split()
    tokens = []
    token_count = 0
    
    for word in words:
        # Simulate word being split into tokens
        if len(word) <= 4:
            tokens.append(f"[{word}]")
            token_count += 1
        else:
            # Longer words might be multiple tokens
            chunks = [word[i:i+4] for i in range(0, len(word), 4)]
            for chunk in chunks:
                tokens.append(f"[{chunk}]")
                token_count += 1
    
    return tokens, token_count


def deep_text_chat_demo():
    """
    Comprehensive demo showing each step of AI text generation
    """
    
    print("\n" + "ğŸ“" * 35)
    print("      DEEP DIVE: WHAT HAPPENS WHEN YOU CALL GEMINI AI")
    print("ğŸ“" * 35)
    
    # User input
    print_section("STEP 1: USER INPUT")
    
    user_prompt = "Explain what happens when you make a cup of tea in 3 steps"
    
    print(f"ğŸ“ User's Question:")
    print(f"   '{user_prompt}'")
    print(f"\nğŸ“Š Length: {len(user_prompt)} characters")
    
    # Tokenization
    print_section("STEP 2: TOKENIZATION (Text â†’ Tokens)")
    
    print("ğŸ’¡ What is Tokenization?")
    print("   AI models don't read text like humans. They convert words")
    print("   into numbers (tokens) that they can process.")
    print()
    
    tokens, token_count = simulate_tokenization(user_prompt)
    
    print(f"ğŸ“¦ Your text broken into tokens (approximate):")
    print("   " + " ".join(tokens[:15]))
    if len(tokens) > 15:
        print("   " + " ".join(tokens[15:]))
    print(f"\nğŸ“Š Approximate Token Count: {token_count} tokens")
    print(f"   (Actual may vary - this is a simplified simulation)")
    
    # Prompt structure
    print_section("STEP 3: PROMPT STRUCTURE")
    
    print("ğŸ—ï¸  Your prompt is structured for the AI:")
    print()
    print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ [SYSTEM CONTEXT]                                            â”‚")
    print("â”‚ â€¢ Model: gemini-2.0-flash                                         â”‚")
    print("â”‚ â€¢ Temperature: 0.7 (default)                                â”‚")
    print("â”‚ â€¢ Max tokens: 8192 (default)                                â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ [USER INPUT]                                                â”‚")
    print(f"â”‚ {user_prompt[:60]:<59} â”‚")
    if len(user_prompt) > 60:
        print(f"â”‚ {user_prompt[60:]:<59} â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # API Call
    print_section("STEP 4: SENDING TO GEMINI")
    
    print("ğŸŒ Making API call to Google's servers...")
    print("   â†“ Your prompt travels over HTTPS")
    print("   â†“ Reaches Google's AI infrastructure")
    print("   â†“ Routed to a Gemini Pro model instance")
    print()
    
    model = genai.GenerativeModel('gemini-2.0-flash')
    
    print("â³ Waiting for response...\n")
    
    start_time = time.time()
    
    # Make the actual API call
    response = model.generate_content(user_prompt)
    
    end_time = time.time()
    response_time = end_time - start_time
    
    # Processing simulation
    print_section("STEP 5: AI PROCESSING (What Happens Inside)")
    
    print("ğŸ§  Inside Gemini's Neural Network:")
    print()
    print("   1ï¸âƒ£  Token Embedding")
    print("      Each token â†’ high-dimensional vector (numbers)")
    print("      Example: 'tea' â†’ [0.234, -0.891, 0.445, ... 2048 dimensions]")
    print()
    print("   2ï¸âƒ£  Attention Mechanism")
    print("      AI analyzes relationships between words")
    print("      'cup' + 'tea' â†’ strong connection")
    print("      'make' + 'steps' â†’ procedural context")
    print()
    print("   3ï¸âƒ£  Transformer Layers (Billions of Parameters)")
    print("      40+ layers processing your input")
    print("      Pattern matching against training data")
    print("      Understanding context and intent")
    print()
    print("   4ï¸âƒ£  Response Generation")
    print("      Predicting next token, one at a time")
    print("      Each token chosen based on probability")
    print("      Temperature controls randomness")
    print()
    
    # Response received
    print_section("STEP 6: RESPONSE RECEIVED")
    
    print(f"âœ… Response generated in {response_time:.2f} seconds")
    print()
    print("ğŸ“¨ Raw Response Object:")
    print(f"   Type: {type(response)}")
    print(f"   Has text: {hasattr(response, 'text')}")
    print()
    
    # Token usage (if available)
    print("ğŸ“Š Token Usage:")
    if hasattr(response, 'usage_metadata') and response.usage_metadata:
        print(f"   â€¢ Prompt tokens: {response.usage_metadata.prompt_token_count}")
        print(f"   â€¢ Response tokens: {response.usage_metadata.candidates_token_count}")
        print(f"   â€¢ Total tokens: {response.usage_metadata.total_token_count}")
    else:
        print("   â€¢ Prompt tokens: ~" + str(token_count))
        print("   â€¢ Response tokens: ~" + str(len(response.text.split()) // 4 * 5))
        print("   (Exact counts require API response metadata)")
    
    # Generated text
    print_section("STEP 7: GENERATED TEXT")
    
    print("ğŸ¤– AI's Response:")
    print()
    print("â”€" * 70)
    print(response.text)
    print("â”€" * 70)
    
    # Response analysis
    print_section("STEP 8: RESPONSE ANALYSIS")
    
    response_words = response.text.split()
    response_chars = len(response.text)
    response_lines = response.text.count('\n') + 1
    
    print("ğŸ“ˆ Response Statistics:")
    print(f"   â€¢ Word count: {len(response_words)}")
    print(f"   â€¢ Character count: {response_chars}")
    print(f"   â€¢ Lines: {response_lines}")
    print(f"   â€¢ Average word length: {response_chars / len(response_words):.1f} chars")
    print()
    print(f"âš¡ Generation Speed:")
    print(f"   â€¢ {len(response_words) / response_time:.1f} words per second")
    print(f"   â€¢ {response_chars / response_time:.1f} characters per second")
    
    # Behind the scenes
    print_section("STEP 9: BEHIND THE SCENES")
    
    print("ğŸ” What You Don't See:")
    print()
    print("   ğŸ’° Cost (for you, with free tier):")
    print("      â€¢ This request: ~$0.000 (free tier)")
    print("      â€¢ In production: ~$0.0005 per 1K tokens")
    print()
    print("   ğŸŒ Infrastructure:")
    print("      â€¢ Datacenter: Google's global network")
    print("      â€¢ Hardware: TPU/GPU clusters")
    print("      â€¢ Model size: Billions of parameters")
    print("      â€¢ Training data: Trillions of tokens")
    print()
    print("   ğŸ” Security:")
    print("      â€¢ HTTPS encryption")
    print("      â€¢ API key authentication")
    print("      â€¢ Rate limiting")
    print("      â€¢ Content filtering")
    print()
    print("   âš™ï¸  Optimization:")
    print("      â€¢ Caching layers")
    print("      â€¢ Load balancing")
    print("      â€¢ Model quantization")
    print("      â€¢ Batch processing")
    
    # Complete flow diagram
    print_section("COMPLETE FLOW DIAGRAM")
    
    print("""
    Your Code                    Google's Infrastructure
    â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
         â”‚
         â”œâ”€â–º generate_content("question")
         â”‚
         â”œâ”€â–º Tokenization
         â”‚      â”‚
         â”‚      â†“
         â”œâ”€â–º HTTPS Request â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º API Gateway
         â”‚                                  â”‚
         â”‚                                  â†“
         â”‚                             Load Balancer
         â”‚                                  â”‚
         â”‚                                  â†“
         â”‚                             Auth Check
         â”‚                                  â”‚
         â”‚                                  â†“
         â”‚                             Rate Limit
         â”‚                                  â”‚
         â”‚                                  â†“
         â”‚                             Model Server
         â”‚                                  â”‚
         â”‚                                  â”œâ”€â–º Embedding
         â”‚                                  â”œâ”€â–º Attention
         â”‚                                  â”œâ”€â–º Transform
         â”‚                                  â”œâ”€â–º Generate
         â”‚                                  â”‚
         â”‚                                  â†“
         â—„â”€â”€â”€ Response â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Response
         â”‚
         â”œâ”€â–º response.text
         â”‚
         â†“
    Display to User
    """)
    
    # Interactive comparison
    print_section("BONUS: INTERACTIVE COMPARISON")
    
    print("ğŸ® Let's try different prompts and see the difference!\n")
    
    test_prompts = [
        ("Short", "What is AI?"),
        ("Detailed", "Explain artificial intelligence, its history, types, and applications in detail"),
        ("Creative", "Write a haiku about programming")
    ]
    
    for label, prompt in test_prompts:
        print(f"\n{'â”€'*70}")
        print(f"ğŸ“ {label} Prompt: '{prompt}'")
        tokens_approx, count = simulate_tokenization(prompt)
        print(f"ğŸ“Š Tokens: ~{count}")
        
        start = time.time()
        resp = model.generate_content(prompt)
        elapsed = time.time() - start
        
        print(f"â±ï¸  Response time: {elapsed:.2f}s")
        print(f"ğŸ“ Response length: {len(resp.text)} chars, {len(resp.text.split())} words")
        print(f"\nğŸ¤– Response preview:")
        print(f"   {resp.text[:100]}..." if len(resp.text) > 100 else f"   {resp.text}")
    
    # Summary
    print_section("ğŸ“ KEY TAKEAWAYS")
    
    print("""
    âœ… What You Learned:
    
    1. Text â†’ Tokens â†’ Numbers (that's how AI reads)
    2. Your prompt + config = structured input
    3. Billions of parameters process your request
    4. Response generated token by token
    5. All this happens in < 1 second!
    
    ğŸ’¡ Why This Matters:
    
    â€¢ Understanding tokens helps optimize costs
    â€¢ Knowing response time helps UX design
    â€¢ Seeing the flow helps debugging
    â€¢ Token limits explain why context matters
    
    ğŸš€ Now You Know:
    
    When you call model.generate_content(), you're:
    â€¢ Tokenizing text
    â€¢ Sending structured prompts
    â€¢ Triggering billions of calculations
    â€¢ Getting back intelligently generated text
    
    That "simple" one-liner does A LOT! ğŸ¤¯
    """)
    
    print("\n" + "="*70)
    print("  END OF DEEP DIVE")
    print("="*70 + "\n")


def main():
    """Run the deep demo"""
    try:
        deep_text_chat_demo()
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        print("\nMake sure you have:")
        print("  1. Set up GOOGLE_API_KEY in .env file")
        print("  2. Installed: pip install google-generativeai python-dotenv")


if __name__ == "__main__":
    main()
