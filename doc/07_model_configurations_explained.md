# Module 07 - Model Configurations - Detailed Code Explanation

This document explains every line of code in the Model Configurations module, with in-depth explanations of generation parameters and how they control AI output.

---

## ğŸ“Š Visual Overview: Generation Parameters

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           GENERATION PARAMETERS = AI OUTPUT CONTROLS            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Think of it like adjusting controls on a machine:

Default Settings:             Custom Settings:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Temperature  â”‚             â”‚ Temperature  â”‚
â”‚     1.0      â”‚             â”‚     0.2      â”‚ â† More predictable
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Top-P      â”‚             â”‚   Top-P      â”‚
â”‚     0.95     â”‚             â”‚     0.8      â”‚ â† More focused
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Max Tokens  â”‚             â”‚  Max Tokens  â”‚
â”‚    2048      â”‚             â”‚     100      â”‚ â† Shorter output
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                            â†“
Creative, varied             Consistent, precise
responses                    responses


HOW IT AFFECTS OUTPUT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Same Prompt: "Complete: The future of AI is..."

High Temperature (1.5):           Low Temperature (0.1):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "The future of AI is   â”‚       â”‚ "The future of AI is   â”‚
â”‚  absolutely mind-      â”‚       â”‚  expected to continue  â”‚
â”‚  blowing! Imagine      â”‚       â”‚  advancing in areas    â”‚
â”‚  sentient robots,      â”‚       â”‚  such as natural       â”‚
â”‚  flying cars, and      â”‚       â”‚  language processing   â”‚
â”‚  telepathic devices!"  â”‚       â”‚  and computer vision." â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      Creative âœ¨                      Predictable ğŸ“Š
      Varied                           Consistent
      Risky                            Safe
```

---

## ğŸŒ¡ï¸ Temperature Parameter Deep Dive

```
WHAT IS TEMPERATURE?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Temperature controls randomness in token selection.

How AI Chooses Next Word:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt: "The cat is very"                   â”‚
â”‚                                             â”‚
â”‚ AI's internal probabilities:                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚ â”‚ Word     â”‚ Prob %  â”‚                     â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚
â”‚ â”‚ cute     â”‚  45%    â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           â”‚
â”‚ â”‚ playful  â”‚  25%    â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              â”‚
â”‚ â”‚ fluffy   â”‚  15%    â”‚ â–ˆâ–ˆâ–ˆ                â”‚
â”‚ â”‚ hungry   â”‚  10%    â”‚ â–ˆâ–ˆ                 â”‚
â”‚ â”‚ loud     â”‚   5%    â”‚ â–ˆ                  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


TEMPERATURE = 0.0 (Deterministic):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Always picks highest probability:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Word     â”‚ Prob %  â”‚ Pick?  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ cute     â”‚  99%    â”‚   âœ…   â”‚ â† Always this one
â”‚ playful  â”‚   1%    â”‚   âŒ   â”‚
â”‚ fluffy   â”‚   0%    â”‚   âŒ   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output (every time): "The cat is very cute"
Use case: Factual answers, consistency


TEMPERATURE = 0.7 (Balanced):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Weighted random selection:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Word     â”‚ Prob %  â”‚ Pick?  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ cute     â”‚  45%    â”‚ Maybe  â”‚ â† Often
â”‚ playful  â”‚  25%    â”‚ Maybe  â”‚ â† Sometimes
â”‚ fluffy   â”‚  15%    â”‚ Maybe  â”‚ â† Rarely
â”‚ hungry   â”‚  10%    â”‚ Maybe  â”‚ â† Very rarely
â”‚ loud     â”‚   5%    â”‚ Maybe  â”‚ â† Almost never
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output (varies):
- "The cat is very cute"
- "The cat is very playful"
- "The cat is very fluffy"
Use case: Natural conversation


TEMPERATURE = 2.0 (Creative/Random):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Nearly equal probabilities:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Word     â”‚ Prob %  â”‚ Pick?  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ cute     â”‚  25%    â”‚ Equal  â”‚ â† All have
â”‚ playful  â”‚  22%    â”‚ chance â”‚   similar
â”‚ fluffy   â”‚  20%    â”‚        â”‚   chance
â”‚ hungry   â”‚  18%    â”‚        â”‚
â”‚ loud     â”‚  15%    â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output (unpredictable):
- "The cat is very philosophical"
- "The cat is very algebraic" 
- "The cat is very quantum"
Use case: Creative writing, brainstorming


VISUAL SCALE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Temperature:
0.0    0.5    1.0    1.5    2.0
â”‚â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”‚
â”‚      â”‚      â”‚      â”‚      â”‚
Boring         Normal         Chaos
Repetitive     Balanced       Nonsense
Predictable    Creative       Random
Factual        Engaging       Wild
```

---

## ğŸ¯ Top-P (Nucleus Sampling)

```
WHAT IS TOP-P?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Top-P limits word choices to smallest set that adds up to P probability.

Example: Top-P = 0.9 (90%)

Word Probabilities:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Word     â”‚ Prob %  â”‚ Cumulativeâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ happy    â”‚  40%    â”‚  40%      â”‚ âœ… Include
â”‚ joyful   â”‚  30%    â”‚  70%      â”‚ âœ… Include
â”‚ excited  â”‚  20%    â”‚  90%      â”‚ âœ… Include (hits 90%)
â”‚ thrilled â”‚   5%    â”‚  95%      â”‚ âŒ Exclude
â”‚ ecstatic â”‚   3%    â”‚  98%      â”‚ âŒ Exclude
â”‚ delightedâ”‚   2%    â”‚ 100%      â”‚ âŒ Exclude
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†‘ Only choose from top 3 words


VISUALIZATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Top-P = 0.5 (50%):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ happy 45% â”‚ âœ… 
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ sad 10%             â”‚ âŒ Cutoff here (55% total)
â”‚ â–ˆâ–ˆâ–ˆ angry 5%               â”‚ âŒ
â”‚ â–ˆâ–ˆ confused 3%             â”‚ âŒ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Only "happy" is considered!
Result: Very focused


Top-P = 0.95 (95%):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ happy 45% â”‚ âœ…
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ sad 20%         â”‚ âœ…
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ angry 15%           â”‚ âœ…
â”‚ â–ˆâ–ˆâ–ˆâ–ˆ confused 10%          â”‚ âœ…
â”‚ â–ˆâ–ˆ surprised 5%            â”‚ âœ… (Total: 95%)
â”‚ â–ˆ other 5%                 â”‚ âŒ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Many options considered!
Result: More diverse


TOP-P vs TOP-K:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Top-P (Dynamic):
  Adapts to probability distribution
  "Include words until 90% probability"
  
  High confidence: few words
  Low confidence: many words

Top-K (Fixed):
  Always considers K words
  "Always include top 40 words"
  
  Simple but inflexible
```

---

## ğŸ—ï¸ Code Structure Map

```
07_model_configurations.py
â”‚
â”œâ”€â”€ ğŸ“¦ IMPORTS
â”‚   â”œâ”€â”€ os
â”‚   â”œâ”€â”€ dotenv
â”‚   â””â”€â”€ google.generativeai
â”‚
â”œâ”€â”€ ğŸ”§ SETUP
â”‚   â”œâ”€â”€ load_dotenv()
â”‚   â””â”€â”€ genai.configure()
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 1: temperature_examples()
â”‚   â”œâ”€â”€ Low (0.1) - Deterministic
â”‚   â”œâ”€â”€ Medium (0.7) - Balanced
â”‚   â””â”€â”€ High (1.5) - Creative
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 2: top_p_examples()
â”‚   â”œâ”€â”€ Low (0.5) - Focused
â”‚   â”œâ”€â”€ Medium (0.8) - Moderate
â”‚   â””â”€â”€ High (0.95) - Diverse
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 3: top_k_examples()
â”‚   â”œâ”€â”€ Low (10) - Limited choices
â”‚   â”œâ”€â”€ Medium (40) - Balanced
â”‚   â””â”€â”€ High (100) - Many options
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 4: max_tokens_examples()
â”‚   â”œâ”€â”€ Short (50 tokens)
â”‚   â”œâ”€â”€ Medium (200 tokens)
â”‚   â””â”€â”€ Long (1000 tokens)
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 5: stop_sequences_examples()
â”‚   â”œâ”€â”€ Stop at punctuation
â”‚   â”œâ”€â”€ Stop at keyword
â”‚   â””â”€â”€ Multiple stop conditions
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 6: combined_configurations()
â”‚   â””â”€â”€ Mix multiple parameters
â”‚
â””â”€â”€ ğŸš€ MAIN MENU
    â””â”€â”€ Interactive parameter testing
```

---

## ğŸ“ Max Output Tokens

```
WHAT ARE TOKENS?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Tokens â‰ˆ Words (rough approximation)
- 1 token â‰ˆ 0.75 words (English)
- "Hello world" = 2 tokens
- "The quick brown fox" = 4 tokens

Token counting:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text: "AI is amazing!"             â”‚
â”‚ Tokens: ["AI", " is", " amazing", "!"]â”‚
â”‚ Count: 4 tokens                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


MAX_OUTPUT_TOKENS EXAMPLES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Prompt: "Write about dogs"

max_output_tokens = 20:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "Dogs are loyal companions   â”‚
â”‚  that bring joy to families  â”‚
â”‚  around the world."          â”‚ â† Stops here (20 tokens)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Use: Short answers, summaries


max_output_tokens = 100:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "Dogs are loyal companions   â”‚
â”‚  that bring joy to families  â”‚
â”‚  around the world. They come â”‚
â”‚  in many breeds, from tiny   â”‚
â”‚  Chihuahuas to large Great   â”‚
â”‚  Danes. Dogs require daily   â”‚
â”‚  exercise, proper nutrition, â”‚
â”‚  and regular vet visits..."  â”‚ â† Stops here (100 tokens)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Use: Paragraphs, explanations


max_output_tokens = 1000:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Full essay about dogs       â”‚
â”‚  with introduction, body     â”‚
â”‚  paragraphs, examples,       â”‚
â”‚  and conclusion]             â”‚
â”‚                              â”‚
â”‚ (Multiple paragraphs)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Use: Articles, detailed content


COST IMPLICATIONS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Higher tokens = Higher cost!

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokens   â”‚ Cost   â”‚ Use Caseâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 50       â”‚ $0.001 â”‚ Quick Q&Aâ”‚
â”‚ 200      â”‚ $0.004 â”‚ Paragraphâ”‚
â”‚ 1000     â”‚ $0.020 â”‚ Article â”‚
â”‚ 4000     â”‚ $0.080 â”‚ Essay   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âš ï¸ Set appropriate limits!
```

---

## ğŸ¨ Parameter Combinations for Different Use Cases

```
USE CASE MATRIX:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. FACTUAL Q&A (e.g., "What is Python?")
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ temperature: 0.1   (Consistent)     â”‚
â”‚ top_p: 0.8         (Focused)        â”‚
â”‚ max_tokens: 200    (Concise)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Goal: Accurate, reliable answers


2. CREATIVE WRITING (e.g., "Write a story")
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ temperature: 1.2   (Creative)       â”‚
â”‚ top_p: 0.95        (Diverse)        â”‚
â”‚ max_tokens: 1000   (Detailed)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Goal: Original, engaging content


3. CODE GENERATION (e.g., "Write Python function")
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ temperature: 0.2   (Predictable)    â”‚
â”‚ top_p: 0.85        (Focused)        â”‚
â”‚ max_tokens: 500    (Complete code)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Goal: Working, correct code


4. CASUAL CHAT (e.g., "How are you?")
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ temperature: 0.7   (Natural)        â”‚
â”‚ top_p: 0.9         (Varied)         â”‚
â”‚ max_tokens: 150    (Brief)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Goal: Engaging conversation


5. BRAINSTORMING (e.g., "Give me ideas")
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ temperature: 1.5   (Very creative)  â”‚
â”‚ top_p: 0.98        (Wide range)     â”‚
â”‚ max_tokens: 300    (Multiple ideas) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Goal: Diverse, unexpected ideas


6. SUMMARIZATION (e.g., "Summarize this")
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ temperature: 0.3   (Focused)        â”‚
â”‚ top_p: 0.85        (Key points)     â”‚
â”‚ max_tokens: 100    (Brief)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Goal: Concise, accurate summary
```

---

## âš™ï¸ Configuration Object Structure

```
PYTHON CONFIGURATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Method 1: Dictionary
config = {
    'temperature': 0.7,
    'top_p': 0.9,
    'top_k': 40,
    'max_output_tokens': 200
}

model = genai.GenerativeModel(
    'gemini-2.0-flash',
    generation_config=config
)


Method 2: Direct Parameters
model = genai.GenerativeModel(
    'gemini-2.0-flash',
    generation_config={
        'temperature': 0.7,
        'top_p': 0.9
    }
)


VISUAL STRUCTURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GenerativeModel                     â”‚
â”‚ â”œâ”€â”€ model_name: 'gemini-2.0-flash' â”‚
â”‚ â””â”€â”€ generation_config:              â”‚
â”‚     â”œâ”€â”€ temperature: 0.7            â”‚
â”‚     â”œâ”€â”€ top_p: 0.9                  â”‚
â”‚     â”œâ”€â”€ top_k: 40                   â”‚
â”‚     â”œâ”€â”€ max_output_tokens: 200      â”‚
â”‚     â”œâ”€â”€ stop_sequences: [...]       â”‚
â”‚     â””â”€â”€ candidate_count: 1          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


AVAILABLE PARAMETERS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Parameter          â”‚ Range    â”‚ Default     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ temperature        â”‚ 0.0-2.0  â”‚ 1.0         â”‚
â”‚ top_p              â”‚ 0.0-1.0  â”‚ 0.95        â”‚
â”‚ top_k              â”‚ 1-100+   â”‚ 40          â”‚
â”‚ max_output_tokens  â”‚ 1-8192   â”‚ 2048        â”‚
â”‚ stop_sequences     â”‚ list     â”‚ []          â”‚
â”‚ candidate_count    â”‚ 1-8      â”‚ 1           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Parameter Interaction Effects

```
TEMPERATURE + TOP-P INTERACTION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Both Low (temp=0.1, top_p=0.5):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Effect: VERY CONSISTENT        â”‚
â”‚ - Same output every time       â”‚
â”‚ - Predictable                  â”‚
â”‚ - Safe for production          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Both High (temp=1.5, top_p=0.98):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Effect: VERY CREATIVE          â”‚
â”‚ - Highly varied output         â”‚
â”‚ - Unpredictable                â”‚
â”‚ - Risk of nonsense             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Mixed (temp=0.2, top_p=0.95):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Effect: CONTROLLED VARIETY     â”‚
â”‚ - Mostly consistent            â”‚
â”‚ - Some variation               â”‚
â”‚ - Balanced approach            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


TOKENS + TEMPERATURE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Short + High Temp:
"Write a story (max 20 tokens, temp=1.5)"
â†’ "Alien robots danced quantum..." 
   (Creative but incomplete)

Long + Low Temp:
"Write a story (max 500 tokens, temp=0.2)"
â†’ Detailed, coherent, complete story
   (Predictable but thorough)


RECOMMENDATION MATRIX:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

         â”‚ Short Output â”‚ Long Output â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
Low Temp â”‚ Quick facts  â”‚ Documentationâ”‚
         â”‚ (Good)       â”‚ (Good)      â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
High Tempâ”‚ Random ideas â”‚ Creative    â”‚
         â”‚ (Risky)      â”‚ stories     â”‚
         â”‚              â”‚ (Good)      â”‚
```

---

## Module Documentation Block

```python
"""
07 - Model Configurations
==========================
```
**Explanation:** Module about generation parameters - the "knobs" that control HOW AI generates text.

```python
This module demonstrates generation parameters and how they affect AI output.
Students will learn:
- Temperature control (creativity vs consistency)
- Top-P (nucleus sampling)
- Top-K sampling
- Max output tokens
- Stop sequences
- How to choose parameters for different use cases
```
**Explanation:** Learning objectives. These parameters are CRITICAL - same prompt with different parameters produces wildly different outputs.

```python
Teaching Points:
- Generation parameters dramatically affect output
- Different tasks need different settings
- Experimentation is key
- Trade-offs between creativity and reliability
"""
```
**Explanation:** **KEY INSIGHT**: There's no "best" settings - it's about matching parameters to your use case. Factual Q&A needs different settings than creative writing.

---

## Import Statements

```python
import os
```
**Explanation:** File/directory operations.

```python
from dotenv import load_dotenv
```
**Explanation:** Environment variables for API keys.

```python
import google.generativeai as genai
```
**Explanation:** Google's Generative AI SDK.

```python
# Setup
load_dotenv()
genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
```
**Explanation:** Standard initialization.

---

## Section 1: Generation Parameters Overview

```python
# ============================================================================
# SECTION 1: Understanding Generation Parameters
# ============================================================================
```
**Explanation:** Educational overview of all parameters.

```python
def generation_parameters_overview():
    """
    Explain all generation parameters
    """
```
**Explanation:** Conceptual foundation - explains what each parameter does.

```python
    print("\n" + "=" * 60)
    print("SECTION 1: Understanding Generation Parameters")
    print("=" * 60)
    
    overview = """
    ğŸ›ï¸ GENERATION PARAMETERS:
    
    These parameters control HOW the AI generates text.
    Think of them as "knobs" you can tune for different effects.
```
**Explanation:** **FUNDAMENTAL CONCEPT**: These parameters don't change WHAT the AI knows, but HOW it selects words from its knowledge.

```python
    1ï¸âƒ£ TEMPERATURE (0.0 - 2.0)
    ===========================
    Controls randomness/creativity
```
**Explanation:** **MOST IMPORTANT PARAMETER**: Temperature controls the randomness of word selection.

```python
    Low (0.0 - 0.3): â„ï¸ Conservative
    â€¢ More focused and deterministic
    â€¢ Safer, more predictable outputs
    â€¢ Good for: Facts, code, structured data
    â€¢ Example: "2+2=" â†’ Always "4"
```
**Explanation:** **LOW TEMPERATURE**: AI picks the most probable next word almost always. At 0.0, it's nearly deterministic (same input = same output). Perfect for facts where you want consistency.

```python
    Medium (0.4 - 0.9): ğŸŒ¡ï¸ Balanced
    â€¢ Natural variation
    â€¢ Good general-purpose setting
    â€¢ Good for: Chat, Q&A, explanations
    â€¢ Example: "Hello!" â†’ Various greetings
```
**Explanation:** **MEDIUM TEMPERATURE**: Natural conversation range. Responses are consistent but not robotic. Default territory for most applications.

```python
    High (1.0 - 2.0): ğŸ”¥ Creative
    â€¢ Very diverse outputs
    â€¢ Can be unexpected/unusual
    â€¢ Good for: Creative writing, brainstorming
    â€¢ Example: "Once upon a time" â†’ Wild stories
```
**Explanation:** **HIGH TEMPERATURE**: AI considers less probable words. Can produce surprising, creative, sometimes weird results. At 2.0, outputs can be nonsensical.

```python
    2ï¸âƒ£ TOP-P / Nucleus Sampling (0.0 - 1.0)
    ========================================
    Controls diversity by probability mass
    
    How it works:
    â€¢ Model calculates probabilities for next token
    â€¢ Selects from smallest set that adds up to P
    â€¢ Higher P = more variety
```
**Explanation:** **TOP-P MECHANISM**: 
- AI calculates probability for each possible next word
- Sorts by probability (e.g., "the" 30%, "a" 20%, "an" 15%, etc.)
- Adds probabilities until reaching P (e.g., 0.9 = 90%)
- Samples only from that subset
- Example: If "the" (30%) + "a" (20%) + "an" (15%) + "this" (10%) + "that" (15%) = 90%, only consider those 5 words, ignore rest

```python
    Low (0.1 - 0.3):
    â€¢ Only most likely tokens
    â€¢ Very focused output
    
    Medium (0.4 - 0.8):
    â€¢ Balanced selection
    â€¢ Default: 0.95
    
    High (0.9 - 1.0):
    â€¢ Includes less likely options
    â€¢ Maximum diversity
```
**Explanation:** **TOP-P RANGES**: At 0.1, only considering top ~10% probability mass (very few words). At 0.95, considering words until they account for 95% of probability (many words).

```python
    3ï¸âƒ£ TOP-K (1 - 100+)
    ====================
    Limits to K most likely next tokens
    
    Low (1 - 10):
    â€¢ Very restricted choices
    â€¢ Highly focused
    
    Medium (10 - 40):
    â€¢ Balanced diversity
    â€¢ Default: 40
    
    High (40+):
    â€¢ Many options considered
    â€¢ More varied output
```
**Explanation:** **TOP-K VS TOP-P**: 
- **Top-K**: Fixed number of words (e.g., top 40 most probable)
- **Top-P**: Dynamic number based on cumulative probability
- Example: For "The cat ___", top-40 considers top 40 words. Top-p 0.9 might consider 20 words if top 20 account for 90% probability, or 60 words if they're more evenly distributed.

```python
    4ï¸âƒ£ MAX OUTPUT TOKENS
    =====================
    Maximum length of generated response
    
    â€¢ 1 token â‰ˆ 0.75 words (English)
    â€¢ Controls response length
    â€¢ Prevents overly long outputs
    â€¢ Default: 2048 for Gemini Pro
```
**Explanation:** **TOKENS VS WORDS**: Tokens are sub-word units. "tokenization" might be 3 tokens: ["token", "ization"]. Rough rule: 1 token â‰ˆ 0.75 words for English.

```python
    Examples:
    â€¢ 50 tokens: ~37 words (short answer)
    â€¢ 500 tokens: ~375 words (paragraph)
    â€¢ 2048 tokens: ~1500 words (article)
```
**Explanation:** Practical examples for sizing. 50 tokens = a sentence or two. 2048 = a full article.

```python
    5ï¸âƒ£ STOP SEQUENCES
    ==================
    Strings that stop generation
    
    Examples:
    â€¢ Stop at "\\n\\n" for paragraphs
    â€¢ Stop at "###" for sections
    â€¢ Stop at specific phrases
    
    Use cases:
    â€¢ Structured output
    â€¢ Format control
    â€¢ Template filling
```
**Explanation:** **STOP SEQUENCES**: Tell AI "stop generating when you hit this text". Useful for:
- Preventing overgeneration
- Structured formats (JSON, Markdown sections)
- Template completion (fill blanks, don't continue beyond)

```python
    ğŸ“Š PARAMETER INTERACTIONS:
    
    Temperature + Top-P:
    â€¢ Often used together
    â€¢ Both affect diversity
    â€¢ Start with one, adjust other
```
**Explanation:** **INTERACTION**: Temperature and top-p both control randomness, but differently. Temperature adjusts the probability distribution (flattens peaks), top-p limits the selection pool. They work together - high temp with low top-p creates focused creativity.

```python
    Top-P vs Top-K:
    â€¢ Alternative approaches
    â€¢ Top-P usually preferred
    â€¢ Can use both together
    """
```
**Explanation:** **TOP-P VS TOP-K**: Modern systems prefer top-p (more adaptive). Can use both: "Consider top-40 words (top-k), then sample from top 90% of those (top-p)".

```python
    print(overview)
```
**Explanation:** Display all concepts.

---

## Section 2: Temperature Comparison

```python
# ============================================================================
# SECTION 2: Temperature Examples
# ============================================================================
```
**Explanation:** Practical demonstration of temperature effects.

```python
def temperature_comparison():
    """
    Compare different temperature settings
    """
```
**Explanation:** Shows how temperature changes output dramatically.

```python
    print("\n" + "=" * 60)
    print("SECTION 2: Temperature Comparison")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    prompt = "Complete this sentence: The future of artificial intelligence is"
```
**Explanation:** Open-ended prompt where creativity matters - perfect for showing temperature effects.

```python
    temperatures = [0.0, 0.5, 1.0, 1.5]
```
**Explanation:** Range from ultra-conservative (0.0) to highly creative (1.5).

```python
    print(f"\nğŸ“ Prompt: '{prompt}'\n")
    print("ğŸ”¬ Testing different temperatures:\n")
    print("=" * 60)
    
    for temp in temperatures:
        print(f"\nğŸŒ¡ï¸  TEMPERATURE: {temp}")
        print("-" * 60)
```
**Explanation:** Loop through temperatures to compare.

```python
        # Configure generation settings
        generation_config = genai.types.GenerationConfig(
            temperature=temp,
            max_output_tokens=100
        )
```
**Explanation:** **KEY OBJECT**: `GenerationConfig` is how we pass parameters. It's a configuration object with all available settings.

```python
        # Generate 3 responses to show variation
        for i in range(3):
            response = model.generate_content(
                prompt,
                generation_config=generation_config
            )
            print(f"\n  Response {i+1}: {response.text}")
```
**Explanation:** **CRITICAL**: Generate 3 times with SAME settings to show variation. At temp 0.0, all 3 should be similar/identical. At temp 1.5, all 3 should be quite different.

```python
        print()
        
        if temp == 0.0:
            print("  ğŸ’¡ Notice: Responses are very similar/identical")
        elif temp == 1.5:
            print("  ğŸ’¡ Notice: Responses are quite diverse and creative")
```
**Explanation:** Educational hints to help students see the pattern.

---

## Section 3: Top-P Examples

```python
# ============================================================================
# SECTION 3: Top-P (Nucleus Sampling)
# ============================================================================
```
**Explanation:** Demonstrating nucleus sampling.

```python
def top_p_examples():
    """
    Demonstrate top-p sampling
    """
```
**Explanation:** Shows how top-p affects word selection pool.

```python
    print("\n" + "=" * 60)
    print("SECTION 3: Top-P (Nucleus Sampling)")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    prompt = "Write a creative opening line for a science fiction story."
```
**Explanation:** Creative task where top-p effects are visible.

```python
    top_p_values = [0.1, 0.5, 0.95]
```
**Explanation:** Range from very restricted (0.1) to very diverse (0.95).

```python
    print(f"\nğŸ“ Prompt: '{prompt}'\n")
    print("=" * 60)
    
    for top_p in top_p_values:
        print(f"\nğŸ¯ TOP-P: {top_p}")
        print("-" * 60)
        
        generation_config = genai.types.GenerationConfig(
            top_p=top_p,
            temperature=0.9,  # Keep temperature constant
            max_output_tokens=80
        )
```
**Explanation:** **IMPORTANT**: Keep temperature constant (0.9) to isolate top-p's effect. Scientific approach - change one variable at a time.

```python
        # Generate multiple responses
        for i in range(2):
            response = model.generate_content(
                prompt,
                generation_config=generation_config
            )
            print(f"\n  Response {i+1}:\n  {response.text}")
        
        print()
```
**Explanation:** Generate twice to show variation at each top-p level.

---

## Section 4: Top-K Examples

```python
# ============================================================================
# SECTION 4: Top-K Sampling
# ============================================================================
```
**Explanation:** Demonstrating fixed-number sampling.

```python
def top_k_examples():
    """
    Demonstrate top-k sampling
    """
```
**Explanation:** Shows how limiting to K words affects diversity.

```python
    print("\n" + "=" * 60)
    print("SECTION 4: Top-K Sampling")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    prompt = "Name a programming language: "
```
**Explanation:** Simple prompt where the variety of answers shows top-k's effect.

```python
    top_k_values = [1, 10, 40]
```
**Explanation:** 
- **top_k=1**: Only most probable word (always same answer)
- **top_k=10**: Choose from 10 most probable words
- **top_k=40**: Choose from 40 most probable words

```python
    print(f"\nğŸ“ Prompt: '{prompt}'\n")
    print("=" * 60)
    
    for top_k in top_k_values:
        print(f"\nğŸ”¢ TOP-K: {top_k}")
        print("-" * 60)
        
        generation_config = genai.types.GenerationConfig(
            top_k=top_k,
            temperature=1.0,
            max_output_tokens=50
        )
```
**Explanation:** Temperature 1.0 ensures randomness; top-k controls the pool size.

```python
        # Generate multiple responses to see variety
        responses = []
        for i in range(5):
            response = model.generate_content(
                prompt,
                generation_config=generation_config
            )
            responses.append(response.text.split()[0])  # Get first word
```
**Explanation:** Generate 5 times, extract first word (the programming language name). `.split()[0]` splits text by whitespace and gets first element.

```python
        print(f"  Responses: {responses}")
        print(f"  Unique answers: {len(set(responses))}")
```
**Explanation:** **CLEVER ANALYSIS**: Show all responses AND count unique ones. `set()` removes duplicates, so `len(set(...))` shows variety. Top-k=1 should give 1 unique answer (always same). Top-k=40 should give more variety.

---

## Section 5: Max Output Tokens

```python
# ============================================================================
# SECTION 5: Max Output Tokens
# ============================================================================
```
**Explanation:** Controlling response length.

```python
def max_tokens_examples():
    """
    Control response length with max tokens
    """
```
**Explanation:** Shows how token limits affect response length.

```python
    print("\n" + "=" * 60)
    print("SECTION 5: Max Output Tokens")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    prompt = "Explain what machine learning is."
```
**Explanation:** Topic that can be explained briefly or extensively - perfect for showing length control.

```python
    token_limits = [30, 100, 500]
```
**Explanation:** Range from very short (30 â‰ˆ 22 words) to long (500 â‰ˆ 375 words).

```python
    print(f"\nğŸ“ Prompt: '{prompt}'\n")
    print("=" * 60)
    
    for max_tokens in token_limits:
        print(f"\nğŸ“ MAX TOKENS: {max_tokens} (~{int(max_tokens * 0.75)} words)")
        print("-" * 60)
```
**Explanation:** Show estimated word count using 0.75 conversion ratio.

```python
        generation_config = genai.types.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=0.7
        )
        
        response = model.generate_content(
            prompt,
            generation_config=generation_config
        )
```
**Explanation:** Generate with different length limits.

```python
        response_text = response.text
        word_count = len(response_text.split())
```
**Explanation:** Count actual words. `.split()` splits on whitespace, `len()` counts elements.

```python
        print(response_text)
        print(f"\n  ğŸ“Š Actual words: {word_count}")
        print()
```
**Explanation:** Show response and actual word count to verify tokenâ†’word conversion.

---

## Section 6: Stop Sequences

```python
# ============================================================================
# SECTION 6: Stop Sequences
# ============================================================================
```
**Explanation:** Using stop sequences for control.

```python
def stop_sequences_examples():
    """
    Use stop sequences to control output
    """
```
**Explanation:** Shows how to make AI stop at specific points.

```python
    print("\n" + "=" * 60)
    print("SECTION 6: Stop Sequences")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    print("\nğŸ’¡ Stop sequences tell the model when to stop generating")
    print("Useful for structured output and format control\n")
```
**Explanation:** **USE CASES**: 
- Stop at paragraph breaks
- Stop at section markers
- Stop when completing templates

```python
    # Example 1: Stop at line break
    print("1ï¸âƒ£ Stop at double newline (paragraph boundary):")
    print("-" * 60)
    
    prompt1 = "Write about Python programming.\n\n"
```
**Explanation:** Prompt ends with `\n\n` to establish pattern, but we'll stop generation at first double newline.

```python
    generation_config1 = genai.types.GenerationConfig(
        stop_sequences=["\n\n"],
        max_output_tokens=200
    )
```
**Explanation:** **STOP_SEQUENCES**: List of strings that trigger stopping. When AI generates any of these strings, it stops immediately.

```python
    response1 = model.generate_content(
        prompt1,
        generation_config=generation_config1
    )
    
    print(response1.text)
    print("\nğŸ“Œ Generation stopped at paragraph break")
```
**Explanation:** AI stops after first paragraph (when it would naturally write `\n\n`).

```python
    # Example 2: Stop at specific marker
    print("\n\n2ï¸âƒ£ Stop at specific marker (###):")
    print("-" * 60)
    
    prompt2 = "List three programming languages:\n1."
```
**Explanation:** Started a list, AI will continue numbering.

```python
    generation_config2 = genai.types.GenerationConfig(
        stop_sequences=["###", "\n\n"],
        max_output_tokens=200
    )
```
**Explanation:** Multiple stop sequences: stop at "###" OR double newline. Whichever comes first.

```python
    response2 = model.generate_content(
        prompt2,
        generation_config=generation_config2
    )
    
    print(response2.text)
```
**Explanation:** AI completes the list and stops at natural paragraph break or if it generates "###".

---

## Section 7: Use Case Configurations

```python
# ============================================================================
# SECTION 7: Use Case Configurations
# ============================================================================
```
**Explanation:** Recommended settings for real applications.

```python
def use_case_configurations():
    """
    Recommended configurations for different use cases
    """
```
**Explanation:** **MOST PRACTICAL SECTION**: Shows proven configurations for common scenarios.

```python
    print("\n" + "=" * 60)
    print("SECTION 7: Use Case Configurations")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    use_cases = {
```
**Explanation:** Dictionary of use cases with their ideal configurations.

```python
        "Factual Q&A": {
            "config": genai.types.GenerationConfig(
                temperature=0.1,
                top_p=0.8,
                max_output_tokens=150
            ),
            "prompt": "What is the capital of France?",
            "rationale": "Low temperature for factual accuracy"
        },
```
**Explanation:** **FACTUAL Q&A**: 
- **Temp 0.1**: Very low for consistency (facts shouldn't vary)
- **Top_p 0.8**: Moderate pool (not too restrictive)
- **Max 150**: Short, direct answers
- Use when: User asks questions with definitive answers

```python
        "Creative Writing": {
            "config": genai.types.GenerationConfig(
                temperature=1.2,
                top_p=0.95,
                max_output_tokens=300
            ),
            "prompt": "Write a creative opening for a mystery novel.",
            "rationale": "High temperature for creativity and variety"
        },
```
**Explanation:** **CREATIVE WRITING**:
- **Temp 1.2**: High for unique, surprising outputs
- **Top_p 0.95**: Large pool for diverse word choices
- **Max 300**: Longer for story development
- Use when: Content should be unique and imaginative

```python
        "Code Generation": {
            "config": genai.types.GenerationConfig(
                temperature=0.2,
                top_p=0.85,
                max_output_tokens=500
            ),
            "prompt": "Write a Python function to calculate factorial.",
            "rationale": "Low temperature for correct, consistent code"
        },
```
**Explanation:** **CODE GENERATION**:
- **Temp 0.2**: Low for correct syntax (code must work)
- **Top_p 0.85**: Balanced (some variety in style, but reliable)
- **Max 500**: Enough for complete functions
- Use when: Generating code that must be functional

```python
        "Brainstorming": {
            "config": genai.types.GenerationConfig(
                temperature=1.0,
                top_p=0.9,
                max_output_tokens=200
            ),
            "prompt": "Generate 5 unique business ideas for a coffee shop.",
            "rationale": "Balanced for diverse but coherent ideas"
        },
```
**Explanation:** **BRAINSTORMING**:
- **Temp 1.0**: High-medium for variety
- **Top_p 0.9**: Good diversity
- **Max 200**: Multiple ideas, not too long
- Use when: Need multiple creative but sensible options

```python
        "Chat/Conversation": {
            "config": genai.types.GenerationConfig(
                temperature=0.7,
                top_p=0.9,
                max_output_tokens=250
            ),
            "prompt": "Hi! How's your day going?",
            "rationale": "Medium temperature for natural conversation"
        }
    }
```
**Explanation:** **CHAT/CONVERSATION**:
- **Temp 0.7**: Sweet spot for natural but not repetitive
- **Top_p 0.9**: Natural language variety
- **Max 250**: Conversational length
- Use when: Building chatbots

```python
    print("\nğŸ“‹ Recommended Configurations by Use Case:\n")
    
    for use_case, settings in use_cases.items():
        print("=" * 60)
        print(f"ğŸ¯ {use_case.upper()}")
        print("=" * 60)
        
        config = settings['config']
        print(f"\nâš™ï¸  Configuration:")
        print(f"   â€¢ Temperature: {config.temperature}")
        print(f"   â€¢ Top-P: {config.top_p}")
        print(f"   â€¢ Max Tokens: {config.max_output_tokens}")
        print(f"\nğŸ’¡ Rationale: {settings['rationale']}")
```
**Explanation:** Display configuration details and reasoning.

```python
        print(f"\nğŸ“ Example Prompt: '{settings['prompt']}'")
        print(f"\nğŸ¤– Response:")
        print("-" * 60)
        
        response = model.generate_content(
            settings['prompt'],
            generation_config=config
        )
        
        print(response.text)
        print()
```
**Explanation:** **DEMONSTRATE**: Actually run each configuration to show real results. Students see theory in action.

---

## Section 8: Interactive Testing

```python
# ============================================================================
# SECTION 8: Interactive Parameter Testing
# ============================================================================
```
**Explanation:** Hands-on experimentation.

```python
def interactive_parameter_testing():
    """
    Let users experiment with parameters
    """
```
**Explanation:** Interactive playground for learning by doing.

```python
    print("\n" + "=" * 60)
    print("SECTION 8: Interactive Parameter Testing")
    print("=" * 60)
    print("\nExperiment with different parameter combinations!")
    print("Type 'quit' to exit\n")
    
    model = genai.GenerativeModel('gemini-pro')
    
    while True:
        print("=" * 60)
```
**Explanation:** Infinite loop for repeated experimentation.

```python
        # Get prompt
        prompt = input("\nEnter your prompt (or 'quit'): ").strip()
        if prompt.lower() in ['quit', 'exit', 'q']:
            break
        
        if not prompt:
            continue
```
**Explanation:** Get user prompt, allow quitting, skip empty inputs.

```python
        # Get parameters
        try:
            temp = float(input("Temperature (0.0-2.0, default 0.7): ").strip() or "0.7")
            top_p = float(input("Top-P (0.0-1.0, default 0.95): ").strip() or "0.95")
            max_tokens = int(input("Max tokens (default 200): ").strip() or "200")
```
**Explanation:** **INPUT WITH DEFAULTS**: `input(...).strip() or "0.7"` means if user hits Enter (empty string), use default. `.strip()` removes whitespace, empty string is falsy in Python, so `or` returns the default.

```python
            # Create configuration
            generation_config = genai.types.GenerationConfig(
                temperature=temp,
                top_p=top_p,
                max_output_tokens=max_tokens
            )
```
**Explanation:** Build config from user inputs.

```python
            # Generate response
            print("\nâ³ Generating...")
            response = model.generate_content(
                prompt,
                generation_config=generation_config
            )
            
            print("\nğŸ¤– Response:")
            print("-" * 60)
            print(response.text)
            print("-" * 60)
            
            print(f"\nğŸ“Š Used: temp={temp}, top_p={top_p}, max_tokens={max_tokens}\n")
```
**Explanation:** Generate and display result with parameters used. Helps users learn what works.

```python
        except ValueError as e:
            print(f"âš ï¸  Invalid input: {e}")
        except Exception as e:
            print(f"âŒ Error: {e}")
```
**Explanation:** Error handling for invalid inputs (non-numeric) or API failures.

---

## Section 9: Parameter Combinations

```python
# ============================================================================
# SECTION 9: Parameter Combinations
# ============================================================================
```
**Explanation:** Showing how parameters work together.

```python
def parameter_combinations():
    """
    Show how parameters work together
    """
```
**Explanation:** Demonstrates synergistic effects of multiple parameters.

```python
    print("\n" + "=" * 60)
    print("SECTION 9: Parameter Combinations")
    print("=" * 60)
    
    model = genai.GenerativeModel('gemini-pro')
    
    prompt = "The best thing about learning to code is"
```
**Explanation:** Open-ended prompt where different combinations show distinct effects.

```python
    combinations = [
        {
            "name": "Ultra Conservative",
            "config": genai.types.GenerationConfig(
                temperature=0.0,
                top_p=0.5,
                top_k=10
            )
        },
```
**Explanation:** **ULTRA CONSERVATIVE**: All parameters set for maximum focus. Temp 0.0 (deterministic) + low top_p (small pool) + low top_k (few options) = extremely predictable.

```python
        {
            "name": "Balanced & Reliable",
            "config": genai.types.GenerationConfig(
                temperature=0.7,
                top_p=0.9,
                top_k=40
            )
        },
```
**Explanation:** **BALANCED**: Default-like settings. Works well for most applications.

```python
        {
            "name": "Maximum Creativity",
            "config": genai.types.GenerationConfig(
                temperature=1.5,
                top_p=0.98,
                top_k=100
            )
        }
    ]
```
**Explanation:** **MAXIMUM CREATIVITY**: All parameters set for maximum diversity. High temp (random) + high top_p (large pool) + high top_k (many options) = very creative/unpredictable.

```python
    print(f"\nğŸ“ Prompt: '{prompt}'\n")
    print("=" * 60)
    
    for combo in combinations:
        print(f"\nâš™ï¸  {combo['name'].upper()}")
        config = combo['config']
        print(f"   Temperature: {config.temperature}, Top-P: {config.top_p}, Top-K: {config.top_k}")
        print("-" * 60)
        
        response = model.generate_content(
            prompt,
            generation_config=config
        )
        
        print(response.text)
        print()
```
**Explanation:** Generate with each combination to show how they interact.

---

## Section 10: Best Practices

```python
# ============================================================================
# SECTION 10: Best Practices
# ============================================================================
```
**Explanation:** Production guidelines.

```python
def configuration_best_practices():
    """
    Best practices for using generation parameters
    """
```
**Explanation:** Accumulated wisdom for real-world usage.

```python
    print("\n" + "=" * 60)
    print("SECTION 10: Best Practices")
    print("=" * 60)
    
    practices = """
    âœ… BEST PRACTICES:
    
    1. START WITH DEFAULTS
       â€¢ Temperature: 0.7
       â€¢ Top-P: 0.95
       â€¢ Top-K: 40
       â€¢ Then adjust based on results
```
**Explanation:** **START SIMPLE**: Don't over-optimize prematurely. Begin with defaults, measure results, then tune.

```python
    2. ADJUST ONE AT A TIME
       â€¢ Change temperature first
       â€¢ Then fine-tune top-p if needed
       â€¢ Easier to understand effects
```
**Explanation:** **SCIENTIFIC METHOD**: Change one variable at a time so you know what causes what. Temperature has biggest impact, start there.

```python
    3. TEST MULTIPLE TIMES
       â€¢ Parameters create variation
       â€¢ Run same prompt 3-5 times
       â€¢ Evaluate consistency
```
**Explanation:** **STATISTICAL VALIDITY**: Single run isn't enough. Parameters introduce randomness - need multiple samples to judge quality.

```python
    4. MATCH TO USE CASE
       â€¢ Facts/Code: Low temperature (0.0-0.3)
       â€¢ Chat: Medium temperature (0.5-0.9)
       â€¢ Creative: High temperature (1.0-1.5)
```
**Explanation:** **CONTEXT MATTERS**: No universal "best" settings. Match to your specific need.

```python
    5. SET APPROPRIATE LIMITS
       â€¢ Max tokens prevents runaway generation
       â€¢ Stop sequences for structured output
       â€¢ Balance cost vs quality
```
**Explanation:** **COST CONTROL**: Longer outputs cost more. Set limits to prevent unexpected bills.

```python
    âš ï¸ COMMON MISTAKES:
    
    1. Temperature too high for facts
       â€¢ Results in hallucinations
       â€¢ Inconsistent answers
       â€¢ Use < 0.3 for factual content
```
**Explanation:** **CRITICAL ERROR**: High temperature on facts = AI makes up wrong answers confidently. "What's 2+2?" with temp 1.5 might say "5" because it's considering unlikely options.

```python
    2. Temperature too low for creativity
       â€¢ Repetitive outputs
       â€¢ Boring conversations
       â€¢ Use > 0.8 for creative tasks
```
**Explanation:** **BORING OUTPUTS**: Temp 0.0 for creative writing = all stories sound the same.

```python
    3. Ignoring token limits
       â€¢ Unexpected truncation
       â€¢ Incomplete responses
       â€¢ Set appropriate max_tokens
```
**Explanation:** **TRUNCATION ISSUES**: If response cuts off mid-sentence, you hit token limit. Always set max_tokens appropriately.

```python
    4. Not testing variations
       â€¢ Parameters affect each prompt differently
       â€¢ Always test before production
       â€¢ Monitor outputs over time
```
**Explanation:** **TEST IN PRODUCTION-LIKE CONDITIONS**: What works on test prompts might not work on real user inputs.

```python
    ğŸ¯ QUICK REFERENCE:
    
    Task                 | Temp  | Top-P | Max Tokens
    ---------------------|-------|-------|------------
    Factual Q&A          | 0.1   | 0.8   | 150
    Code Generation      | 0.2   | 0.85  | 500
    Chat/Conversation    | 0.7   | 0.9   | 250
    Creative Writing     | 1.2   | 0.95  | 500
    Brainstorming        | 1.0   | 0.95  | 200
    Data Extraction      | 0.0   | 0.7   | 100
    Summarization        | 0.3   | 0.85  | 300
    Translation          | 0.3   | 0.9   | Variable
```
**Explanation:** **CHEAT SHEET**: Copy-paste ready configurations for common tasks. Starting point for your tuning.

```python
    ğŸ’¡ PRO TIPS:
    
    â€¢ Temperature 0 is deterministic (mostly)
    â€¢ Higher temperature â‰  always better
    â€¢ Top-p and top-k are alternatives, not required together
    â€¢ Monitor costs - longer outputs cost more
    â€¢ Save successful configurations
    â€¢ Document your parameter choices
    â€¢ A/B test different settings
    â€¢ User feedback is invaluable
    """
```
**Explanation:** Advanced tips from production experience.

```python
    print(practices)
```
**Explanation:** Display all best practices.

---

## Main Function

```python
# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """
    Main function with menu
    """
    print("\n")
    print("ğŸ“ " + "=" * 58 + " ğŸ“")
    print("  GENERATIVE AI SESSION - MODULE 7: MODEL CONFIGURATIONS")
    print("ğŸ“ " + "=" * 58 + " ğŸ“")
```
**Explanation:** Standard main setup.

```python
    menu = """
    Choose a section to run:
    
    1. Generation Parameters Overview
    2. Temperature Comparison
    3. Top-P (Nucleus Sampling)
    4. Top-K Sampling
    5. Max Output Tokens
    6. Stop Sequences
    7. Use Case Configurations
    8. Interactive Parameter Testing
    9. Parameter Combinations
    10. Best Practices
    
    all - Run all (except interactive)
    quit - Exit
    
    """
```
**Explanation:** Menu with 10 sections.

```python
    while True:
        print(menu)
        choice = input("Your choice: ").strip().lower()
        
        if choice in ['quit', 'q', 'exit']:
            print("ğŸ‘‹ Goodbye!")
            break
        elif choice == '1':
            generation_parameters_overview()
        # ... [rest of choices] ...
```
**Explanation:** Standard menu loop.

```python
        elif choice == 'all':
            generation_parameters_overview()
            temperature_comparison()
            top_p_examples()
            top_k_examples()
            max_tokens_examples()
            stop_sequences_examples()
            use_case_configurations()
            parameter_combinations()
            configuration_best_practices()
            print("\nâœ… All sections completed!")
            print("ğŸ’¡ Try section 8 separately for interactive testing")
            break
```
**Explanation:** 'all' runs all except interactive (section 8).

```python
        else:
            print("âš ï¸  Invalid choice. Please try again.")
```
**Explanation:** Invalid input handling.

---

## Script Entry Point

```python
if __name__ == "__main__":
    main()
    
    # Teaching Questions:
    # 1. How does temperature affect output?
    # 2. When should you use low vs high temperature?
    # 3. What's the difference between top-p and top-k?
```
**Explanation:** Entry point with discussion questions.

---

## Summary

This module teaches **generation parameters** - the controls that determine HOW AI selects words when generating text.

### The Five Key Parameters:

1. **Temperature (0.0-2.0)**: Controls randomness
   - **How it works**: Adjusts probability distribution (flattens peaks at higher temps)
   - **Low (0.0-0.3)**: Deterministic, consistent, safe
   - **Medium (0.4-0.9)**: Natural, balanced
   - **High (1.0-2.0)**: Creative, diverse, unpredictable

2. **Top-P / Nucleus Sampling (0.0-1.0)**: Controls diversity by probability mass
   - **How it works**: Samples from smallest set of words that sum to P probability
   - **Example**: At 0.9, consider words until their probabilities add to 90%
   - **Adaptive**: Considers more words when probabilities are flat, fewer when peaked

3. **Top-K (1-100+)**: Limits to K most likely words
   - **How it works**: Fixed number of candidate words
   - **Example**: Top-40 always considers exactly 40 words
   - **Fixed**: Same count regardless of probability distribution

4. **Max Output Tokens**: Controls response length
   - **Conversion**: 1 token â‰ˆ 0.75 words (English)
   - **Examples**: 50 tokens = short answer, 2048 = article

5. **Stop Sequences**: Strings that halt generation
   - **Use cases**: Paragraph breaks (`\n\n`), section markers (`###`), template boundaries

### Critical Concepts:

- **Parameters interact**: Temperature + top-p both affect diversity differently
- **Context-dependent**: Different tasks need different settings
- **No universal best**: Must match to use case
- **Trade-offs**: Creativity vs consistency, length vs cost

### Quick Reference:

| Task | Temp | Top-P | Max Tokens | Why |
|------|------|-------|------------|-----|
| **Factual Q&A** | 0.1 | 0.8 | 150 | Accuracy matters |
| **Code** | 0.2 | 0.85 | 500 | Must be correct |
| **Chat** | 0.7 | 0.9 | 250 | Natural variation |
| **Creative** | 1.2 | 0.95 | 500 | Unique outputs |
| **Brainstorm** | 1.0 | 0.95 | 200 | Diverse ideas |

### Best Practices:

1. **Start with defaults** (temp 0.7, top-p 0.95)
2. **Adjust one parameter at a time**
3. **Test multiple times** (randomness requires statistical sampling)
4. **Match to use case** (facts = low temp, creativity = high temp)
5. **Monitor costs** (longer = more expensive)

### Common Mistakes:

- âŒ High temp for facts â†’ Hallucinations
- âŒ Low temp for creativity â†’ Boring, repetitive
- âŒ Not setting token limits â†’ Unexpected truncation
- âŒ Not testing variations â†’ Production surprises

### The Mental Model:

Think of text generation as:
1. **AI calculates probability for every possible next word**
2. **Top-k/top-p filter the candidate pool**
3. **Temperature adjusts the probability distribution**
4. **AI randomly samples from the adjusted pool**
5. **Repeats until reaching max tokens or stop sequence**

**Key insight**: Parameters don't change what the AI "knows" - they change how it selects from what it knows. Same knowledge, different selection process = dramatically different outputs!