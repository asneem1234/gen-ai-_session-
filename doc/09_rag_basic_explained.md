# Module 09 - RAG (Retrieval-Augmented Generation) - Basic - Detailed Code Explanation

This document explains every line of code in the RAG Basic module, with in-depth explanations of how to build intelligent search and question-answering systems.

---

## ğŸ“Š Visual Overview: RAG Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RAG = AI + YOUR DOCUMENTS = SMART ANSWERS            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

THE PROBLEM RAG SOLVES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

WITHOUT RAG:
User: "What's in our Q3 report?"
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Model                    â”‚
â”‚ (Only knows training data)  â”‚  â†’ "I don't have access
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     to your Q3 report"
                                    âŒ Can't help


WITH RAG:
User: "What's in our Q3 report?"
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. SEARCH your documents             â”‚ â†’ Find Q3 report
â”‚ 2. RETRIEVE relevant sections        â”‚ â†’ Get key info
â”‚ 3. INJECT into AI prompt             â”‚ â†’ Add context
â”‚ 4. GENERATE answer                   â”‚ â†’ "Q3 revenue
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    was $5M..."
                                           âœ… Accurate!


FULL RAG PIPELINE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RAG SYSTEM FLOW                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PHASE 1: INDEXING (One-time setup)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Your Documents:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ company_docs.pdf               â”‚
â”‚ â€¢ product_specs.docx             â”‚
â”‚ â€¢ q3_report.txt                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Load & Chunk           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚  Break into small pieces:       â”‚
â”‚                                 â”‚
â”‚  "Q3 revenue was $5M..."  â”€â”€â”€â”€â–º Chunk 1
â”‚  "Customer count: 1,200"  â”€â”€â”€â”€â–º Chunk 2
â”‚  "New product launch..."  â”€â”€â”€â”€â–º Chunk 3
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Create Embeddings      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚  Convert text â†’ numbers:        â”‚
â”‚                                 â”‚
â”‚  Chunk 1 â†’ [0.23, 0.87, ...]   â”‚
â”‚  Chunk 2 â†’ [0.45, 0.12, ...]   â”‚
â”‚  Chunk 3 â†’ [0.91, 0.33, ...]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Store in Database      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚  (Vector Store)                 â”‚
â”‚                                 â”‚
â”‚  Chunk 1 + Embedding 1          â”‚
â”‚  Chunk 2 + Embedding 2          â”‚
â”‚  Chunk 3 + Embedding 3          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
        âœ… Ready for queries!


PHASE 2: QUERYING (Every user question)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

User Question:
"What was Q3 revenue?"
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Embed Question         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚
â”‚  "What was Q3 revenue?"         â”‚
â”‚       â†“                         â”‚
â”‚  [0.25, 0.89, 0.44, ...]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Similarity Search      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚  Compare question embedding     â”‚
â”‚  to all chunk embeddings        â”‚
â”‚                                 â”‚
â”‚  Question: [0.25, 0.89, ...]    â”‚
â”‚     vs                          â”‚
â”‚  Chunk 1:  [0.23, 0.87, ...]  âœ… 95% similar
â”‚  Chunk 2:  [0.45, 0.12, ...]    78% similar
â”‚  Chunk 3:  [0.91, 0.33, ...]    42% similar
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 6: Retrieve Top Chunks    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚
â”‚  Get most relevant:             â”‚
â”‚                                 â”‚
â”‚  âœ… Chunk 1: "Q3 revenue $5M"   â”‚
â”‚  âœ… Chunk 2: "Customer: 1,200"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 7: Build Enhanced Prompt  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚                                 â”‚
â”‚  Context: "Q3 revenue was $5M.  â”‚
â”‚            Customer count 1,200"â”‚
â”‚                                 â”‚
â”‚  Question: "What was Q3 revenue?"â”‚
â”‚                                 â”‚
â”‚  Instruction: "Answer using     â”‚
â”‚  only the context provided"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 8: AI Generation          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚
â”‚  AI reads context + answers:    â”‚
â”‚                                 â”‚
â”‚  "Based on the Q3 report,       â”‚
â”‚   revenue was $5M with 1,200    â”‚
â”‚   customers."                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
        ğŸ“ Answer delivered!
```

---

## ğŸ” What Are Embeddings?

```
EMBEDDINGS = MEANING AS NUMBERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Text cannot be compared mathematically.
Numbers can be compared mathematically.
â†’ Convert text to numbers = embeddings!


CONCEPTUAL EXAMPLE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Words:         Embeddings (simplified):
"cat"    â†’     [0.9, 0.1, 0.3]  â† High "animal" score
"dog"    â†’     [0.8, 0.2, 0.4]  â† High "animal" score
"car"    â†’     [0.1, 0.9, 0.7]  â† High "vehicle" score
"truck"  â†’     [0.2, 0.8, 0.6]  â† High "vehicle" score

Similarity:
"cat" vs "dog"   â†’ Very similar embeddings âœ…
"cat" vs "car"   â†’ Different embeddings âŒ


REAL EMBEDDINGS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Actual embeddings are MUCH longer vectors:
- Google's embedding model: 768 dimensions
- OpenAI ada-002: 1536 dimensions

Example (simplified):
"Python programming" â†’ 
[0.234, -0.891, 0.456, 0.123, -0.567, 0.789, ...]
  â†‘      â†‘       â†‘      â†‘       â†‘       â†‘
768 numbers total (we show 6)


SEMANTIC SIMILARITY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Similar meaning = Similar numbers:

"Python coding"        [0.23, 0.45, 0.67, ...]
"Python programming"   [0.25, 0.43, 0.69, ...]
                       â†‘ Very close numbers!

"Pizza recipe"         [0.91, -0.34, 0.12, ...]
"Python programming"   [0.25, 0.43, 0.69, ...]
                       â†‘ Different numbers


DISTANCE CALCULATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Cosine Similarity (most common):

Text A: [0.5, 0.8, 0.3]
Text B: [0.6, 0.7, 0.4]
         â†“
Similarity Score: 0.95 (95% similar)

Text A: [0.5, 0.8, 0.3]
Text C: [0.1, 0.2, 0.9]
         â†“
Similarity Score: 0.42 (42% similar)


VISUAL REPRESENTATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Imagine 2D space (real embeddings are 768D):

        High "Animal"
              â†‘
              â”‚
     ğŸ± cat   â”‚   ğŸ• dog
              â”‚
              â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º High "Technology"
              â”‚
              â”‚
     ğŸš— car   â”‚   ğŸšš truck
              â”‚
              â†“

Words with similar meanings cluster together!
```

---

## ğŸ“¦ Document Chunking Strategies

```
WHY CHUNK DOCUMENTS?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Problem: Documents are too large
- Entire book = too much context
- Single paragraph = too little context
- Need: Just-right sized pieces âœ…

Solution: Split into chunks!


CHUNKING STRATEGY COMPARISON:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. FIXED SIZE (Simple)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Split every N characters/words
   
   Example: 500 characters per chunk
   
   Document: "The quick brown fox jumps over..."
            â†“
   Chunk 1: "The quick brown fox jumps over..." (500 chars)
   Chunk 2: "...the lazy dog. The fox was very..." (500 chars)
   
   âœ… Simple, predictable
   âŒ May cut mid-sentence
   âŒ No semantic awareness


2. SENTENCE-BASED (Better)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Keep complete sentences together
   
   Example: 3-5 sentences per chunk
   
   Document: "Python is great. It's easy to learn.
              It has many libraries. It's very popular."
            â†“
   Chunk 1: "Python is great. It's easy to learn.
             It has many libraries."
   Chunk 2: "It's very popular. [next sentences...]"
   
   âœ… Natural boundaries
   âœ… Complete thoughts
   âŒ Variable chunk sizes


3. PARAGRAPH-BASED (Good)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   One chunk = one paragraph
   
   Document:
   "Paragraph 1: Introduction...
   
    Paragraph 2: Main content...
    
    Paragraph 3: Conclusion..."
            â†“
   Chunk 1: "Paragraph 1: Introduction..."
   Chunk 2: "Paragraph 2: Main content..."
   Chunk 3: "Paragraph 3: Conclusion..."
   
   âœ… Semantic coherence
   âœ… Natural structure
   âŒ Inconsistent sizes


4. OVERLAP STRATEGY (Best)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Chunks share some content
   
   Chunk size: 500 chars
   Overlap: 100 chars
   
   Document: "ABCDEFGHIJ..."
            â†“
   Chunk 1: "ABCDE"
   Chunk 2:     "DEFGH"  â† "DE" overlaps
   Chunk 3:         "GHIJK"  â† "GH" overlaps
   
   âœ… Captures context across boundaries
   âœ… No missed information
   âŒ More storage needed


RECOMMENDED APPROACH:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

For general RAG:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Chunk size: 500-1000 charsâ”‚
â”‚ â€¢ Overlap: 100-200 chars    â”‚
â”‚ â€¢ Method: Sentence-aware    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


CHUNKING EXAMPLE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Original Document (1,500 chars):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python is a high-level programming       â”‚
â”‚ language. It was created by Guido van    â”‚
â”‚ Rossum in 1991. Python emphasizes code   â”‚
â”‚ readability with significant whitespace. â”‚
â”‚                                          â”‚
â”‚ Python supports multiple programming     â”‚
â”‚ paradigms including OOP and functional.  â”‚
â”‚ It has a comprehensive standard library. â”‚
â”‚                                          â”‚
â”‚ Python is widely used in web development,â”‚
â”‚ data science, and machine learning.      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After Chunking (500 chars, 100 overlap):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk 1 (500 chars):                     â”‚
â”‚ "Python is a high-level programming      â”‚
â”‚  language. It was created by Guido van   â”‚
â”‚  Rossum in 1991. Python emphasizes..."   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 2 (500 chars, overlaps 100):      â”‚
â”‚ "...emphasizes code readability with     â”‚
â”‚  significant whitespace. Python supports â”‚
â”‚  multiple programming paradigms..."      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 3 (500 chars, overlaps 100):      â”‚
â”‚ "...paradigms including OOP and          â”‚
â”‚  functional. It has comprehensive        â”‚
â”‚  standard library. Python is widely..."  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Code Structure Map

```
09_rag_basic.py
â”‚
â”œâ”€â”€ ğŸ“¦ IMPORTS
â”‚   â”œâ”€â”€ os, dotenv
â”‚   â”œâ”€â”€ google.generativeai
â”‚   â””â”€â”€ numpy (for similarity calculations)
â”‚
â”œâ”€â”€ ğŸ”§ SETUP
â”‚   â”œâ”€â”€ load_dotenv()
â”‚   â”œâ”€â”€ genai.configure()
â”‚   â””â”€â”€ Initialize models
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 1: create_embeddings()
â”‚   â”œâ”€â”€ Input: text or list of texts
â”‚   â”œâ”€â”€ Call: genai.embed_content()
â”‚   â””â”€â”€ Output: embedding vectors
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 2: chunk_text()
â”‚   â”œâ”€â”€ Split text into chunks
â”‚   â”œâ”€â”€ Apply overlap strategy
â”‚   â””â”€â”€ Return: list of chunks
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 3: cosine_similarity()
â”‚   â”œâ”€â”€ Calculate vector similarity
â”‚   â”œâ”€â”€ Formula: dot(A,B) / (norm(A)*norm(B))
â”‚   â””â”€â”€ Return: similarity score (0-1)
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 4: find_relevant_chunks()
â”‚   â”œâ”€â”€ Embed query
â”‚   â”œâ”€â”€ Compare with all chunks
â”‚   â”œâ”€â”€ Sort by similarity
â”‚   â””â”€â”€ Return: top N chunks
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 5: build_rag_prompt()
â”‚   â”œâ”€â”€ Format: Context + Question
â”‚   â”œâ”€â”€ Add instructions
â”‚   â””â”€â”€ Return: complete prompt
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 6: rag_query()
â”‚   â”œâ”€â”€ Find relevant chunks
â”‚   â”œâ”€â”€ Build prompt
â”‚   â”œâ”€â”€ Call AI model
â”‚   â””â”€â”€ Return: answer
â”‚
â”œâ”€â”€ ğŸ¯ FUNCTION 7: create_knowledge_base()
â”‚   â”œâ”€â”€ Load documents
â”‚   â”œâ”€â”€ Chunk all docs
â”‚   â”œâ”€â”€ Create embeddings
â”‚   â””â”€â”€ Store: chunks + embeddings
â”‚
â””â”€â”€ ğŸš€ MAIN DEMO
    â”œâ”€â”€ Create sample documents
    â”œâ”€â”€ Build knowledge base
    â”œâ”€â”€ Run example queries
    â””â”€â”€ Display results
```

---

## ğŸ”„ RAG vs Regular AI

```
COMPARISON:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Question: "What did the CEO say in the meeting?"

REGULAR AI (No RAG):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User: "What did the CEO say?"
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AI Model                â”‚
â”‚ (No access to meeting)  â”‚  â†’ "I don't have information
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     about that meeting."
                                âŒ Cannot answer


RAG AI (With Documents):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User: "What did the CEO say?"
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Search meeting transcripts        â”‚
â”‚ 2. Find: "CEO mentioned Q4 goals..."â”‚
â”‚ 3. Add to AI prompt                  â”‚
â”‚ 4. Generate answer                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
"The CEO outlined three Q4 goals:
 1. Increase revenue by 20%
 2. Launch new product line
 3. Expand to European market"
âœ… Accurate, grounded answer


BENEFITS OF RAG:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… Access to YOUR specific documents
âœ… Overcomes knowledge cutoff dates
âœ… Reduces hallucinations (AI making up facts)
âœ… Answers grounded in real data
âœ… Can cite sources
âœ… Easily updatable (add new docs)


WITHOUT RAG:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AI Knowledge = Training Data Only
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… General knowledge (Wikipedia) â”‚
â”‚ âœ… Common facts                  â”‚
â”‚ âŒ Your company docs             â”‚
â”‚ âŒ Recent events (after cutoff)  â”‚
â”‚ âŒ Private information           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


WITH RAG:
â”€â”€â”€â”€â”€â”€â”€â”€â”€

AI Knowledge = Training Data + YOUR Docs
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âœ… General knowledge             â”‚
â”‚ âœ… Common facts                  â”‚
â”‚ âœ… Your company docs         NEW!â”‚
â”‚ âœ… Recent events (in docs)   NEW!â”‚
â”‚ âœ… Private information       NEW!â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Similarity Search Visualization

```
FINDING RELEVANT CHUNKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Knowledge Base (5 chunks):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk 1: "Python is a programming language" â”‚
â”‚ Embedding: [0.2, 0.8, 0.3, ...]            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 2: "Dogs are loyal pets"             â”‚
â”‚ Embedding: [0.7, 0.2, 0.9, ...]            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 3: "Machine learning uses Python"    â”‚
â”‚ Embedding: [0.3, 0.7, 0.4, ...]            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 4: "Pizza recipe with cheese"        â”‚
â”‚ Embedding: [0.9, 0.1, 0.2, ...]            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk 5: "Python libraries for AI"         â”‚
â”‚ Embedding: [0.2, 0.7, 0.5, ...]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Query: "How to use Python for AI?"
Query Embedding: [0.25, 0.75, 0.45, ...]

Similarity Calculation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query vs Chunk 1: 0.78  (78% similar)   â­   â”‚
â”‚ Query vs Chunk 2: 0.32  (32% similar)       â”‚
â”‚ Query vs Chunk 3: 0.92  (92% similar)   â­â­â­â”‚
â”‚ Query vs Chunk 4: 0.15  (15% similar)       â”‚
â”‚ Query vs Chunk 5: 0.89  (89% similar)   â­â­ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Top 3 Retrieved:
1. Chunk 3: "Machine learning uses Python" (92%)
2. Chunk 5: "Python libraries for AI" (89%)
3. Chunk 1: "Python is a programming..." (78%)

These become the CONTEXT for the AI!


SIMILARITY FORMULA:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Cosine Similarity:

Vector A = [aâ‚, aâ‚‚, aâ‚ƒ]
Vector B = [bâ‚, bâ‚‚, bâ‚ƒ]

           aâ‚Ã—bâ‚ + aâ‚‚Ã—bâ‚‚ + aâ‚ƒÃ—bâ‚ƒ
Similarity = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             âˆš(aâ‚Â²+aâ‚‚Â²+aâ‚ƒÂ²) Ã— âˆš(bâ‚Â²+bâ‚‚Â²+bâ‚ƒÂ²)

Result: 0.0 (totally different) to 1.0 (identical)

Example:
A = [1, 0, 1]  "Python programming"
B = [1, 0, 1]  "Python programming"
Similarity = 1.0 (identical)

A = [1, 0, 1]  "Python programming"
C = [0, 1, 0]  "Pizza recipe"
Similarity = 0.0 (completely different)
```

---

## ğŸ¯ RAG Prompt Structure

```
ANATOMY OF RAG PROMPT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Standard AI Prompt:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: "What is Python?"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Simple, but AI uses only training data


RAG-Enhanced Prompt:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [1. SYSTEM INSTRUCTION]                    â”‚
â”‚ You are a helpful assistant. Answer using  â”‚
â”‚ only the provided context.                 â”‚
â”‚                                            â”‚
â”‚ [2. CONTEXT]                               â”‚
â”‚ --- Retrieved Information ---              â”‚
â”‚ Python is a high-level programming         â”‚
â”‚ language created in 1991. It emphasizes    â”‚
â”‚ code readability and has extensive         â”‚
â”‚ libraries for various applications.        â”‚
â”‚ --- End of Context ---                     â”‚
â”‚                                            â”‚
â”‚ [3. INSTRUCTION]                           â”‚
â”‚ Based ONLY on the context above, answer:   â”‚
â”‚                                            â”‚
â”‚ [4. USER QUESTION]                         â”‚
â”‚ What is Python?                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AI now has SPECIFIC information to use!


COMPONENT BREAKDOWN:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. System Instruction:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Purpose: Set AI behavior            â”‚
   â”‚ Key phrase: "Use only the context"  â”‚
   â”‚ Why: Prevents hallucinations        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. Context (Retrieved Chunks):
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Source: Similarity search results   â”‚
   â”‚ Format: Clear delimiters            â”‚
   â”‚ Why: Provides grounded facts        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. Instruction:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Purpose: Reinforce context usage    â”‚
   â”‚ Prevents: AI inventing information  â”‚
   â”‚ Why: Ensures faithfulness to docs   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. User Question:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Original query                      â”‚
   â”‚ Unchanged from user input           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


EXAMPLE COMPARISON:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Without Context:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Q: "What's our Q4 revenue target?"     â”‚
â”‚ A: "I don't have access to your        â”‚
â”‚     company's Q4 targets."             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


With RAG Context:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context: "Board approved Q4 target     â”‚
â”‚           of $12M revenue, 15% growth" â”‚
â”‚                                        â”‚
â”‚ Q: "What's our Q4 revenue target?"     â”‚
â”‚ A: "According to the board decision,   â”‚
â”‚     the Q4 revenue target is $12M,     â”‚
â”‚     representing 15% growth."          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ RAG Implementation Steps

```
STEP-BY-STEP IMPLEMENTATION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PHASE 1: SETUP (One-time)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 1: Prepare Documents
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ Collect all docs          â”‚
â”‚ â€¢ Clean/format text         â”‚
â”‚ â€¢ Remove unnecessary parts  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: Chunk Documents
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ for doc in documents:       â”‚
â”‚     chunks = chunk_text(    â”‚
â”‚         doc,                â”‚
â”‚         size=500,           â”‚
â”‚         overlap=100         â”‚
â”‚     )                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 3: Create Embeddings
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ embeddings = []             â”‚
â”‚ for chunk in chunks:        â”‚
â”‚     emb = model.embed(      â”‚
â”‚         chunk               â”‚
â”‚     )                       â”‚
â”‚     embeddings.append(emb)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 4: Store in Database
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ knowledge_base = {          â”‚
â”‚     'chunks': chunks,       â”‚
â”‚     'embeddings': embeddingsâ”‚
â”‚ }                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


PHASE 2: QUERY (Every request)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 5: Embed User Query
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ query = "What is Python?"    â”‚
â”‚ query_emb = model.embed(     â”‚
â”‚     query                    â”‚
â”‚ )                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 6: Find Similar Chunks
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ similarities = []            â”‚
â”‚ for emb in embeddings:       â”‚
â”‚     sim = cosine_similarity( â”‚
â”‚         query_emb, emb       â”‚
â”‚     )                        â”‚
â”‚     similarities.append(sim) â”‚
â”‚                              â”‚
â”‚ top_indices = get_top_k(     â”‚
â”‚     similarities, k=3        â”‚
â”‚ )                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 7: Build Context
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ context = ""                 â”‚
â”‚ for idx in top_indices:      â”‚
â”‚     context += chunks[idx]   â”‚
â”‚     context += "\n\n"        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 8: Create RAG Prompt
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ prompt = f"""                â”‚
â”‚ Context: {context}           â”‚
â”‚                              â”‚
â”‚ Question: {query}            â”‚
â”‚                              â”‚
â”‚ Answer based on context:     â”‚
â”‚ """                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 9: Generate Answer
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ response = model.generate(   â”‚
â”‚     prompt                   â”‚
â”‚ )                            â”‚
â”‚                              â”‚
â”‚ return response.text         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


COMPLETE FLOW:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

User Query
    â†“
Embed Query
    â†“
Similarity Search
    â†“
Retrieve Top Chunks
    â†“
Build Context
    â†“
Create Prompt
    â†“
Generate Answer
    â†“
Return to User
```

---

## ğŸ’¡ RAG Best Practices

```
CHUNKING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… DO:
â€¢ Use 500-1000 character chunks
â€¢ Include 10-20% overlap
â€¢ Preserve sentence boundaries
â€¢ Keep semantic units together

âŒ DON'T:
â€¢ Make chunks too small (< 200 chars)
â€¢ Make chunks too large (> 2000 chars)
â€¢ Split mid-sentence
â€¢ Ignore document structure


EMBEDDING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… DO:
â€¢ Use consistent embedding model
â€¢ Batch embed for efficiency
â€¢ Cache embeddings
â€¢ Use domain-specific models if available

âŒ DON'T:
â€¢ Mix different embedding models
â€¢ Re-embed unnecessarily
â€¢ Ignore embedding dimensions


RETRIEVAL:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… DO:
â€¢ Retrieve 3-5 top chunks
â€¢ Set similarity threshold (e.g., > 0.7)
â€¢ Re-rank results if needed
â€¢ Include metadata (source, date)

âŒ DON'T:
â€¢ Retrieve too many chunks (> 10)
â€¢ Include low-similarity chunks
â€¢ Ignore source diversity


PROMPT DESIGN:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ… DO:
â€¢ Clearly mark context boundaries
â€¢ Instruct to use only context
â€¢ Include source citations
â€¢ Handle "no relevant info" case

âŒ DON'T:
â€¢ Mix context with question
â€¢ Allow hallucinations
â€¢ Exceed token limits
â€¢ Ignore formatting


EXAMPLE CONFIGURATIONS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

General Q&A:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk size: 800 chars  â”‚
â”‚ Overlap: 150 chars     â”‚
â”‚ Top-K: 3 chunks        â”‚
â”‚ Min similarity: 0.7    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Technical Documentation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk size: 1200 chars â”‚
â”‚ Overlap: 200 chars     â”‚
â”‚ Top-K: 5 chunks        â”‚
â”‚ Min similarity: 0.75   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Short Answers:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk size: 400 chars  â”‚
â”‚ Overlap: 100 chars     â”‚
â”‚ Top-K: 2 chunks        â”‚
â”‚ Min similarity: 0.8    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Module Documentation Block

```python
"""
09 - RAG (Retrieval-Augmented Generation) - Basic
===================================================
```
**Explanation:** Module about RAG - one of the MOST IMPORTANT patterns in modern AI applications. RAG enables AI to access external knowledge.

```python
This module demonstrates building a RAG system from scratch.
Students will learn:
- What RAG is and why it's important
- Document chunking strategies
- Creating embeddings
- Similarity search
- Context injection into prompts
- Building a basic RAG pipeline
```
**Explanation:** Learning objectives. RAG is THE solution for giving AI access to your specific documents/data.

```python
Teaching Points:
- RAG enables AI to access external knowledge
- Overcomes knowledge cutoff limitations
- More accurate than pure generation
- Foundation for intelligent search systems
"""
```
**Explanation:** **KEY INSIGHT**: RAG solves the fundamental problem that AI models don't know about YOUR specific documents, recent events, or proprietary data.

---

## Import Statements

```python
import os
```
**Explanation:** File/directory operations.

```python
from dotenv import load_dotenv
```
**Explanation:** Environment variables for API keys.

```python
import google.generativeai as genai
```
**Explanation:** Google's Generative AI SDK.

```python
import numpy as np
```
**Explanation:** **CRITICAL**: NumPy for vector operations. Embeddings are vectors (arrays of numbers), and we need to calculate similarity between vectors using dot products and norms.

```python
from typing import List, Dict
```
**Explanation:** Type hints for better code documentation.

```python
# Setup
load_dotenv()
genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
```
**Explanation:** Standard initialization.

---

## Section 1: Understanding RAG

```python
# ============================================================================
# SECTION 1: Understanding RAG
# ============================================================================
```
**Explanation:** Conceptual foundation.

```python
def rag_concepts():
    """
    Explain what RAG is and how it works
    """
```
**Explanation:** Educational overview of the RAG paradigm.

```python
    print("\n" + "=" * 60)
    print("SECTION 1: Understanding RAG")
    print("=" * 60)
    
    explanation = """
    ğŸ” WHAT IS RAG?
    
    RAG = Retrieval-Augmented Generation
    
    Combines:
    1. RETRIEVAL: Finding relevant information
    2. GENERATION: Creating response using that information
```
**Explanation:** **CORE CONCEPT**: RAG is TWO steps combined. First FIND relevant information, then GENERATE response using it.

```python
    âŒ WITHOUT RAG:
    ---------------
    User: "What's in our latest product documentation?"
    AI: "I don't have access to your documentation."
    
    â€¢ Limited to training data
    â€¢ No access to private/recent information
    â€¢ Can't answer company-specific questions
```
**Explanation:** **THE PROBLEM**: 
- AI models are frozen at training time
- Don't know about documents created after training
- Don't have access to private company data
- Can't answer questions like "What's in our Q3 report?" or "Who is our new CTO?"

```python
    âœ… WITH RAG:
    ------------
    User: "What's in our latest product documentation?"
    System:
      1. Search documentation
      2. Find relevant sections
      3. Provide to AI as context
    AI: "According to your documentation: [answer]"
    
    â€¢ Can access any documents
    â€¢ Works with private/recent data
    â€¢ More accurate, grounded responses
```
**Explanation:** **THE SOLUTION**: 
- Retrieve relevant document chunks
- Include them in the prompt
- AI generates answer BASED ON provided context
- Result: AI can "know" about YOUR specific documents

```python
    ğŸ”„ HOW RAG WORKS:
    
    Step 1: INDEXING (Done once)
    ----------------------------
    Documents â†’ Split into chunks â†’ Create embeddings â†’ Store in database
    
    Example:
    "Python Guide" â†’ 
      Chunk 1: "Python is a programming language..."
      Chunk 2: "Variables store data..."
      Chunk 3: "Functions are reusable..."
    
    Each chunk gets an embedding (vector representation)
```
**Explanation:** **INDEXING PHASE** (offline, one-time):
1. **Chunking**: Break large documents into smaller pieces (paragraphs/sections)
2. **Embedding**: Convert each chunk into a vector (array of numbers)
3. **Storage**: Save chunks and their embeddings in a database

**Why chunks?** Documents are too large to fit in prompts. We need small, focused pieces.

```python
    Step 2: RETRIEVAL (For each query)
    -----------------------------------
    User query â†’ Create query embedding â†’ Find similar chunks â†’ Retrieve
    
    Example:
    Query: "How do I create a function in Python?"
    â†’ Search for similar chunks
    â†’ Find Chunk 3: "Functions are reusable..."
```
**Explanation:** **RETRIEVAL PHASE** (online, per-query):
1. **Query embedding**: Convert user question to vector
2. **Similarity search**: Find chunks with similar vectors (semantic similarity)
3. **Retrieve**: Get the most similar chunks

**How similarity works**: Vectors that point in similar directions = semantically similar. "cat" and "dog" vectors are closer than "cat" and "car".

```python
    Step 3: AUGMENTATION
    --------------------
    Take retrieved chunks + user query â†’ Build enhanced prompt
    
    Enhanced prompt:
    "Context: Functions are reusable blocks of code...
     
     User question: How do I create a function in Python?
     
     Answer based on the context:"
```
**Explanation:** **AUGMENTATION PHASE**: Construct a prompt that includes:
- Retrieved context (the relevant chunks)
- User's original question
- Instructions to answer based on context

This is the "magic" - we're giving the AI the information it needs!

```python
    Step 4: GENERATION
    ------------------
    Send enhanced prompt to AI â†’ Get grounded response
```
**Explanation:** **GENERATION PHASE**: Send augmented prompt to LLM, get answer that's grounded in the provided context.

```python
    ğŸ¯ USE CASES:
    
    â€¢ Question answering over documents
    â€¢ Customer support with knowledge bases
    â€¢ Research assistants
    â€¢ Legal/medical document analysis
    â€¢ Internal company wikis
    â€¢ Educational tutoring
    â€¢ Code documentation search
```
**Explanation:** **REAL-WORLD APPLICATIONS**: RAG is used everywhere - customer support chatbots, internal company Q&A, research tools, code assistants, etc.

```python
    ğŸ’¡ KEY BENEFITS:
    
    â€¢ Up-to-date information
    â€¢ Private/proprietary data access
    â€¢ Reduced hallucinations
    â€¢ Verifiable sources
    â€¢ Domain-specific knowledge
    â€¢ Cost-effective vs fine-tuning
    """
```
**Explanation:** **ADVANTAGES**:
- **Up-to-date**: Add new documents anytime, no retraining
- **Private data**: Works with confidential docs
- **Fewer hallucinations**: AI answers from provided context, not imagination
- **Verifiable**: Can show which document chunk answer came from
- **Cost-effective**: Much cheaper than fine-tuning a model

```python
    print(explanation)
```
**Explanation:** Display concepts.

---

## Section 2: Document Chunking

```python
# ============================================================================
# SECTION 2: Document Chunking
# ============================================================================
```
**Explanation:** How to split documents effectively.

```python
def document_chunking():
    """
    Demonstrate different chunking strategies
    """
```
**Explanation:** **CRITICAL SKILL**: Bad chunking = bad retrieval. Must preserve semantic meaning.

```python
    print("\n" + "=" * 60)
    print("SECTION 2: Document Chunking")
    print("=" * 60)
    
    # Sample document
    sample_doc = """
Python is a high-level, interpreted programming language known for its simplicity and readability. 
It was created by Guido van Rossum and first released in 1991.

Python supports multiple programming paradigms including procedural, object-oriented, and functional programming.
The language emphasizes code readability with its use of significant indentation.

Python has a comprehensive standard library and a vast ecosystem of third-party packages available through PyPI.
This makes it suitable for various applications from web development to data science and machine learning.

Popular frameworks include Django for web development, NumPy and Pandas for data analysis, and TensorFlow for machine learning.
Python's versatility and ease of learning make it an excellent choice for beginners and professionals alike.
"""
```
**Explanation:** Sample document with multiple paragraphs for demonstrating chunking strategies.

```python
    print("\nğŸ“„ Sample Document:")
    print("-" * 60)
    print(sample_doc.strip())
    print("-" * 60)
    
    # Strategy 1: Fixed size chunking
    print("\n\n1ï¸âƒ£ FIXED SIZE CHUNKING")
    print("="*60)
    
    def chunk_by_size(text: str, chunk_size: int = 200) -> List[str]:
        """Split text into fixed-size chunks"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0
```
**Explanation:** **STRATEGY 1: Fixed Size**. Split by character/word count.

```python
        for word in words:
            current_length += len(word) + 1
            if current_length > chunk_size:
                chunks.append(' '.join(current_chunk))
                current_chunk = [word]
                current_length = len(word)
            else:
                current_chunk.append(word)
```
**Explanation:** **ALGORITHM**:
- Track current length
- Add words until reaching size limit
- Start new chunk when limit exceeded
- `len(word) + 1` accounts for spaces

```python
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
```
**Explanation:** Don't forget last chunk! Common bug to omit remaining words.

```python
    fixed_chunks = chunk_by_size(sample_doc, 200)
    print(f"\nCreated {len(fixed_chunks)} chunks (max 200 chars each):\n")
    for i, chunk in enumerate(fixed_chunks, 1):
        print(f"Chunk {i} ({len(chunk)} chars):")
        print(f"  {chunk[:100]}...\n")
```
**Explanation:** Generate and display fixed-size chunks.

**Pros**: Simple, predictable size
**Cons**: Can split mid-sentence, breaks semantic meaning

```python
    # Strategy 2: Sentence-based chunking
    print("\n2ï¸âƒ£ SENTENCE-BASED CHUNKING")
    print("="*60)
    
    def chunk_by_sentences(text: str, sentences_per_chunk: int = 2) -> List[str]:
        """Split text by sentences"""
        sentences = text.replace('\n', ' ').split('.')
        sentences = [s.strip() + '.' for s in sentences if s.strip()]
```
**Explanation:** **STRATEGY 2: Sentence-Based**. 
- Split by periods (sentence boundaries)
- Replace newlines with spaces
- Re-add periods (removed by split)
- Filter empty strings

```python
        chunks = []
        for i in range(0, len(sentences), sentences_per_chunk):
            chunk = ' '.join(sentences[i:i+sentences_per_chunk])
            chunks.append(chunk)
        
        return chunks
```
**Explanation:** Group sentences together. `range(..., sentences_per_chunk)` steps by that amount, so we get groups of N sentences.

```python
    sentence_chunks = chunk_by_sentences(sample_doc, 2)
    print(f"\nCreated {len(sentence_chunks)} chunks (2 sentences each):\n")
    for i, chunk in enumerate(sentence_chunks, 1):
        print(f"Chunk {i}:")
        print(f"  {chunk}\n")
```
**Explanation:** Display sentence-based chunks.

**Pros**: Preserves sentence meaning
**Cons**: Variable size, simple period splitting fails on abbreviations (Dr. Smith)

```python
    # Strategy 3: Paragraph-based chunking
    print("\n3ï¸âƒ£ PARAGRAPH-BASED CHUNKING")
    print("="*60)
    
    def chunk_by_paragraphs(text: str) -> List[str]:
        """Split text by paragraphs"""
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        return paragraphs
```
**Explanation:** **STRATEGY 3: Paragraph-Based**. Split on double newlines (`\n\n`). Natural semantic units.

```python
    para_chunks = chunk_by_paragraphs(sample_doc)
    print(f"\nCreated {len(para_chunks)} chunks (by paragraphs):\n")
    for i, chunk in enumerate(para_chunks, 1):
        print(f"Chunk {i}:")
        print(f"  {chunk[:100]}...\n")
```
**Explanation:** Display paragraph chunks.

**Pros**: Best semantic units, preserves topic coherence
**Cons**: Variable size, some paragraphs might be too large

```python
    print("\nğŸ’¡ CHOOSING A CHUNKING STRATEGY:")
    print("-" * 60)
    print("""
    â€¢ Fixed Size: Simple, consistent size, may break context
    â€¢ Sentences: Preserves meaning, variable size
    â€¢ Paragraphs: Best semantic units, can be too large
    â€¢ Hybrid: Combine strategies for best results
    
    Typical chunk sizes: 200-500 tokens (150-375 words)
    """)
```
**Explanation:** **BEST PRACTICES**: 
- **200-500 tokens**: Sweet spot for most applications
- **Hybrid**: Often best - try paragraph first, split if too large
- **Domain-specific**: Code needs different chunking than prose

---

## Section 3: Creating Embeddings

```python
# ============================================================================
# SECTION 3: Creating Embeddings
# ============================================================================
```
**Explanation:** Converting text to vectors.

```python
def creating_embeddings():
    """
    Generate embeddings for text chunks
    """
```
**Explanation:** Embeddings are THE KEY to semantic search.

```python
    print("\n" + "=" * 60)
    print("SECTION 3: Creating Embeddings")
    print("=" * 60)
    
    print("\nğŸ“Š What are embeddings?")
    print("-" * 60)
    print("""
    Embeddings convert text into numerical vectors (arrays of numbers).
    Similar meanings â†’ Similar vectors
    
    Example:
    "cat" â†’ [0.2, 0.8, 0.1, ...]
    "dog" â†’ [0.25, 0.75, 0.12, ...] (similar to cat)
    "car" â†’ [0.9, 0.1, 0.8, ...] (different from cat/dog)
    """)
```
**Explanation:** **FUNDAMENTAL CONCEPT**: 
- Text â†’ Vector (array of numbers)
- Semantically similar text â†’ Similar vectors (close in vector space)
- Example: "king" - "man" + "woman" â‰ˆ "queen" (famous example)

**Why vectors?** Can't do math on words, but CAN calculate distance/similarity between vectors.

```python
    # Sample texts
    texts = [
        "Python is a programming language",
        "JavaScript is used for web development",
        "Machine learning models need training data",
        "Deep learning is a subset of machine learning",
        "Cats are popular pets"
    ]
```
**Explanation:** Sample texts with varying semantic relatedness to demonstrate similarity.

```python
    print("\nğŸ”„ Generating embeddings...")
    print("-" * 60)
    
    # Generate embeddings using Gemini
    embeddings = []
    for i, text in enumerate(texts, 1):
        result = genai.embed_content(
            model="models/embedding-001",
            content=text,
            task_type="retrieval_document"
        )
```
**Explanation:** **EMBEDDING API CALL**:
- `model="models/embedding-001"`: Google's embedding model
- `content=text`: What to embed
- `task_type="retrieval_document"`: Optimize for document storage (vs "retrieval_query" for queries)

**Why different task types?** Queries and documents have different characteristics. Optimizing separately improves retrieval.

```python
        embedding = result['embedding']
        embeddings.append(embedding)
        
        print(f"\n{i}. Text: \"{text}\"")
        print(f"   Embedding: [{embedding[0]:.4f}, {embedding[1]:.4f}, {embedding[2]:.4f}, ... ] ({len(embedding)} dimensions)")
```
**Explanation:** Display first 3 dimensions and total dimension count. Typical: 768 or 1536 dimensions.

```python
    # Calculate similarity
    print("\n\nğŸ“ Similarity Scores (Cosine Similarity):")
    print("-" * 60)
    
    def cosine_similarity(a, b):
        """Calculate cosine similarity between two vectors"""
        dot_product = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        return dot_product / (norm_a * norm_b)
```
**Explanation:** **COSINE SIMILARITY FORMULA**:
- **Dot product**: aÂ·b = sum of element-wise multiplications
- **Norm**: ||a|| = sqrt(sum of squared elements) = vector length
- **Cosine similarity**: (aÂ·b) / (||a|| Ã— ||b||)

**Range**: -1 to 1 (usually 0 to 1 for embeddings)
- 1 = identical direction (very similar)
- 0 = perpendicular (unrelated)
- -1 = opposite direction (antonyms)

**Why cosine?** Measures angle between vectors, independent of magnitude. Perfect for semantic similarity.

```python
    # Compare some pairs
    comparisons = [
        (0, 1, "Python vs JavaScript"),
        (2, 3, "ML vs Deep Learning"),
        (0, 4, "Python vs Cats"),
        (2, 4, "ML vs Cats")
    ]
    
    for idx1, idx2, label in comparisons:
        similarity = cosine_similarity(embeddings[idx1], embeddings[idx2])
        print(f"\n{label}:")
        print(f"  '{texts[idx1]}'")
        print(f"  vs")
        print(f"  '{texts[idx2]}'")
        print(f"  Similarity: {similarity:.4f} {'ğŸ”¥ High' if similarity > 0.7 else 'â„ï¸ Low'}")
```
**Explanation:** Compare pairs to show:
- **Related topics** (Python vs JavaScript, ML vs Deep Learning) = HIGH similarity
- **Unrelated topics** (Programming vs Cats) = LOW similarity

---

## Section 4: Similarity Search

```python
# ============================================================================
# SECTION 4: Similarity Search
# ============================================================================
```
**Explanation:** Finding relevant documents.

```python
def similarity_search():
    """
    Implement basic similarity search
    """
```
**Explanation:** The retrieval component of RAG.

```python
    print("\n" + "=" * 60)
    print("SECTION 4: Similarity Search")
    print("=" * 60)
    
    # Knowledge base
    knowledge_base = [
        "Python was created by Guido van Rossum in 1991.",
        "Python is known for its simple and readable syntax.",
        "Python supports multiple programming paradigms.",
        "Django is a popular Python web framework.",
        "NumPy is used for numerical computing in Python.",
        "Machine learning can be done with Python using libraries like scikit-learn.",
        "Python has a large standard library included by default.",
        "Python code uses indentation for structure instead of braces."
    ]
```
**Explanation:** Our "database" of knowledge. In production, this would be thousands/millions of documents.

```python
    print("\nğŸ“š Knowledge Base:")
    print("-" * 60)
    for i, doc in enumerate(knowledge_base, 1):
        print(f"{i}. {doc}")
    
    # Generate embeddings for knowledge base
    print("\n\nğŸ”„ Generating embeddings for knowledge base...")
    
    kb_embeddings = []
    for doc in knowledge_base:
        result = genai.embed_content(
            model="models/embedding-001",
            content=doc,
            task_type="retrieval_document"
        )
        kb_embeddings.append(result['embedding'])
    
    print("âœ… Embeddings created!")
```
**Explanation:** **INDEXING**: Create embeddings for all documents. This is done ONCE (offline), not per-query.

```python
    # Search function
    def search(query: str, top_k: int = 3) -> List[Dict]:
        """Search for most similar documents"""
        # Generate query embedding
        query_result = genai.embed_content(
            model="models/embedding-001",
            content=query,
            task_type="retrieval_query"
        )
        query_embedding = query_result['embedding']
```
**Explanation:** **KEY DIFFERENCE**: `task_type="retrieval_query"` for queries (vs "retrieval_document" for documents). Optimizes embedding for query-document matching.

```python
        # Calculate similarities
        similarities = []
        for i, doc_embedding in enumerate(kb_embeddings):
            similarity = np.dot(query_embedding, doc_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)
            )
```
**Explanation:** **BRUTE FORCE SEARCH**: Calculate cosine similarity between query and EVERY document. 

**In production**: Use vector databases (Pinecone, Weaviate, Chroma) with approximate nearest neighbor search for speed.

```python
            similarities.append({
                'index': i,
                'document': knowledge_base[i],
                'similarity': similarity
            })
        
        # Sort and return top k
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        return similarities[:top_k]
```
**Explanation:** 
- Sort by similarity (highest first)
- Return top K results
- `top_k=3`: Typical value, balances context richness vs noise

```python
    # Test queries
    test_queries = [
        "Who created Python?",
        "What is Python used for in data science?",
        "How is Python code structured?"
    ]
    
    for query in test_queries:
        print(f"\n\n{'='*60}")
        print(f"ğŸ” Query: '{query}'")
        print(f"{'='*60}")
        
        results = search(query, top_k=3)
        
        print("\nTop 3 Most Relevant Documents:")
        for i, result in enumerate(results, 1):
            print(f"\n{i}. [Similarity: {result['similarity']:.4f}]")
            print(f"   {result['document']}")
```
**Explanation:** Test queries demonstrate semantic search:
- "Who created Python?" â†’ finds "Python was created by Guido van Rossum"
- Doesn't need exact keyword match
- Understands semantic intent

---

## Section 5: Basic RAG Pipeline

```python
# ============================================================================
# SECTION 5: Basic RAG Pipeline
# ============================================================================
```
**Explanation:** Complete end-to-end RAG system.

```python
def basic_rag_pipeline():
    """
    Complete RAG implementation
    """
```
**Explanation:** **THE FULL PIPELINE**: Retrieval + Augmentation + Generation.

```python
    print("\n" + "=" * 60)
    print("SECTION 5: Basic RAG Pipeline")
    print("=" * 60)
    
    # Knowledge base
    documents = [
        "Python is a high-level programming language created by Guido van Rossum in 1991.",
        "Python emphasizes code readability with significant indentation.",
        "Python supports object-oriented, procedural, and functional programming paradigms.",
        "The Django framework is used for building web applications in Python.",
        "Flask is a lightweight web framework for Python.",
        "NumPy provides support for large, multi-dimensional arrays and matrices.",
        "Pandas is a data analysis library that offers data structures like DataFrames.",
        "Scikit-learn is a machine learning library for Python.",
        "TensorFlow and PyTorch are deep learning frameworks available in Python.",
        "Python's pip package manager makes it easy to install third-party libraries."
    ]
```
**Explanation:** Richer knowledge base covering various Python topics.

```python
    print("\nğŸ“š Loading knowledge base...")
    print(f"   {len(documents)} documents loaded")
    
    # Generate embeddings
    print("\nğŸ”„ Creating embeddings...")
    
    doc_embeddings = []
    for doc in documents:
        result = genai.embed_content(
            model="models/embedding-001",
            content=doc,
            task_type="retrieval_document"
        )
        doc_embeddings.append(result['embedding'])
    
    print("   âœ… Embeddings created!")
```
**Explanation:** **INDEXING PHASE**: One-time embedding generation.

```python
    # RAG function
    def rag_query(question: str, top_k: int = 3) -> str:
        """
        Complete RAG pipeline:
        1. Retrieve relevant documents
        2. Augment prompt with context
        3. Generate response
        """
```
**Explanation:** **COMPLETE RAG FUNCTION**: All three phases in one.

```python
        # Step 1: Retrieve
        print(f"\nğŸ” Retrieving relevant documents...")
        
        query_result = genai.embed_content(
            model="models/embedding-001",
            content=question,
            task_type="retrieval_query"
        )
        query_embedding = query_result['embedding']
        
        # Calculate similarities
        similarities = []
        for i, doc_emb in enumerate(doc_embeddings):
            sim = np.dot(query_embedding, doc_emb) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)
            )
            similarities.append((i, sim))
        
        # Get top k
        similarities.sort(key=lambda x: x[1], reverse=True)
        top_docs = [documents[i] for i, _ in similarities[:top_k]]
        
        print(f"   âœ… Retrieved {len(top_docs)} documents")
```
**Explanation:** **RETRIEVAL**: Same as before - find top K most similar documents.

```python
        # Step 2: Augment
        print(f"\nğŸ“ Building augmented prompt...")
        
        context = "\n".join([f"- {doc}" for doc in top_docs])
        
        augmented_prompt = f"""Based on the following context, answer the question.
        
Context:
{context}

Question: {question}

Answer: Provide a clear, concise answer based ONLY on the context above. If the context doesn't contain the answer, say so."""
```
**Explanation:** **AUGMENTATION**: Build prompt with:
1. **Clear instruction**: "Based on the following context..."
2. **Context section**: Retrieved documents
3. **Question**: User's query
4. **Constraint**: "based ONLY on the context" (reduces hallucinations)

**Critical**: Format matters! Clear separation between context and question helps the model.

```python
        # Step 3: Generate
        print(f"\nğŸ¤– Generating response...")
        
        model = genai.GenerativeModel('gemini-pro')
        response = model.generate_content(augmented_prompt)
        
        return response.text, top_docs
```
**Explanation:** **GENERATION**: Send augmented prompt to LLM, get grounded response.

```python
    # Test the RAG system
    test_questions = [
        "Who created Python and when?",
        "What web frameworks are available for Python?",
        "What libraries can I use for data analysis?"
    ]
    
    for question in test_questions:
        print("\n" + "="*60)
        print(f"â“ Question: {question}")
        print("="*60)
        
        answer, retrieved_docs = rag_query(question)
        
        print("\nğŸ“„ Retrieved Context:")
        for i, doc in enumerate(retrieved_docs, 1):
            print(f"   {i}. {doc}")
        
        print(f"\nâœ… Answer:")
        print("-"*60)
        print(answer)
        print()
```
**Explanation:** Test queries demonstrate complete RAG pipeline. Shows which documents were retrieved and the final answer.

---

## Section 6: RAG vs Non-RAG Comparison

```python
# ============================================================================
# SECTION 6: RAG vs Non-RAG Comparison
# ============================================================================
```
**Explanation:** Showing RAG's value.

```python
def rag_comparison():
    """
    Compare RAG vs non-RAG responses
    """
```
**Explanation:** Side-by-side comparison to demonstrate RAG benefits.

```python
    print("\n" + "=" * 60)
    print("SECTION 6: RAG vs Non-RAG Comparison")
    print("=" * 60)
    
    # Specific company knowledge
    company_docs = [
        "Our company, TechCorp, was founded in 2020 by Jane Smith.",
        "TechCorp specializes in AI-powered customer service solutions.",
        "Our main product is ChatAssist, launched in January 2023.",
        "ChatAssist has helped over 5000 businesses improve customer satisfaction.",
        "TechCorp is headquartered in San Francisco with 150 employees."
    ]
```
**Explanation:** **COMPANY-SPECIFIC DATA**: Information the model CAN'T know (not in training data). Perfect for demonstrating RAG.

```python
    # Generate embeddings
    doc_embeddings = []
    for doc in company_docs:
        result = genai.embed_content(
            model="models/embedding-001",
            content=doc,
            task_type="retrieval_document"
        )
        doc_embeddings.append(result['embedding'])
    
    question = "When was TechCorp founded and by whom?"
    
    model = genai.GenerativeModel('gemini-pro')
    
    # Without RAG
    print("\nâŒ WITHOUT RAG:")
    print("-"*60)
    print(f"Question: {question}\n")
    
    response_no_rag = model.generate_content(question)
    print(f"Answer: {response_no_rag.text}")
    print("\nğŸ’¡ Notice: Generic answer or says it doesn't know")
```
**Explanation:** **WITHOUT RAG**: AI doesn't know about TechCorp. Will either:
- Say "I don't have information about TechCorp"
- Hallucinate (make up) an answer
- Give generic response

```python
    # With RAG
    print("\n\nâœ… WITH RAG:")
    print("-"*60)
    print(f"Question: {question}\n")
    
    # Retrieve
    query_result = genai.embed_content(
        model="models/embedding-001",
        content=question,
        task_type="retrieval_query"
    )
    
    similarities = []
    for i, doc_emb in enumerate(doc_embeddings):
        sim = np.dot(query_result['embedding'], doc_emb) / (
            np.linalg.norm(query_result['embedding']) * np.linalg.norm(doc_emb)
        )
        similarities.append((i, sim))
    
    similarities.sort(key=lambda x: x[1], reverse=True)
    relevant_doc = company_docs[similarities[0][0]]
    
    print(f"Retrieved Context: {relevant_doc}\n")
    
    # Generate with context
    rag_prompt = f"Context: {relevant_doc}\n\nQuestion: {question}\n\nAnswer based on the context:"
    response_rag = model.generate_content(rag_prompt)
    
    print(f"Answer: {response_rag.text}")
    print("\nğŸ’¡ Notice: Specific, accurate answer based on provided context")
```
**Explanation:** **WITH RAG**: AI gets the relevant document as context and provides accurate answer: "TechCorp was founded in 2020 by Jane Smith."

**The difference is DRAMATIC**: RAG enables AI to answer questions it couldn't answer otherwise.

---

## Section 7: Best Practices

```python
# ============================================================================
# SECTION 7: Best Practices
# ============================================================================
```
**Explanation:** Production guidelines.

```python
def rag_best_practices():
    """
    Best practices for building RAG systems
    """
```
**Explanation:** Accumulated wisdom for production RAG systems.

```python
    print("\n" + "=" * 60)
    print("SECTION 7: RAG Best Practices")
    print("=" * 60)
    
    practices = """
    âœ… DOCUMENT PREPARATION:
    
    1. Clean your documents
       â€¢ Remove irrelevant content
       â€¢ Fix formatting issues
       â€¢ Standardize structure
```
**Explanation:** **GARBAGE IN = GARBAGE OUT**: Clean documents = better retrieval. Remove boilerplate, fix OCR errors, standardize formatting.

```python
    2. Choose appropriate chunk size
       â€¢ Too small: Loses context
       â€¢ Too large: Less precise retrieval
       â€¢ Sweet spot: 200-500 tokens
```
**Explanation:** **CHUNK SIZE TRADE-OFF**:
- **Too small** (< 100 tokens): Loses surrounding context
- **Too large** (> 1000 tokens): Retrieved chunk might contain irrelevant info
- **Sweet spot** (200-500 tokens): Enough context, precise retrieval

```python
    3. Add metadata
       â€¢ Source, date, author
       â€¢ Helps with filtering
       â€¢ Improves traceability
```
**Explanation:** **METADATA**: Store source info with chunks. Enables:
- Filtering ("only 2023 documents")
- Citation ("According to Q3 report...")
- Freshness scoring (prioritize recent)

```python
    âœ… EMBEDDING STRATEGY:
    
    1. Use domain-specific models when possible
    2. Consistent embedding model for index and query
    3. Consider embedding costs at scale
    4. Cache embeddings for reuse
```
**Explanation:** **EMBEDDING TIPS**:
- **Domain-specific**: Medical/legal embeddings understand jargon better
- **Consistency**: MUST use same model for documents and queries
- **Costs**: Embedding costs add up at scale
- **Caching**: Don't re-embed same text

```python
    âœ… RETRIEVAL:
    
    1. Tune top_k parameter
       â€¢ More context vs noise
       â€¢ Typical: 3-5 documents
```
**Explanation:** **TOP_K TUNING**:
- **Too low** (1-2): Might miss relevant info
- **Too high** (10+): Adds noise, costs more tokens
- **Sweet spot** (3-5): Good coverage without overload

```python
    2. Implement hybrid search
       â€¢ Semantic (embeddings)
       â€¢ Keyword (BM25)
       â€¢ Combined scoring
```
**Explanation:** **HYBRID SEARCH**: Combine semantic and keyword search. Semantic finds conceptually similar, keyword finds exact matches. Best of both worlds.

```python
    3. Use reranking
       â€¢ Initial retrieval: Fast, broad
       â€¢ Reranking: Slow, precise
       â€¢ Best of both worlds
```
**Explanation:** **TWO-STAGE RETRIEVAL**:
1. **Fast retrieval**: Get top 50 candidates (quick, approximate)
2. **Slow reranking**: Score top 50 carefully, return best 5
Result: Speed + accuracy

```python
    âœ… PROMPT ENGINEERING:
    
    1. Clear instructions
       "Answer based on context"
       "Cite sources when possible"
       "Say 'I don't know' if context insufficient"
```
**Explanation:** **PROMPT INSTRUCTIONS**: Tell the model:
- Use context (not general knowledge)
- Cite sources (for verification)
- Admit uncertainty (don't hallucinate)

```python
    2. Format context clearly
       Separate context from question
       Number or bullet points
       Include source references
```
**Explanation:** **FORMATTING MATTERS**: Clear separation helps model understand what's context vs question.

```python
    3. Handle edge cases
       No relevant documents found
       Contradictory information
       Outdated information
```
**Explanation:** **EDGE CASES**:
- **No results**: Graceful degradation, ask user to rephrase
- **Contradictory**: Present both, note contradiction
- **Outdated**: Prioritize recent, note date

```python
    âœ… SYSTEM DESIGN:
    
    1. Caching
       â€¢ Cache embeddings
       â€¢ Cache common queries
       â€¢ Reduce API calls
```
**Explanation:** **CACHING**: Huge cost/latency savings:
- **Embedding cache**: Don't re-embed same chunks
- **Query cache**: Store popular query results

```python
    2. Error handling
       â€¢ Embedding failures
       â€¢ Search failures
       â€¢ Generation failures
       â€¢ Graceful degradation
```
**Explanation:** **ROBUSTNESS**: Handle failures gracefully. If embedding API down, fall back to keyword search.

```python
    3. Monitoring
       â€¢ Query patterns
       â€¢ Retrieval quality
       â€¢ Response quality
       â€¢ User feedback
```
**Explanation:** **OBSERVABILITY**: Track:
- What users ask (identify gaps)
- Retrieval accuracy (A/B test chunking strategies)
- Response quality (user thumbs up/down)

```python
    4. Updating knowledge base
       â€¢ Regular updates
       â€¢ Version control
       â€¢ Incremental indexing
       â€¢ Remove outdated info
```
**Explanation:** **MAINTENANCE**: Knowledge base needs care:
- **Regular updates**: Add new docs
- **Version control**: Track changes
- **Incremental**: Don't re-index everything
- **Cleanup**: Remove obsolete docs

```python
    âš ï¸ COMMON PITFALLS:
    
    1. Context overflow
       â€¢ Too many retrieved docs
       â€¢ Exceeds token limit
       â€¢ Solution: Summarize or truncate
```
**Explanation:** **PITFALL 1**: Retrieving 10 long documents might exceed model's context window. Solution: Retrieve more, use fewer, or summarize.

```python
    2. Poor chunking
       â€¢ Breaks semantic meaning
       â€¢ Solution: Semantic-aware splitting
```
**Explanation:** **PITFALL 2**: Splitting mid-sentence or separating related info. Solution: Use paragraph/section boundaries.

```python
    3. Ignoring recency
       â€¢ Old information prioritized
       â€¢ Solution: Weight by date
```
**Explanation:** **PITFALL 3**: 2020 doc ranked same as 2024 doc. Solution: Boost similarity score by recency.

```python
    4. No source attribution
       â€¢ Can't verify claims
       â€¢ Solution: Include sources in response
```
**Explanation:** **PITFALL 4**: Can't verify where answer came from. Solution: "According to [Document X]..."

```python
    ğŸ’¡ EVALUATION:
    
    1. Retrieval quality
       â€¢ Are relevant docs retrieved?
       â€¢ Precision and recall
    
    2. Answer quality
       â€¢ Factually correct?
       â€¢ Based on context?
       â€¢ Well-formatted?
    
    3. User satisfaction
       â€¢ Helpful?
       â€¢ Accurate?
       â€¢ Fast enough?
    """
```
**Explanation:** **METRICS**:
- **Retrieval**: Are right docs found?
- **Generation**: Is answer correct and grounded?
- **UX**: Is user happy?

```python
    print(practices)
```
**Explanation:** Display all practices.

---

## Main Function

```python
# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """
    Main function with menu
    """
    print("\n")
    print("ğŸ“ " + "=" * 58 + " ğŸ“")
    print("      GENERATIVE AI SESSION - MODULE 9: RAG (BASIC)")
    print("ğŸ“ " + "=" * 58 + " ğŸ“")
```
**Explanation:** Standard main setup.

```python
    menu = """
    Choose a section to run:
    
    1. Understanding RAG
    2. Document Chunking
    3. Creating Embeddings
    4. Similarity Search
    5. Basic RAG Pipeline
    6. RAG vs Non-RAG Comparison
    7. Best Practices
    
    all - Run all sections
    quit - Exit
    
    """
```
**Explanation:** Menu with 7 sections.

```python
    while True:
        print(menu)
        choice = input("Your choice: ").strip().lower()
        
        if choice in ['quit', 'q', 'exit']:
            print("ğŸ‘‹ Goodbye!")
            break
        elif choice == '1':
            rag_concepts()
        # ... [rest of choices] ...
```
**Explanation:** Standard menu loop.

```python
        elif choice == 'all':
            rag_concepts()
            document_chunking()
            creating_embeddings()
            similarity_search()
            basic_rag_pipeline()
            rag_comparison()
            rag_best_practices()
            print("\nâœ… All sections completed!")
            break
```
**Explanation:** 'all' runs all sections sequentially.

```python
        else:
            print("âš ï¸  Invalid choice. Please try again.")
```
**Explanation:** Invalid input handling.

---

## Script Entry Point

```python
if __name__ == "__main__":
    main()
    
    # Teaching Questions:
    # 1. What problem does RAG solve?
    # 2. How does chunking affect retrieval quality?
    # 3. When would you use RAG vs fine-tuning?
```
**Explanation:** Entry point with discussion questions.

---

## Summary

This module teaches **RAG (Retrieval-Augmented Generation)** - THE most important pattern for building AI systems that work with your specific documents and data.

### The RAG Problem:

AI models are **frozen at training time**:
- Don't know about documents created after training
- No access to private/proprietary data
- Can't answer company-specific questions
- Limited to general knowledge

### The RAG Solution:

**Four-Phase Pipeline:**

1. **Indexing** (offline, once):
   - Chunk documents (200-500 tokens)
   - Generate embeddings (text â†’ vectors)
   - Store in vector database

2. **Retrieval** (online, per-query):
   - Embed user query
   - Find similar chunks (cosine similarity)
   - Retrieve top K results

3. **Augmentation**:
   - Build prompt: Context + Question
   - Format clearly for the model

4. **Generation**:
   - Send augmented prompt to LLM
   - Get grounded response

### Key Concepts:

**Embeddings**: Text â†’ numerical vectors (arrays). Semantically similar text â†’ similar vectors.

**Cosine Similarity**: Measures angle between vectors. Formula: (aÂ·b) / (||a|| Ã— ||b||). Range 0-1, higher = more similar.

**Chunking Strategies**:
- **Fixed size**: Simple, may break context
- **Sentences**: Preserves meaning, variable size  
- **Paragraphs**: Best semantic units, can be large
- **Sweet spot**: 200-500 tokens

### Implementation Pattern:

```python
# 1. Index (once)
embeddings = [embed(chunk) for chunk in chunks]

# 2. Retrieve (per query)
query_emb = embed(query)
top_chunks = find_similar(query_emb, embeddings, k=3)

# 3. Augment
prompt = f"Context: {top_chunks}\n\nQ: {query}\n\nA:"

# 4. Generate
response = model.generate(prompt)
```

### Best Practices:

**Document Prep**:
- Clean formatting
- Optimal chunk size (200-500 tokens)
- Add metadata (source, date, author)

**Retrieval**:
- top_k = 3-5 (sweet spot)
- Hybrid search (semantic + keyword)
- Two-stage (fast retrieval + slow reranking)

**Prompting**:
- Clear instructions ("Answer based on context")
- Format separation (context vs question)
- Handle edge cases ("Say 'I don't know' if uncertain")

**System Design**:
- Cache embeddings and queries
- Error handling and fallbacks
- Monitor retrieval/answer quality
- Regular knowledge base updates

### Common Pitfalls:

âŒ **Context overflow**: Too many docs â†’ exceeds token limit
âŒ **Poor chunking**: Breaks semantic meaning
âŒ **Ignoring recency**: Old docs ranked same as new
âŒ **No attribution**: Can't verify answer source

### Use Cases:

- Customer support (company knowledge base)
- Internal Q&A (company wiki, docs)
- Research assistants (papers, reports)
- Code assistants (documentation)
- Legal/medical (case law, records)

### RAG vs Alternatives:

**RAG vs Fine-tuning**:
- **RAG**: Dynamic, updatable, source attribution, cheaper
- **Fine-tuning**: Static knowledge, no sources, expensive

**When to use RAG**:
- Frequently changing information
- Need source attribution
- Private/proprietary data
- Quick deployment

**Key insight**: RAG transforms AI from "frozen knowledge" to "dynamic knowledge with sources". It's THE pattern enabling AI to work with YOUR specific data!