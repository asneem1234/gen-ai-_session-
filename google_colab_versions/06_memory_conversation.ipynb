{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6c3890",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-generativeai python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025bee7",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a860e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7647adc",
   "metadata": {},
   "source": [
    "## Step 3: Configure API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9041bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Direct input\n",
    "GOOGLE_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "print(\"‚úÖ API configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Colab Secrets (uncomment to use)\n",
    "# from google.colab import userdata\n",
    "# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "# genai.configure(api_key=GOOGLE_API_KEY)\n",
    "# print(\"‚úÖ API configured successfully using Colab Secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48a356",
   "metadata": {},
   "source": [
    "## 1. Simple Chat with Memory\n",
    "\n",
    "The AI remembers previous messages in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a07067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chat_with_memory():\n",
    "    \"\"\"Demonstrate basic conversation memory\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    \n",
    "    # Start chat with empty history\n",
    "    chat = model.start_chat(history=[])\n",
    "    \n",
    "    print(\"üí¨ Chat with Memory Demo\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    messages = [\n",
    "        \"My name is Alice\",\n",
    "        \"What's my name?\",\n",
    "        \"I like Python programming\",\n",
    "        \"What do I like?\"\n",
    "    ]\n",
    "    \n",
    "    for msg in messages:\n",
    "        print(f\"\\nüë§ You: {msg}\")\n",
    "        response = chat.send_message(msg)\n",
    "        print(f\"ü§ñ AI: {response.text}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "# Run the demo\n",
    "simple_chat_with_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40617b0a",
   "metadata": {},
   "source": [
    "## 2. View Chat History\n",
    "\n",
    "See all messages exchanged in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9fb769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat session\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "# Have a conversation\n",
    "print(\"Having a conversation...\\n\")\n",
    "\n",
    "chat.send_message(\"I'm learning about machine learning\")\n",
    "print(\"‚úÖ Message 1 sent\")\n",
    "\n",
    "chat.send_message(\"What topics should I study first?\")\n",
    "print(\"‚úÖ Message 2 sent\")\n",
    "\n",
    "chat.send_message(\"Can you recommend some resources?\")\n",
    "print(\"‚úÖ Message 3 sent\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìú Chat History:\\n\")\n",
    "\n",
    "# View the history\n",
    "for i, message in enumerate(chat.history, 1):\n",
    "    role = \"üë§ You\" if message.role == \"user\" else \"ü§ñ AI\"\n",
    "    text = message.parts[0].text\n",
    "    \n",
    "    # Truncate long messages\n",
    "    if len(text) > 100:\n",
    "        text = text[:100] + \"...\"\n",
    "    \n",
    "    print(f\"{i}. {role}: {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0015ec48",
   "metadata": {},
   "source": [
    "## 3. Chat with Pre-existing History\n",
    "\n",
    "Start a chat with previous conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f03dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_history():\n",
    "    \"\"\"Initialize chat with existing history\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    \n",
    "    # Define previous conversation history\n",
    "    history = [\n",
    "        {\"role\": \"user\", \"parts\": [\"Hi, I'm learning AI\"]},\n",
    "        {\"role\": \"model\", \"parts\": [\"Hello! That's great! I'm here to help you learn about AI.\"]},\n",
    "        {\"role\": \"user\", \"parts\": [\"I'm particularly interested in neural networks\"]},\n",
    "        {\"role\": \"model\", \"parts\": [\"Neural networks are fascinating! They're inspired by the human brain.\"]}\n",
    "    ]\n",
    "    \n",
    "    # Start chat with this history\n",
    "    chat = model.start_chat(history=history)\n",
    "    \n",
    "    print(\"üìã Previous Conversation:\")\n",
    "    print(\"=\"*60)\n",
    "    for msg in history:\n",
    "        role = \"üë§ You\" if msg[\"role\"] == \"user\" else \"ü§ñ AI\"\n",
    "        print(f\"{role}: {msg['parts'][0]}\\n\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nüîÑ Continuing the conversation...\\n\")\n",
    "    \n",
    "    # Continue the conversation\n",
    "    response = chat.send_message(\"Can you remind me what we're discussing?\")\n",
    "    print(f\"üë§ You: Can you remind me what we're discussing?\")\n",
    "    print(f\"\\nü§ñ AI: {response.text}\")\n",
    "\n",
    "# Run the function\n",
    "chat_with_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7777b187",
   "metadata": {},
   "source": [
    "## 4. Multi-turn Conversation\n",
    "\n",
    "Complex conversation with context building over multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2345df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_turn_conversation():\n",
    "    \"\"\"Demonstrate context building across multiple turns\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    chat = model.start_chat(history=[])\n",
    "    \n",
    "    print(\"üîÑ Multi-turn Conversation Demo\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Turn 1\n",
    "    print(\"\\nüë§ You: I want to learn about neural networks\")\n",
    "    response = chat.send_message(\"I want to learn about neural networks\")\n",
    "    print(f\"ü§ñ AI: {response.text}\")\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    \n",
    "    # Turn 2 - References previous context\n",
    "    print(\"\\nüë§ You: Can you explain it in simpler terms?\")\n",
    "    response = chat.send_message(\"Can you explain it in simpler terms?\")\n",
    "    print(f\"ü§ñ AI: {response.text}\")\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    \n",
    "    # Turn 3 - Further builds on context\n",
    "    print(\"\\nüë§ You: Give me an example\")\n",
    "    response = chat.send_message(\"Give me an example\")\n",
    "    print(f\"ü§ñ AI: {response.text}\")\n",
    "\n",
    "# Run the conversation\n",
    "multi_turn_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f86bd0",
   "metadata": {},
   "source": [
    "## 5. Context-Aware Conversation\n",
    "\n",
    "Test how the AI uses context from earlier messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34827d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat session\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "print(\"üß† Context-Aware Conversation Test\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Provide information\n",
    "msg1 = \"I'm a beginner programmer. I know HTML and CSS.\"\n",
    "print(f\"\\nüë§ You: {msg1}\")\n",
    "response1 = chat.send_message(msg1)\n",
    "print(f\"ü§ñ AI: {response1.text}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "# Ask context-dependent question\n",
    "msg2 = \"What programming language should I learn next?\"\n",
    "print(f\"\\nüë§ You: {msg2}\")\n",
    "response2 = chat.send_message(msg2)\n",
    "print(f\"ü§ñ AI: {response2.text}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "# Another context-dependent question\n",
    "msg3 = \"Why that one specifically for my background?\"\n",
    "print(f\"\\nüë§ You: {msg3}\")\n",
    "response3 = chat.send_message(msg3)\n",
    "print(f\"ü§ñ AI: {response3.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd6c19a",
   "metadata": {},
   "source": [
    "## 6. Save and Load Conversation History\n",
    "\n",
    "Export conversation history to continue later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7330789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversation\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "print(\"Starting new conversation...\\n\")\n",
    "\n",
    "chat.send_message(\"I'm working on a web project\")\n",
    "chat.send_message(\"It's an e-commerce website\")\n",
    "chat.send_message(\"I need help with the payment integration\")\n",
    "\n",
    "print(\"‚úÖ Conversation completed\\n\")\n",
    "\n",
    "# Extract history\n",
    "history_data = []\n",
    "for msg in chat.history:\n",
    "    history_data.append({\n",
    "        \"role\": msg.role,\n",
    "        \"parts\": [part.text for part in msg.parts]\n",
    "    })\n",
    "\n",
    "print(\"üì§ Exported History:\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(history_data, indent=2))\n",
    "\n",
    "# Save to variable for later use\n",
    "saved_history = history_data\n",
    "print(\"\\n‚úÖ History saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f1f84",
   "metadata": {},
   "source": [
    "### Load Saved History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved history and continue\n",
    "if 'saved_history' in locals():\n",
    "    print(\"üì• Loading saved conversation...\\n\")\n",
    "    \n",
    "    # Create new chat with loaded history\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    restored_chat = model.start_chat(history=saved_history)\n",
    "    \n",
    "    print(\"‚úÖ History loaded!\\n\")\n",
    "    print(\"üîÑ Continuing conversation...\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Continue from where we left off\n",
    "    msg = \"Can you recommend a payment gateway?\"\n",
    "    print(f\"\\nüë§ You: {msg}\")\n",
    "    response = restored_chat.send_message(msg)\n",
    "    print(f\"\\nü§ñ AI: {response.text}\")\n",
    "    \n",
    "    print(\"\\nüí° The AI remembered the e-commerce project context!\")\n",
    "else:\n",
    "    print(\"‚ùå No saved history found. Run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d14a9f",
   "metadata": {},
   "source": [
    "## 7. Interactive Chat Session\n",
    "\n",
    "Build your own conversation step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b459346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fresh chat session\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "my_chat = model.start_chat(history=[])\n",
    "\n",
    "print(\"üéØ Your Personal Chat Session Started!\\n\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nRun the cells below to chat...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba7a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message 1 - Change this to your message\n",
    "your_message_1 = \"I'm learning data science\"\n",
    "\n",
    "print(f\"üë§ You: {your_message_1}\")\n",
    "response = my_chat.send_message(your_message_1)\n",
    "print(f\"\\nü§ñ AI: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message 2 - AI remembers message 1\n",
    "your_message_2 = \"What libraries should I learn?\"\n",
    "\n",
    "print(f\"üë§ You: {your_message_2}\")\n",
    "response = my_chat.send_message(your_message_2)\n",
    "print(f\"\\nü§ñ AI: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5105e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message 3 - AI remembers entire conversation\n",
    "your_message_3 = \"Can you explain the first one you mentioned?\"\n",
    "\n",
    "print(f\"üë§ You: {your_message_3}\")\n",
    "response = my_chat.send_message(your_message_3)\n",
    "print(f\"\\nü§ñ AI: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View your entire conversation\n",
    "print(\"\\nüìú Your Complete Conversation History:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, msg in enumerate(my_chat.history, 1):\n",
    "    role = \"üë§ You\" if msg.role == \"user\" else \"ü§ñ AI\"\n",
    "    text = msg.parts[0].text\n",
    "    \n",
    "    print(f\"\\n{i}. {role}:\")\n",
    "    print(f\"   {text[:150]}{'...' if len(text) > 150 else ''}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfaba7",
   "metadata": {},
   "source": [
    "## 8. Memory Limitations Test\n",
    "\n",
    "See how the AI handles multiple pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory with multiple facts\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "print(\"üß™ Memory Test\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Provide multiple facts\n",
    "facts = [\n",
    "    \"My favorite color is blue\",\n",
    "    \"I live in New York\",\n",
    "    \"I work as a software engineer\",\n",
    "    \"I have a dog named Max\",\n",
    "    \"I enjoy playing guitar\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìù Providing information...\\n\")\n",
    "for fact in facts:\n",
    "    chat.send_message(fact)\n",
    "    print(f\"‚úÖ {fact}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚ùì Testing memory recall...\\n\")\n",
    "\n",
    "# Test recall\n",
    "questions = [\n",
    "    \"What's my favorite color?\",\n",
    "    \"Where do I live?\",\n",
    "    \"What's my pet's name?\",\n",
    "    \"What's my hobby?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"üë§ You: {q}\")\n",
    "    response = chat.send_message(q)\n",
    "    print(f\"ü§ñ AI: {response.text}\\n\")\n",
    "    print(\"-\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f310abe",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "You've learned how to:\n",
    "- ‚úÖ Create chat sessions with memory using `start_chat()`\n",
    "- ‚úÖ Build multi-turn conversations with context\n",
    "- ‚úÖ Initialize chats with pre-existing history\n",
    "- ‚úÖ View and inspect conversation history\n",
    "- ‚úÖ Save and load conversation state\n",
    "- ‚úÖ Use context for natural conversations\n",
    "- ‚úÖ Test memory and context retention\n",
    "\n",
    "## üí° Key Concepts:\n",
    "\n",
    "### Chat vs Generate:\n",
    "\n",
    "```python\n",
    "# Single-turn (no memory)\n",
    "model.generate_content(\"What is AI?\")  # Forgets immediately\n",
    "\n",
    "# Multi-turn (with memory)\n",
    "chat = model.start_chat(history=[])\n",
    "chat.send_message(\"What is AI?\")       # Remembers\n",
    "chat.send_message(\"Tell me more\")      # Knows context\n",
    "```\n",
    "\n",
    "### History Structure:\n",
    "\n",
    "```python\n",
    "history = [\n",
    "    {\"role\": \"user\", \"parts\": [\"Hello\"]},\n",
    "    {\"role\": \"model\", \"parts\": [\"Hi there!\"]},\n",
    "    {\"role\": \"user\", \"parts\": [\"How are you?\"]},\n",
    "    {\"role\": \"model\", \"parts\": [\"I'm doing well!\"]}\n",
    "]\n",
    "```\n",
    "\n",
    "## üîß Technical Details:\n",
    "\n",
    "- **History Storage**: Stored in `chat.history` list\n",
    "- **Message Format**: `{\"role\": \"user\"|\"model\", \"parts\": [\"text\"]}`\n",
    "- **Context Window**: Gemini has large context window (tokens)\n",
    "- **Persistence**: History lost when session ends (unless saved)\n",
    "\n",
    "## üíæ Saving Conversations:\n",
    "\n",
    "```python\n",
    "# Export\n",
    "history = [{\n",
    "    \"role\": msg.role,\n",
    "    \"parts\": [part.text for part in msg.parts]\n",
    "} for msg in chat.history]\n",
    "\n",
    "# Import\n",
    "chat = model.start_chat(history=history)\n",
    "```\n",
    "\n",
    "## üéØ Best Practices:\n",
    "\n",
    "1. **Use chat for conversations**: Multiple related messages\n",
    "2. **Use generate for one-offs**: Single unrelated queries\n",
    "3. **Clear context**: Start new chat when topic changes\n",
    "4. **Save important chats**: Export history for later\n",
    "5. **Monitor context length**: Very long histories may impact performance\n",
    "\n",
    "## üöÄ Use Cases:\n",
    "\n",
    "- Customer support chatbots\n",
    "- Educational tutoring systems\n",
    "- Personal assistants\n",
    "- Interactive documentation\n",
    "- Therapy/counseling bots\n",
    "- Technical support\n",
    "\n",
    "## Next Steps:\n",
    "1. Build a persistent chat application\n",
    "2. Implement conversation summarization\n",
    "3. Move on to lesson 07 (Model Configurations)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
