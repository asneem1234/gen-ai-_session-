{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c1a430c",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78afe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-generativeai python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacabfec",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7200d8",
   "metadata": {},
   "source": [
    "## Step 3: Configure API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Direct input\n",
    "GOOGLE_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "print(\"âœ… API configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Colab Secrets (uncomment to use)\n",
    "# from google.colab import userdata\n",
    "# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "# genai.configure(api_key=GOOGLE_API_KEY)\n",
    "# print(\"âœ… API configured successfully using Colab Secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d94e36c",
   "metadata": {},
   "source": [
    "## 1. Basic Streaming\n",
    "\n",
    "Stream response chunks in real-time as they're generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18681a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_streaming():\n",
    "    \"\"\"Stream a response chunk by chunk\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    prompt = \"Write a short story about AI in 5 sentences.\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"ğŸ¤– Streaming response:\\n\")\n",
    "    \n",
    "    # stream=True enables streaming\n",
    "    for chunk in model.generate_content(prompt, stream=True):\n",
    "        print(chunk.text, end='', flush=True)\n",
    "    \n",
    "    print(\"\\n\\nâœ… Streaming complete!\")\n",
    "\n",
    "# Run the function\n",
    "basic_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49468462",
   "metadata": {},
   "source": [
    "## 2. Streaming vs Non-Streaming Comparison\n",
    "\n",
    "See the difference between streaming and waiting for complete response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-streaming (traditional)\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "prompt = \"Explain machine learning in 3 paragraphs.\"\n",
    "\n",
    "print(\"â³ NON-STREAMING (Wait for full response):\\n\")\n",
    "start = time.time()\n",
    "\n",
    "response = model.generate_content(prompt)  # No stream parameter\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Response: {response.text}\")\n",
    "print(f\"\\nâ±ï¸ Time to first text: {elapsed:.2f}s\")\n",
    "print(f\"â±ï¸ Total time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "prompt = \"Explain machine learning in 3 paragraphs.\"\n",
    "\n",
    "print(\"\\nâš¡ STREAMING (Real-time chunks):\\n\")\n",
    "start = time.time()\n",
    "first_chunk_time = None\n",
    "\n",
    "full_text = \"\"\n",
    "for i, chunk in enumerate(model.generate_content(prompt, stream=True)):\n",
    "    if i == 0:\n",
    "        first_chunk_time = time.time() - start\n",
    "    print(chunk.text, end='', flush=True)\n",
    "    full_text += chunk.text\n",
    "\n",
    "total_time = time.time() - start\n",
    "\n",
    "print(f\"\\n\\nâ±ï¸ Time to first chunk: {first_chunk_time:.2f}s\")\n",
    "print(f\"â±ï¸ Total time: {total_time:.2f}s\")\n",
    "print(f\"\\nğŸ’¡ Streaming showed first results {first_chunk_time:.2f}s faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe659b",
   "metadata": {},
   "source": [
    "## 3. Streaming with Detailed Timing\n",
    "\n",
    "Track timing for each chunk to understand streaming behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_with_timing():\n",
    "    \"\"\"Stream with detailed timing analysis\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    prompt = \"Explain neural networks in simple terms.\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"ğŸ¤– Response (with chunk timing):\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    start = time.time()\n",
    "    chunk_count = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for chunk in model.generate_content(prompt, stream=True):\n",
    "        chunk_count += 1\n",
    "        total_chars += len(chunk.text)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        print(chunk.text, end='', flush=True)\n",
    "    \n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*60)\n",
    "    print(f\"\\nğŸ“Š Statistics:\")\n",
    "    print(f\"   Total chunks: {chunk_count}\")\n",
    "    print(f\"   Total characters: {total_chars}\")\n",
    "    print(f\"   Total time: {total_time:.2f}s\")\n",
    "    print(f\"   Avg chars/chunk: {total_chars/chunk_count:.1f}\")\n",
    "    print(f\"   Throughput: {total_chars/total_time:.1f} chars/second\")\n",
    "\n",
    "# Run the analysis\n",
    "streaming_with_timing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a03141f",
   "metadata": {},
   "source": [
    "## 4. Interactive Streaming Chat\n",
    "\n",
    "Multi-turn conversation with streaming responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc57f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_streaming_chat():\n",
    "    \"\"\"Chat with streaming responses\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    chat = model.start_chat(history=[])\n",
    "    \n",
    "    messages = [\n",
    "        \"Hi! What's artificial intelligence?\",\n",
    "        \"Tell me about Python programming\",\n",
    "        \"Thanks for the information!\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ’¬ Starting Streaming Chat\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for msg in messages:\n",
    "        print(f\"\\nğŸ‘¤ You: {msg}\")\n",
    "        print(\"ğŸ¤– AI: \", end='', flush=True)\n",
    "        \n",
    "        # Stream the chat response\n",
    "        response = chat.send_message(msg, stream=True)\n",
    "        for chunk in response:\n",
    "            print(chunk.text, end='', flush=True)\n",
    "        \n",
    "        print()  # New line after response\n",
    "        print(\"-\"*60)\n",
    "\n",
    "# Run the chat\n",
    "interactive_streaming_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba644c8",
   "metadata": {},
   "source": [
    "## 5. Custom Streaming Chat Session\n",
    "\n",
    "Create your own streaming conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c5b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new streaming chat\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "chat = model.start_chat(history=[])\n",
    "\n",
    "print(\"ğŸ¯ Custom Streaming Chat Session Started\\n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0874927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send your first message\n",
    "message1 = \"What are the benefits of using AI?\"\n",
    "\n",
    "print(f\"ğŸ‘¤ You: {message1}\")\n",
    "print(\"ğŸ¤– AI: \", end='', flush=True)\n",
    "\n",
    "response1 = chat.send_message(message1, stream=True)\n",
    "for chunk in response1:\n",
    "    print(chunk.text, end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915c0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a follow-up message\n",
    "message2 = \"Can you give me specific examples?\"\n",
    "\n",
    "print(f\"\\nğŸ‘¤ You: {message2}\")\n",
    "print(\"ğŸ¤– AI: \", end='', flush=True)\n",
    "\n",
    "response2 = chat.send_message(message2, stream=True)\n",
    "for chunk in response2:\n",
    "    print(chunk.text, end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a6f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another follow-up\n",
    "message3 = \"Which of these is most impactful?\"\n",
    "\n",
    "print(f\"\\nğŸ‘¤ You: {message3}\")\n",
    "print(\"ğŸ¤– AI: \", end='', flush=True)\n",
    "\n",
    "response3 = chat.send_message(message3, stream=True)\n",
    "for chunk in response3:\n",
    "    print(chunk.text, end='', flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40a284",
   "metadata": {},
   "source": [
    "## 6. Long Response Streaming\n",
    "\n",
    "See streaming benefits with longer content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2472d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request long content\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write a detailed explanation of how machine learning works, \n",
    "covering supervised learning, unsupervised learning, and \n",
    "reinforcement learning. Include examples for each.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ“– Generating long content with streaming...\\n\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "start = time.time()\n",
    "char_count = 0\n",
    "chunk_count = 0\n",
    "\n",
    "for chunk in model.generate_content(prompt, stream=True):\n",
    "    print(chunk.text, end='', flush=True)\n",
    "    char_count += len(chunk.text)\n",
    "    chunk_count += 1\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(f\"\\nğŸ“Š Long Content Stats:\")\n",
    "print(f\"   Chunks received: {chunk_count}\")\n",
    "print(f\"   Total characters: {char_count}\")\n",
    "print(f\"   Time: {elapsed:.2f}s\")\n",
    "print(f\"   Speed: {char_count/elapsed:.0f} chars/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8309551",
   "metadata": {},
   "source": [
    "## 7. Collecting Streamed Content\n",
    "\n",
    "Stream to display but also collect the full response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream and collect\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "prompt = \"List 5 benefits of learning Python programming\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"ğŸ¤– Streaming Response:\\n\")\n",
    "\n",
    "# Collect full text while streaming\n",
    "full_response = \"\"\n",
    "\n",
    "for chunk in model.generate_content(prompt, stream=True):\n",
    "    print(chunk.text, end='', flush=True)\n",
    "    full_response += chunk.text\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"\\nğŸ“ Full collected response:\")\n",
    "print(f\"\\nLength: {len(full_response)} characters\")\n",
    "print(f\"Word count: {len(full_response.split())} words\")\n",
    "print(f\"\\nFirst 100 chars: {full_response[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5438eec",
   "metadata": {},
   "source": [
    "## 8. Error Handling in Streaming\n",
    "\n",
    "Handle potential errors during streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e24166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming with error handling\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "prompt = \"Explain quantum computing\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"ğŸ¤– Response (with error handling):\\n\")\n",
    "\n",
    "try:\n",
    "    for chunk in model.generate_content(prompt, stream=True):\n",
    "        print(chunk.text, end='', flush=True)\n",
    "    print(\"\\n\\nâœ… Streaming completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n\\nâŒ Error during streaming: {e}\")\n",
    "    print(\"Streaming was interrupted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0657d50",
   "metadata": {},
   "source": [
    "## ğŸ‰ Summary\n",
    "\n",
    "You've learned how to:\n",
    "- âœ… Enable streaming with `stream=True`\n",
    "- âœ… Display real-time responses chunk by chunk\n",
    "- âœ… Compare streaming vs non-streaming performance\n",
    "- âœ… Track timing and statistics\n",
    "- âœ… Stream chat conversations\n",
    "- âœ… Collect streamed content\n",
    "- âœ… Handle errors during streaming\n",
    "\n",
    "## ğŸ’¡ Key Concepts:\n",
    "\n",
    "### Why Use Streaming?\n",
    "1. **Better UX**: Users see results immediately\n",
    "2. **Perceived Speed**: Feels faster even if total time is same\n",
    "3. **Long Responses**: Essential for lengthy content\n",
    "4. **Interactive Feel**: More natural conversation flow\n",
    "\n",
    "### Streaming vs Non-Streaming:\n",
    "\n",
    "```python\n",
    "# Non-streaming (wait for complete response)\n",
    "response = model.generate_content(prompt)\n",
    "print(response.text)  # Shows all at once\n",
    "\n",
    "# Streaming (real-time chunks)\n",
    "for chunk in model.generate_content(prompt, stream=True):\n",
    "    print(chunk.text, end='', flush=True)  # Shows as generated\n",
    "```\n",
    "\n",
    "## ğŸ”§ Technical Notes:\n",
    "\n",
    "- **flush=True**: Forces immediate output (important!)\n",
    "- **end=''**: Prevents newlines between chunks\n",
    "- **Chunks**: Variable size, depends on model\n",
    "- **Total Time**: Usually similar to non-streaming\n",
    "- **First Token**: Streaming shows first token much faster\n",
    "\n",
    "## âš¡ Performance Tips:\n",
    "\n",
    "1. Always use `flush=True` for real-time display\n",
    "2. Use streaming for responses > 2-3 sentences\n",
    "3. Collect chunks if you need the full text later\n",
    "4. Add error handling for production code\n",
    "5. Consider chunk size for optimal UX\n",
    "\n",
    "## ğŸ¯ When to Use Streaming:\n",
    "\n",
    "**Use Streaming:**\n",
    "- âœ… Long responses (stories, essays, explanations)\n",
    "- âœ… Interactive chat interfaces\n",
    "- âœ… Real-time dashboards\n",
    "- âœ… User-facing applications\n",
    "\n",
    "**Skip Streaming:**\n",
    "- âŒ Very short responses (1-2 words)\n",
    "- âŒ Batch processing\n",
    "- âŒ When you need complete text before processing\n",
    "- âŒ Automated pipelines\n",
    "\n",
    "## Next Steps:\n",
    "1. Experiment with different prompt lengths\n",
    "2. Build a streaming chat UI\n",
    "3. Move on to lesson 06 (Memory & Conversation)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
