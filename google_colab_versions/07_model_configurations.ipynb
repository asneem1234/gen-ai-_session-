{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0898ce",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fced8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-generativeai python-dotenv -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf73e7e0",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209f13d",
   "metadata": {},
   "source": [
    "## Step 3: Configure API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91494eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Direct input\n",
    "GOOGLE_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "print(\"‚úÖ API configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4de870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Colab Secrets (uncomment to use)\n",
    "# from google.colab import userdata\n",
    "# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "# genai.configure(api_key=GOOGLE_API_KEY)\n",
    "# print(\"‚úÖ API configured successfully using Colab Secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14125bcb",
   "metadata": {},
   "source": [
    "## 1. Temperature Control\n",
    "\n",
    "**Temperature** controls randomness/creativity in responses.\n",
    "\n",
    "- **Low (0.0 - 0.3)**: Deterministic, consistent, focused\n",
    "- **Medium (0.4 - 0.7)**: Balanced\n",
    "- **High (0.8 - 2.0)**: Creative, varied, unpredictable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ba5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_control():\n",
    "    \"\"\"Compare responses at different temperatures\"\"\"\n",
    "    prompt = \"Complete this sentence: The future of AI is\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Low temperature - deterministic\n",
    "    print(\"\\nüîµ Low temperature (0.1) - Deterministic/Focused:\")\n",
    "    model_low = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash', \n",
    "        generation_config={'temperature': 0.1}\n",
    "    )\n",
    "    response_low = model_low.generate_content(prompt)\n",
    "    print(response_low.text)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    \n",
    "    # High temperature - creative\n",
    "    print(\"\\nüî¥ High temperature (1.5) - Creative/Unpredictable:\")\n",
    "    model_high = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash',\n",
    "        generation_config={'temperature': 1.5}\n",
    "    )\n",
    "    response_high = model_high.generate_content(prompt)\n",
    "    print(response_high.text)\n",
    "\n",
    "# Run the comparison\n",
    "temperature_control()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126fbe2",
   "metadata": {},
   "source": [
    "### Test Temperature Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different temperatures on the same prompt\n",
    "prompt = \"Write a creative opening line for a sci-fi story\"\n",
    "\n",
    "temperatures = [0.2, 0.7, 1.5]\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nüå°Ô∏è Temperature = {temp}:\")\n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash',\n",
    "        generation_config={'temperature': temp}\n",
    "    )\n",
    "    response = model.generate_content(prompt)\n",
    "    print(response.text)\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba24c4d",
   "metadata": {},
   "source": [
    "## 2. Max Output Tokens\n",
    "\n",
    "**max_output_tokens** controls the maximum length of the response.\n",
    "\n",
    "- Limits how many tokens (roughly 0.75 words) the AI can generate\n",
    "- Useful for controlling response length\n",
    "- AI will stop when limit is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18cb7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_tokens_control():\n",
    "    \"\"\"Compare responses with different token limits\"\"\"\n",
    "    prompt = \"Explain quantum computing\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Short response\n",
    "    print(\"\\nüìè Short response (50 tokens):\")\n",
    "    model_short = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash',\n",
    "        generation_config={'max_output_tokens': 50}\n",
    "    )\n",
    "    response_short = model_short.generate_content(prompt)\n",
    "    print(response_short.text)\n",
    "    print(f\"\\nApprox words: {len(response_short.text.split())}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    \n",
    "    # Longer response\n",
    "    print(\"\\nüìè Longer response (200 tokens):\")\n",
    "    model_long = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash',\n",
    "        generation_config={'max_output_tokens': 200}\n",
    "    )\n",
    "    response_long = model_long.generate_content(prompt)\n",
    "    print(response_long.text)\n",
    "    print(f\"\\nApprox words: {len(response_long.text.split())}\")\n",
    "\n",
    "# Run the comparison\n",
    "max_tokens_control()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d949e",
   "metadata": {},
   "source": [
    "### Experiment with Token Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b6470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different token limits\n",
    "prompt = \"Write a story about a robot learning to paint\"\n",
    "\n",
    "token_limits = [30, 100, 300]\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for limit in token_limits:\n",
    "    print(f\"\\nüìä Max tokens = {limit}:\")\n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash',\n",
    "        generation_config={'max_output_tokens': limit}\n",
    "    )\n",
    "    response = model.generate_content(prompt)\n",
    "    print(response.text)\n",
    "    print(f\"\\nüìè Word count: {len(response.text.split())}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f9b61",
   "metadata": {},
   "source": [
    "## 3. Top-P Sampling (Nucleus Sampling)\n",
    "\n",
    "**top_p** controls diversity by limiting cumulative probability.\n",
    "\n",
    "- **Low (0.1 - 0.5)**: Focused, less variety\n",
    "- **Medium (0.6 - 0.8)**: Balanced\n",
    "- **High (0.9 - 1.0)**: More diverse, exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sampling():\n",
    "    \"\"\"Compare responses with different top_p values\"\"\"\n",
    "    prompt = \"Tell me an interesting fact\"\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Focused\n",
    "    print(\"\\nüéØ Top-p = 0.5 (Focused):\")\n",
    "    model_focused = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash',\n",
    "        generation_config={'top_p': 0.5}\n",
    "    )\n",
    "    response_focused = model_focused.generate_content(prompt)\n",
    "    print(response_focused.text)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    \n",
    "    # Diverse\n",
    "    print(\"\\nüåà Top-p = 0.95 (Diverse):\")\n",
    "    model_diverse = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash',\n",
    "        generation_config={'top_p': 0.95}\n",
    "    )\n",
    "    response_diverse = model_diverse.generate_content(prompt)\n",
    "    print(response_diverse.text)\n",
    "\n",
    "# Run the comparison\n",
    "top_p_sampling()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c841c1f2",
   "metadata": {},
   "source": [
    "## 4. Top-K Sampling\n",
    "\n",
    "**top_k** limits to the K most likely next tokens.\n",
    "\n",
    "- Lower values = more focused\n",
    "- Higher values = more diverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d352ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different top_k values\n",
    "prompt = \"Describe a futuristic city\"\n",
    "\n",
    "top_k_values = [10, 40, 100]\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for k in top_k_values:\n",
    "    print(f\"\\nüî¢ Top-k = {k}:\")\n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash',\n",
    "        generation_config={'top_k': k}\n",
    "    )\n",
    "    response = model.generate_content(prompt)\n",
    "    print(response.text)\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acabab7e",
   "metadata": {},
   "source": [
    "## 5. Combined Settings\n",
    "\n",
    "Use multiple parameters together for fine-tuned control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_settings():\n",
    "    \"\"\"Use multiple configuration parameters together\"\"\"\n",
    "    \n",
    "    # Define combined configuration\n",
    "    config = {\n",
    "        'temperature': 0.7,        # Moderate creativity\n",
    "        'max_output_tokens': 100,  # Limit length\n",
    "        'top_p': 0.9,             # Good diversity\n",
    "        'top_k': 40               # Reasonable variety\n",
    "    }\n",
    "    \n",
    "    print(\"‚öôÔ∏è Combined Configuration:\")\n",
    "    print(\"=\"*60)\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash', \n",
    "        generation_config=config\n",
    "    )\n",
    "    \n",
    "    prompt = \"Write a creative tagline for an AI company\"\n",
    "    print(f\"\\nPrompt: {prompt}\\n\")\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    print(f\"ü§ñ Response:\\n{response.text}\")\n",
    "\n",
    "# Run with combined settings\n",
    "combined_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97d890",
   "metadata": {},
   "source": [
    "## 6. Preset Configurations for Common Tasks\n",
    "\n",
    "Pre-configured settings for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa705e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define presets\n",
    "presets = {\n",
    "    \"factual\": {\n",
    "        \"name\": \"Factual/Precise\",\n",
    "        \"config\": {'temperature': 0.1, 'top_p': 0.5},\n",
    "        \"use_case\": \"Technical docs, fact-checking, coding\"\n",
    "    },\n",
    "    \"balanced\": {\n",
    "        \"name\": \"Balanced\",\n",
    "        \"config\": {'temperature': 0.7, 'top_p': 0.8},\n",
    "        \"use_case\": \"General chat, Q&A, explanations\"\n",
    "    },\n",
    "    \"creative\": {\n",
    "        \"name\": \"Creative\",\n",
    "        \"config\": {'temperature': 1.2, 'top_p': 0.95},\n",
    "        \"use_case\": \"Stories, poems, brainstorming\"\n",
    "    }\n",
    "}\n",
    "\n",
    "prompt = \"Describe artificial intelligence\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for preset_name, preset in presets.items():\n",
    "    print(f\"\\nüé® {preset['name']} Mode\")\n",
    "    print(f\"   Use case: {preset['use_case']}\")\n",
    "    print(f\"   Config: {preset['config']}\")\n",
    "    print()\n",
    "    \n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash',\n",
    "        generation_config=preset['config']\n",
    "    )\n",
    "    response = model.generate_content(prompt)\n",
    "    print(response.text)\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48771b36",
   "metadata": {},
   "source": [
    "## 7. Interactive Configuration Builder\n",
    "\n",
    "Build your own custom configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ff529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize these values\n",
    "my_config = {\n",
    "    'temperature': 0.8,        # Change me: 0.0 - 2.0\n",
    "    'max_output_tokens': 150,  # Change me: 1 - 8192\n",
    "    'top_p': 0.9,             # Change me: 0.0 - 1.0\n",
    "    'top_k': 40               # Change me: 1 - 100+\n",
    "}\n",
    "\n",
    "my_prompt = \"Write a short poem about technology\"  # Change me\n",
    "\n",
    "print(\"üîß Your Custom Configuration:\")\n",
    "print(\"=\"*60)\n",
    "for key, value in my_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-2.0-flash',\n",
    "    generation_config=my_config\n",
    ")\n",
    "\n",
    "print(f\"\\nPrompt: {my_prompt}\\n\")\n",
    "response = model.generate_content(my_prompt)\n",
    "print(f\"ü§ñ Response:\\n{response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf602f",
   "metadata": {},
   "source": [
    "## 8. Compare All Parameters Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d432b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare extreme settings\n",
    "configs = {\n",
    "    \"Conservative\": {'temperature': 0.2, 'top_p': 0.4, 'top_k': 10},\n",
    "    \"Moderate\": {'temperature': 0.7, 'top_p': 0.8, 'top_k': 40},\n",
    "    \"Adventurous\": {'temperature': 1.5, 'top_p': 0.95, 'top_k': 100}\n",
    "}\n",
    "\n",
    "prompt = \"The best thing about the future is\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, config in configs.items():\n",
    "    print(f\"\\nüéØ {name} Settings:\")\n",
    "    print(f\"   {config}\\n\")\n",
    "    \n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.0-flash',\n",
    "        generation_config=config\n",
    "    )\n",
    "    response = model.generate_content(prompt)\n",
    "    print(response.text)\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f76a92",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "You've learned how to:\n",
    "- ‚úÖ Control randomness with **temperature**\n",
    "- ‚úÖ Limit response length with **max_output_tokens**\n",
    "- ‚úÖ Control diversity with **top_p** (nucleus sampling)\n",
    "- ‚úÖ Limit vocabulary with **top_k**\n",
    "- ‚úÖ Combine multiple parameters\n",
    "- ‚úÖ Create preset configurations\n",
    "- ‚úÖ Choose settings for different use cases\n",
    "\n",
    "## üí° Parameter Guide:\n",
    "\n",
    "### Temperature\n",
    "```python\n",
    "temperature: 0.0 - 2.0\n",
    "\n",
    "0.0-0.3: Deterministic (coding, facts)\n",
    "0.4-0.7: Balanced (general chat)\n",
    "0.8-2.0: Creative (stories, brainstorming)\n",
    "```\n",
    "\n",
    "### Max Output Tokens\n",
    "```python\n",
    "max_output_tokens: 1 - 8192\n",
    "\n",
    "50-100: Short answers\n",
    "200-500: Paragraphs\n",
    "1000+: Long-form content\n",
    "```\n",
    "\n",
    "### Top-P (Nucleus Sampling)\n",
    "```python\n",
    "top_p: 0.0 - 1.0\n",
    "\n",
    "0.1-0.5: Focused\n",
    "0.6-0.8: Balanced\n",
    "0.9-1.0: Diverse\n",
    "```\n",
    "\n",
    "### Top-K\n",
    "```python\n",
    "top_k: 1 - 100+\n",
    "\n",
    "1-20: Very focused\n",
    "21-60: Balanced\n",
    "61+: Very diverse\n",
    "```\n",
    "\n",
    "## üéØ Use Case Recommendations:\n",
    "\n",
    "### Coding/Technical (Factual)\n",
    "```python\n",
    "{\n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.5,\n",
    "    'top_k': 20\n",
    "}\n",
    "```\n",
    "\n",
    "### Customer Support (Balanced)\n",
    "```python\n",
    "{\n",
    "    'temperature': 0.6,\n",
    "    'top_p': 0.8,\n",
    "    'top_k': 40\n",
    "}\n",
    "```\n",
    "\n",
    "### Creative Writing (Exploratory)\n",
    "```python\n",
    "{\n",
    "    'temperature': 1.2,\n",
    "    'top_p': 0.95,\n",
    "    'top_k': 80\n",
    "}\n",
    "```\n",
    "\n",
    "### Summarization (Concise)\n",
    "```python\n",
    "{\n",
    "    'temperature': 0.3,\n",
    "    'max_output_tokens': 100,\n",
    "    'top_p': 0.6\n",
    "}\n",
    "```\n",
    "\n",
    "## ‚öôÔ∏è Configuration Syntax:\n",
    "\n",
    "```python\n",
    "# Method 1: Inline\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-2.0-flash',\n",
    "    generation_config={'temperature': 0.7}\n",
    ")\n",
    "\n",
    "# Method 2: Separate config\n",
    "config = {\n",
    "    'temperature': 0.7,\n",
    "    'max_output_tokens': 200,\n",
    "    'top_p': 0.9\n",
    "}\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-2.0-flash',\n",
    "    generation_config=config\n",
    ")\n",
    "```\n",
    "\n",
    "## üöÄ Best Practices:\n",
    "\n",
    "1. **Start with defaults** - Only adjust if needed\n",
    "2. **Test iteratively** - Change one parameter at a time\n",
    "3. **Match task to settings** - Factual vs creative needs\n",
    "4. **Document your configs** - Save what works\n",
    "5. **Consider trade-offs** - Creativity vs consistency\n",
    "\n",
    "## Next Steps:\n",
    "1. Experiment with different combinations\n",
    "2. Create presets for your use cases\n",
    "3. Move on to lesson 08 (System Instructions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
