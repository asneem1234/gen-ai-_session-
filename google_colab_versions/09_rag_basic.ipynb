{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57d3118",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-generativeai python-dotenv numpy -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575e007",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f3cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea6683",
   "metadata": {},
   "source": [
    "## Step 3: Configure API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba3e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Direct input\n",
    "GOOGLE_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "print(\"‚úÖ API configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3369f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Colab Secrets (uncomment to use)\n",
    "# from google.colab import userdata\n",
    "# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "# genai.configure(api_key=GOOGLE_API_KEY)\n",
    "# print(\"‚úÖ API configured successfully using Colab Secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1923d5",
   "metadata": {},
   "source": [
    "## üìö What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that:\n",
    "1. **Retrieves** relevant information from a knowledge base\n",
    "2. **Augments** the AI prompt with that information\n",
    "3. **Generates** a response based on the retrieved context\n",
    "\n",
    "### Why RAG?\n",
    "- ‚úÖ Provides up-to-date information\n",
    "- ‚úÖ Reduces hallucinations\n",
    "- ‚úÖ Grounds responses in facts\n",
    "- ‚úÖ Works with private/custom data\n",
    "- ‚úÖ More accurate domain-specific answers\n",
    "\n",
    "### Basic RAG Pipeline:\n",
    "```\n",
    "Query ‚Üí Retrieve Relevant Docs ‚Üí Combine with Query ‚Üí AI Generation ‚Üí Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1353c",
   "metadata": {},
   "source": [
    "## 1. Create Knowledge Base\n",
    "\n",
    "Start with a simple dictionary-based knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99808ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_knowledge_base() -> Dict[str, str]:\n",
    "    \"\"\"Create a simple knowledge base\"\"\"\n",
    "    return {\n",
    "        \"Python Basics\": \"Python is a high-level programming language known for its simplicity and readability. It was created by Guido van Rossum in 1991.\",\n",
    "        \"Machine Learning\": \"ML is a subset of AI that enables systems to learn from data without explicit programming. It uses algorithms to find patterns.\",\n",
    "        \"Neural Networks\": \"Neural networks are computing systems inspired by biological neural networks in animal brains. They consist of interconnected nodes (neurons).\",\n",
    "        \"Deep Learning\": \"Deep learning uses neural networks with multiple layers to learn from large amounts of data. It excels at image, speech, and text processing.\",\n",
    "        \"Natural Language Processing\": \"NLP is a field of AI focused on enabling computers to understand, interpret, and generate human language.\",\n",
    "        \"Computer Vision\": \"Computer vision enables computers to derive meaningful information from digital images and videos. It's used in facial recognition, self-driving cars.\"\n",
    "    }\n",
    "\n",
    "# Create the knowledge base\n",
    "kb = create_knowledge_base()\n",
    "\n",
    "print(\"üìö Knowledge Base Created:\")\n",
    "print(\"=\"*60)\n",
    "for i, (topic, content) in enumerate(kb.items(), 1):\n",
    "    print(f\"\\n{i}. {topic}:\")\n",
    "    print(f\"   {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2372e9ea",
   "metadata": {},
   "source": [
    "## 2. Simple Text Embeddings\n",
    "\n",
    "Convert text to numerical vectors for similarity comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c96e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_embedding(text: str, seed: int = None) -> np.ndarray:\n",
    "    \"\"\"Create a simple embedding (for demo purposes)\"\"\"\n",
    "    # In production, use proper embeddings like sentence-transformers\n",
    "    # This is just for demonstration\n",
    "    if seed is not None:\n",
    "        np.random.seed(hash(text) % 10000)\n",
    "    return np.random.rand(10)\n",
    "\n",
    "# Demo: Create embeddings\n",
    "text1 = \"What is Python?\"\n",
    "text2 = \"Tell me about machine learning\"\n",
    "\n",
    "emb1 = simple_embedding(text1, seed=42)\n",
    "emb2 = simple_embedding(text2, seed=42)\n",
    "\n",
    "print(\"üî¢ Text Embeddings Demo:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nText 1: {text1}\")\n",
    "print(f\"Embedding: {emb1[:5]}... (showing first 5 dimensions)\")\n",
    "print(f\"\\nText 2: {text2}\")\n",
    "print(f\"Embedding: {emb2[:5]}... (showing first 5 dimensions)\")\n",
    "print(f\"\\nEmbedding shape: {emb1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac9871",
   "metadata": {},
   "source": [
    "### ‚ö†Ô∏è Note on Embeddings\n",
    "\n",
    "This demo uses **random embeddings** for simplicity. In production:\n",
    "- Use **Google's Embedding API** (`models/embedding-001`)\n",
    "- Or use **sentence-transformers** models\n",
    "- Or use **OpenAI embeddings**\n",
    "\n",
    "Real embeddings capture semantic meaning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a6ee3",
   "metadata": {},
   "source": [
    "## 3. Similarity Search\n",
    "\n",
    "Find the most relevant document for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:\n",
    "    \"\"\"Calculate cosine similarity between embeddings\"\"\"\n",
    "    return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "\n",
    "def find_similar(query: str, knowledge_base: Dict[str, str], top_k: int = 1) -> List[Tuple[str, str, float]]:\n",
    "    \"\"\"Find most similar documents to query\"\"\"\n",
    "    query_emb = simple_embedding(query, seed=42)\n",
    "    \n",
    "    similarities = []\n",
    "    for topic, content in knowledge_base.items():\n",
    "        content_emb = simple_embedding(content, seed=42)\n",
    "        similarity = calculate_similarity(query_emb, content_emb)\n",
    "        similarities.append((topic, content, similarity))\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    similarities.sort(key=lambda x: x[2], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Demo: Find similar documents\n",
    "query = \"What is Python?\"\n",
    "\n",
    "print(f\"üîç Similarity Search Demo:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nQuery: {query}\\n\")\n",
    "\n",
    "results = find_similar(query, kb, top_k=3)\n",
    "\n",
    "print(\"Top 3 Most Relevant Documents:\")\n",
    "for i, (topic, content, score) in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {topic} (similarity: {score:.4f})\")\n",
    "    print(f\"   {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead27b5c",
   "metadata": {},
   "source": [
    "## 4. Basic RAG Pipeline\n",
    "\n",
    "Combine retrieval with generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9672b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query: str, knowledge_base: Dict[str, str], show_context: bool = True) -> str:\n",
    "    \"\"\"Execute RAG query\"\"\"\n",
    "    # Step 1: Retrieve relevant context\n",
    "    results = find_similar(query, knowledge_base, top_k=1)\n",
    "    topic, relevant_context, score = results[0]\n",
    "    \n",
    "    if show_context:\n",
    "        print(f\"üìÑ Retrieved Context:\")\n",
    "        print(f\"   Topic: {topic}\")\n",
    "        print(f\"   Similarity: {score:.4f}\")\n",
    "        print(f\"   Content: {relevant_context}\")\n",
    "        print()\n",
    "    \n",
    "    # Step 2: Create prompt with context\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    prompt = f\"\"\"Context: {relevant_context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the context provided:\"\"\"\n",
    "    \n",
    "    # Step 3: Generate response\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Demo: RAG query\n",
    "query = \"What is Python?\"\n",
    "\n",
    "print(f\"ü§ñ RAG Query Demo:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚ùì Query: {query}\\n\")\n",
    "\n",
    "answer = rag_query(query, kb)\n",
    "print(f\"‚úÖ Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41fdb4",
   "metadata": {},
   "source": [
    "## 5. Multiple RAG Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3ca881",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What is Python?\",\n",
    "    \"Tell me about machine learning\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"Explain deep learning\"\n",
    "]\n",
    "\n",
    "print(\"üîÑ Multiple RAG Queries:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query {i}/{len(queries)}: {query}\")\n",
    "    print('='*60)\n",
    "    answer = rag_query(query, kb, show_context=True)\n",
    "    print(f\"\\n‚úÖ Answer:\\n{answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d60cd5",
   "metadata": {},
   "source": [
    "## 6. RAG vs Non-RAG Comparison\n",
    "\n",
    "See the difference between RAG and regular generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rag_vs_non_rag(query: str, knowledge_base: Dict[str, str]):\n",
    "    \"\"\"Compare RAG vs non-RAG responses\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    \n",
    "    print(f\"‚ùì Query: {query}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Non-RAG response\n",
    "    print(\"\\nüîµ WITHOUT RAG (No Context):\")\n",
    "    print(\"-\"*60)\n",
    "    response_no_rag = model.generate_content(query)\n",
    "    print(response_no_rag.text)\n",
    "    \n",
    "    # RAG response\n",
    "    print(\"\\nüü¢ WITH RAG (Context-Based):\")\n",
    "    print(\"-\"*60)\n",
    "    results = find_similar(query, knowledge_base, top_k=1)\n",
    "    topic, context, score = results[0]\n",
    "    print(f\"Retrieved: {topic} (score: {score:.4f})\\n\")\n",
    "    \n",
    "    prompt_rag = f\"\"\"Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the context provided:\"\"\"\n",
    "    response_rag = model.generate_content(prompt_rag)\n",
    "    print(response_rag.text)\n",
    "\n",
    "# Run comparison\n",
    "print(\"‚öñÔ∏è RAG vs Non-RAG Comparison\\n\")\n",
    "compare_rag_vs_non_rag(\"What is Python?\", kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d22d8",
   "metadata": {},
   "source": [
    "## 7. Multi-Document Retrieval\n",
    "\n",
    "Retrieve multiple relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb7875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_multi_doc(query: str, knowledge_base: Dict[str, str], top_k: int = 2) -> str:\n",
    "    \"\"\"RAG with multiple documents\"\"\"\n",
    "    # Retrieve top K documents\n",
    "    results = find_similar(query, knowledge_base, top_k=top_k)\n",
    "    \n",
    "    print(f\"üìö Retrieved {top_k} Documents:\")\n",
    "    contexts = []\n",
    "    for i, (topic, content, score) in enumerate(results, 1):\n",
    "        print(f\"\\n{i}. {topic} (score: {score:.4f})\")\n",
    "        print(f\"   {content}\")\n",
    "        contexts.append(f\"[{topic}]: {content}\")\n",
    "    \n",
    "    # Combine contexts\n",
    "    combined_context = \"\\n\\n\".join(contexts)\n",
    "    \n",
    "    # Generate response\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    prompt = f\"\"\"Context from multiple sources:\n",
    "{combined_context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the contexts provided:\"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Demo\n",
    "query = \"How does AI learn?\"\n",
    "\n",
    "print(f\"üîç Multi-Document RAG Demo:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚ùì Query: {query}\\n\")\n",
    "\n",
    "answer = rag_multi_doc(query, kb, top_k=3)\n",
    "print(f\"\\n‚úÖ Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b36c1c",
   "metadata": {},
   "source": [
    "## 8. Custom Knowledge Base\n",
    "\n",
    "Create your own knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f47e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom knowledge base about your company/product\n",
    "my_knowledge_base = {\n",
    "    \"Product Overview\": \"Our AI chatbot helps businesses automate customer support. It integrates with existing systems and handles 80% of common queries.\",\n",
    "    \"Pricing\": \"We offer three tiers: Starter ($99/mo), Professional ($299/mo), and Enterprise (custom pricing). All include 24/7 support.\",\n",
    "    \"Integration\": \"Our platform integrates with Slack, Discord, Teams, and custom APIs. Setup takes less than 30 minutes with our guided onboarding.\",\n",
    "    \"Security\": \"All data is encrypted at rest and in transit. We're SOC 2 Type II certified and GDPR compliant. Data is never used for training.\",\n",
    "    \"Support\": \"Free tier includes email support. Paid tiers get priority support, dedicated account managers, and custom training sessions.\"\n",
    "}\n",
    "\n",
    "print(\"üè¢ Custom Knowledge Base:\")\n",
    "print(\"=\"*60)\n",
    "for topic in my_knowledge_base.keys():\n",
    "    print(f\"  ‚Ä¢ {topic}\")\n",
    "\n",
    "# Test queries\n",
    "customer_queries = [\n",
    "    \"How much does it cost?\",\n",
    "    \"Can you integrate with Slack?\",\n",
    "    \"Is my data secure?\"\n",
    "]\n",
    "\n",
    "print(\"\\nüí¨ Customer Support Bot Demo:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query in customer_queries:\n",
    "    print(f\"\\nüë§ Customer: {query}\")\n",
    "    answer = rag_query(query, my_knowledge_base, show_context=False)\n",
    "    print(f\"ü§ñ Bot: {answer}\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261e4c0",
   "metadata": {},
   "source": [
    "## 9. Document Analysis\n",
    "\n",
    "Analyze what's in your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53482116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_knowledge_base(kb: Dict[str, str]):\n",
    "    \"\"\"Analyze knowledge base statistics\"\"\"\n",
    "    print(\"üìä Knowledge Base Analysis:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüìö Total Documents: {len(kb)}\")\n",
    "    \n",
    "    # Word counts\n",
    "    word_counts = {topic: len(content.split()) for topic, content in kb.items()}\n",
    "    avg_words = sum(word_counts.values()) / len(word_counts)\n",
    "    \n",
    "    print(f\"\\nüìù Average Words per Document: {avg_words:.1f}\")\n",
    "    print(\"\\nDocument Lengths:\")\n",
    "    for topic, count in sorted(word_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  ‚Ä¢ {topic}: {count} words\")\n",
    "    \n",
    "    # Character counts\n",
    "    total_chars = sum(len(content) for content in kb.values())\n",
    "    print(f\"\\nüìè Total Characters: {total_chars:,}\")\n",
    "    \n",
    "    return {\n",
    "        'total_docs': len(kb),\n",
    "        'avg_words': avg_words,\n",
    "        'total_chars': total_chars\n",
    "    }\n",
    "\n",
    "# Analyze both knowledge bases\n",
    "analyze_knowledge_base(kb)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "analyze_knowledge_base(my_knowledge_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b88d26e",
   "metadata": {},
   "source": [
    "## 10. Interactive RAG Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ec7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chat_session(knowledge_base: Dict[str, str], queries: List[str]):\n",
    "    \"\"\"Interactive RAG chat with history\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    chat = model.start_chat(history=[])\n",
    "    \n",
    "    print(\"üí¨ RAG Chat Session:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Knowledge Base: {len(knowledge_base)} documents loaded\\n\")\n",
    "    \n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n[Turn {i}]\")\n",
    "        print(f\"üë§ You: {query}\")\n",
    "        \n",
    "        # Retrieve context\n",
    "        results = find_similar(query, knowledge_base, top_k=1)\n",
    "        topic, context, score = results[0]\n",
    "        \n",
    "        # Add context to query\n",
    "        context_query = f\"\"\"[Retrieved Context: {context}]\n",
    "\n",
    "User question: {query}\n",
    "\n",
    "Answer based on the context:\"\"\"\n",
    "        \n",
    "        response = chat.send_message(context_query)\n",
    "        print(f\"ü§ñ Assistant: {response.text}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "# Demo chat\n",
    "chat_queries = [\n",
    "    \"What is Python?\",\n",
    "    \"Can you give me an example?\",\n",
    "    \"What makes it different from other languages?\"\n",
    "]\n",
    "\n",
    "rag_chat_session(kb, chat_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd9389",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "You've learned how to:\n",
    "- ‚úÖ Create a knowledge base\n",
    "- ‚úÖ Generate text embeddings\n",
    "- ‚úÖ Perform similarity search\n",
    "- ‚úÖ Build a basic RAG pipeline\n",
    "- ‚úÖ Compare RAG vs non-RAG responses\n",
    "- ‚úÖ Retrieve multiple documents\n",
    "- ‚úÖ Create custom knowledge bases\n",
    "- ‚úÖ Analyze document collections\n",
    "- ‚úÖ Build RAG chat sessions\n",
    "\n",
    "## üí° Key Concepts:\n",
    "\n",
    "### RAG Pipeline:\n",
    "```\n",
    "1. User Query\n",
    "   ‚Üì\n",
    "2. Embed Query\n",
    "   ‚Üì\n",
    "3. Search Knowledge Base (similarity)\n",
    "   ‚Üì\n",
    "4. Retrieve Top K Documents\n",
    "   ‚Üì\n",
    "5. Combine Query + Context\n",
    "   ‚Üì\n",
    "6. Generate Response with AI\n",
    "   ‚Üì\n",
    "7. Return Answer\n",
    "```\n",
    "\n",
    "### Embeddings:\n",
    "- Convert text to numerical vectors\n",
    "- Capture semantic meaning\n",
    "- Enable similarity comparison\n",
    "- Essential for retrieval\n",
    "\n",
    "### Similarity Search:\n",
    "- Compare query embedding with document embeddings\n",
    "- Use cosine similarity\n",
    "- Return top K most similar\n",
    "- Fast with proper indexing\n",
    "\n",
    "## üìã RAG Template:\n",
    "\n",
    "```python\n",
    "# 1. Create knowledge base\n",
    "kb = {...}\n",
    "\n",
    "# 2. Find relevant context\n",
    "context = find_similar(query, kb)\n",
    "\n",
    "# 3. Create prompt with context\n",
    "prompt = f\"\"\"Context: {context}\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "# 4. Generate response\n",
    "response = model.generate_content(prompt)\n",
    "```\n",
    "\n",
    "## üéØ Best Practices:\n",
    "\n",
    "1. **Chunking**: Break large documents into smaller chunks\n",
    "2. **Quality Data**: Ensure knowledge base is accurate\n",
    "3. **Proper Embeddings**: Use production-grade models\n",
    "4. **Top K**: Retrieve 1-3 most relevant docs\n",
    "5. **Context Length**: Stay within model limits\n",
    "6. **Citations**: Track which docs were used\n",
    "7. **Refresh**: Update knowledge base regularly\n",
    "8. **Fallback**: Handle no-match scenarios\n",
    "\n",
    "## ‚ö†Ô∏è Limitations of This Demo:\n",
    "\n",
    "This demo uses **random embeddings** for simplicity. For production:\n",
    "\n",
    "### Use Real Embeddings:\n",
    "```python\n",
    "# Google's Embedding API\n",
    "result = genai.embed_content(\n",
    "    model=\"models/embedding-001\",\n",
    "    content=text\n",
    ")\n",
    "embedding = result['embedding']\n",
    "```\n",
    "\n",
    "### Or Sentence Transformers:\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding = model.encode(text)\n",
    "```\n",
    "\n",
    "## üöÄ Real-World RAG Use Cases:\n",
    "\n",
    "1. **Customer Support**: Answer based on documentation\n",
    "2. **Internal Knowledge**: Company wikis, policies\n",
    "3. **Legal**: Search case law, contracts\n",
    "4. **Medical**: Research papers, patient data\n",
    "5. **Education**: Course materials, textbooks\n",
    "6. **Code**: Search codebases, documentation\n",
    "7. **E-commerce**: Product catalogs, reviews\n",
    "8. **News**: Search articles, archives\n",
    "\n",
    "## üìä RAG Metrics:\n",
    "\n",
    "Evaluate your RAG system:\n",
    "- **Retrieval Accuracy**: Did it find the right docs?\n",
    "- **Answer Relevance**: Is the answer on-topic?\n",
    "- **Faithfulness**: Is it based on the context?\n",
    "- **Context Utilization**: Did it use the retrieved info?\n",
    "- **Response Quality**: Is it clear and helpful?\n",
    "\n",
    "## üîÑ RAG Improvements:\n",
    "\n",
    "1. **Better Embeddings**: Use semantic models\n",
    "2. **Vector Databases**: Pinecone, Weaviate, ChromaDB\n",
    "3. **Hybrid Search**: Combine keyword + semantic\n",
    "4. **Reranking**: Score and reorder results\n",
    "5. **Query Expansion**: Rephrase for better retrieval\n",
    "6. **Metadata Filtering**: Filter by date, category\n",
    "7. **Multi-hop**: Chain multiple retrievals\n",
    "\n",
    "## Next Steps:\n",
    "1. Experiment with your own documents\n",
    "2. Try different retrieval strategies\n",
    "3. Measure RAG performance\n",
    "4. Move on to lesson 10 (RAG with Pinecone vector database)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
