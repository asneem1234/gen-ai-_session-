{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370a0021",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e63290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-generativeai python-dotenv pinecone-client sentence-transformers -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901985bc",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from typing import List, Dict, Tuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71040253",
   "metadata": {},
   "source": [
    "## Step 3: Configure API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45335d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Direct input\n",
    "GOOGLE_API_KEY = \"YOUR_GOOGLE_API_KEY_HERE\"\n",
    "# PINECONE_API_KEY = \"YOUR_PINECONE_API_KEY_HERE\"  # Optional: For real Pinecone\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "print(\"‚úÖ Google API configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094674c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using Colab Secrets (uncomment to use)\n",
    "# from google.colab import userdata\n",
    "# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
    "# PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
    "# genai.configure(api_key=GOOGLE_API_KEY)\n",
    "# print(\"‚úÖ API configured successfully using Colab Secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7625fcf",
   "metadata": {},
   "source": [
    "## üìä What is a Vector Database?\n",
    "\n",
    "A **vector database** is a specialized database for storing and searching embeddings.\n",
    "\n",
    "### Key Features:\n",
    "- ‚úÖ **Fast Similarity Search**: Find similar vectors in milliseconds\n",
    "- ‚úÖ **Scalability**: Handle millions/billions of vectors\n",
    "- ‚úÖ **Metadata Filtering**: Combine semantic + traditional search\n",
    "- ‚úÖ **Real-time Updates**: Add/delete vectors on the fly\n",
    "- ‚úÖ **Production-Ready**: Built for high-traffic applications\n",
    "\n",
    "### Popular Vector Databases:\n",
    "- **Pinecone**: Fully managed, easy to use\n",
    "- **Weaviate**: Open-source, flexible\n",
    "- **ChromaDB**: Lightweight, embeddable\n",
    "- **Qdrant**: Fast, open-source\n",
    "- **Milvus**: Scalable, open-source\n",
    "\n",
    "### Why Vector DB vs Basic Search?\n",
    "```\n",
    "Basic In-Memory Search:\n",
    "- Limited to small datasets\n",
    "- Slow with many documents\n",
    "- No persistence\n",
    "- Linear search O(n)\n",
    "\n",
    "Vector Database:\n",
    "- Millions of documents\n",
    "- Fast approximate search\n",
    "- Persistent storage\n",
    "- Sublinear search O(log n)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94cf296",
   "metadata": {},
   "source": [
    "## 1. Simple Vector Database Simulator\n",
    "\n",
    "First, let's create a simple vector DB for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorDB:\n",
    "    \"\"\"Simple in-memory vector database for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"demo-index\"):\n",
    "        self.name = name\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        print(f\"‚úÖ Created vector database: {name}\")\n",
    "    \n",
    "    def add_document(self, doc_id: str, text: str, embedding: List[float], metadata: Dict = None):\n",
    "        \"\"\"Add document with embedding\"\"\"\n",
    "        self.documents.append({\"id\": doc_id, \"text\": text})\n",
    "        self.embeddings.append(embedding)\n",
    "        self.metadata.append(metadata or {})\n",
    "        print(f\"  ‚úì Added: {doc_id}\")\n",
    "    \n",
    "    def search(self, query_embedding: List[float], top_k: int = 3, filter_metadata: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        # Calculate similarities (simplified)\n",
    "        similarities = []\n",
    "        for i, doc_emb in enumerate(self.embeddings):\n",
    "            # Apply metadata filter if specified\n",
    "            if filter_metadata:\n",
    "                match = all(self.metadata[i].get(k) == v for k, v in filter_metadata.items())\n",
    "                if not match:\n",
    "                    continue\n",
    "            \n",
    "            # Cosine similarity\n",
    "            query_arr = np.array(query_embedding)\n",
    "            doc_arr = np.array(doc_emb)\n",
    "            similarity = np.dot(query_arr, doc_arr) / (np.linalg.norm(query_arr) * np.linalg.norm(doc_arr))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top K\n",
    "        results = []\n",
    "        for i, score in similarities[:top_k]:\n",
    "            results.append({\n",
    "                \"id\": self.documents[i][\"id\"],\n",
    "                \"text\": self.documents[i][\"text\"],\n",
    "                \"score\": float(score),\n",
    "                \"metadata\": self.metadata[i]\n",
    "            })\n",
    "        return results\n",
    "    \n",
    "    def stats(self):\n",
    "        \"\"\"Get database statistics\"\"\"\n",
    "        print(f\"\\nüìä Database Stats:\")\n",
    "        print(f\"  Name: {self.name}\")\n",
    "        print(f\"  Documents: {len(self.documents)}\")\n",
    "        print(f\"  Embedding dimension: {len(self.embeddings[0]) if self.embeddings else 0}\")\n",
    "\n",
    "# Create database\n",
    "db = SimpleVectorDB(\"tech-docs\")\n",
    "print(\"\\nüéâ Vector database ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d8d27",
   "metadata": {},
   "source": [
    "## 2. Generate Real Embeddings\n",
    "\n",
    "Use Google's embedding model for semantic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding using Google's API\"\"\"\n",
    "    result = genai.embed_content(\n",
    "        model=\"models/embedding-001\",\n",
    "        content=text,\n",
    "        task_type=\"retrieval_document\"\n",
    "    )\n",
    "    return result['embedding']\n",
    "\n",
    "def get_query_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Get embedding for search query\"\"\"\n",
    "    result = genai.embed_content(\n",
    "        model=\"models/embedding-001\",\n",
    "        content=text,\n",
    "        task_type=\"retrieval_query\"\n",
    "    )\n",
    "    return result['embedding']\n",
    "\n",
    "# Test embeddings\n",
    "print(\"üî¢ Testing Google Embeddings:\\n\")\n",
    "test_text = \"What is machine learning?\"\n",
    "embedding = get_embedding(test_text)\n",
    "\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"First 5 values: {embedding[:5]}\")\n",
    "print(\"\\n‚úÖ Embeddings working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f01885c",
   "metadata": {},
   "source": [
    "## 3. Create Knowledge Base with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58cf118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store():\n",
    "    \"\"\"Create vector store with documents\"\"\"\n",
    "    \n",
    "    documents = [\n",
    "        {\n",
    "            \"id\": \"doc1\",\n",
    "            \"text\": \"Python is a versatile programming language used for web development, data science, and automation. It's known for its simplicity and readability.\",\n",
    "            \"metadata\": {\"category\": \"programming\", \"language\": \"python\", \"level\": \"beginner\"}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc2\",\n",
    "            \"text\": \"Machine learning algorithms can learn patterns from data to make predictions. They improve automatically through experience without explicit programming.\",\n",
    "            \"metadata\": {\"category\": \"ai\", \"topic\": \"machine-learning\", \"level\": \"intermediate\"}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc3\",\n",
    "            \"text\": \"Neural networks consist of layers of interconnected nodes that process information. They're inspired by biological neural networks in animal brains.\",\n",
    "            \"metadata\": {\"category\": \"ai\", \"topic\": \"neural-networks\", \"level\": \"advanced\"}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc4\",\n",
    "            \"text\": \"Deep learning models require large amounts of data and computational power. They use neural networks with multiple hidden layers to learn complex patterns.\",\n",
    "            \"metadata\": {\"category\": \"ai\", \"topic\": \"deep-learning\", \"level\": \"advanced\"}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc5\",\n",
    "            \"text\": \"Natural language processing enables computers to understand human language. NLP powers chatbots, translation, and sentiment analysis.\",\n",
    "            \"metadata\": {\"category\": \"ai\", \"topic\": \"nlp\", \"level\": \"intermediate\"}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc6\",\n",
    "            \"text\": \"JavaScript is the programming language of the web. It runs in browsers and enables interactive web pages and dynamic user interfaces.\",\n",
    "            \"metadata\": {\"category\": \"programming\", \"language\": \"javascript\", \"level\": \"beginner\"}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"doc7\",\n",
    "            \"text\": \"Data visualization helps communicate insights from data. Tools like matplotlib, seaborn, and plotly create charts and graphs.\",\n",
    "            \"metadata\": {\"category\": \"data-science\", \"topic\": \"visualization\", \"level\": \"beginner\"}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"üìö Adding documents to vector database:\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Get embedding\n",
    "        embedding = get_embedding(doc[\"text\"])\n",
    "        \n",
    "        # Add to database\n",
    "        db.add_document(\n",
    "            doc_id=doc[\"id\"],\n",
    "            text=doc[\"text\"],\n",
    "            embedding=embedding,\n",
    "            metadata=doc[\"metadata\"]\n",
    "        )\n",
    "        time.sleep(0.1)  # Rate limiting\n",
    "    \n",
    "    print(\"\\n‚úÖ Vector store created!\")\n",
    "    db.stats()\n",
    "\n",
    "# Create the vector store\n",
    "create_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32fdf93",
   "metadata": {},
   "source": [
    "## 4. Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba181dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, top_k: int = 3, filter_metadata: Dict = None):\n",
    "    \"\"\"Perform semantic search\"\"\"\n",
    "    print(f\"üîç Searching for: '{query}'\")\n",
    "    if filter_metadata:\n",
    "        print(f\"üìã Filters: {filter_metadata}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_embedding = get_query_embedding(query)\n",
    "    \n",
    "    # Search\n",
    "    results = db.search(query_embedding, top_k=top_k, filter_metadata=filter_metadata)\n",
    "    \n",
    "    print(f\"\\nüìÑ Found {len(results)} results:\\n\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. [{result['id']}] (score: {result['score']:.4f})\")\n",
    "        print(f\"   {result['text']}\")\n",
    "        print(f\"   Metadata: {result['metadata']}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test semantic search\n",
    "semantic_search(\"What is Python used for?\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4935b4d7",
   "metadata": {},
   "source": [
    "## 5. RAG with Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_vector_db(query: str, top_k: int = 2, filter_metadata: Dict = None, show_context: bool = True) -> str:\n",
    "    \"\"\"RAG query using vector database\"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve\n",
    "    query_embedding = get_query_embedding(query)\n",
    "    results = db.search(query_embedding, top_k=top_k, filter_metadata=filter_metadata)\n",
    "    \n",
    "    if show_context:\n",
    "        print(\"üìö Retrieved Context:\")\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n  {i}. [{result['id']}] (score: {result['score']:.4f})\")\n",
    "            print(f\"     {result['text'][:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    # Step 2: Combine contexts\n",
    "    context = \"\\n\\n\".join([f\"[{r['id']}]: {r['text']}\" for r in results])\n",
    "    \n",
    "    # Step 3: Generate\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    prompt = f\"\"\"Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer based on the context:\"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Test RAG\n",
    "print(\"ü§ñ RAG Query Demo:\")\n",
    "print(\"=\"*60)\n",
    "query = \"What is Python used for?\"\n",
    "print(f\"\\n‚ùì Query: {query}\\n\")\n",
    "\n",
    "answer = rag_with_vector_db(query, top_k=2)\n",
    "print(f\"‚úÖ Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51db8431",
   "metadata": {},
   "source": [
    "## 6. Multiple RAG Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"How do neural networks work?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain natural language processing\",\n",
    "    \"What programming languages are popular?\"\n",
    "]\n",
    "\n",
    "print(\"üîÑ Multiple RAG Queries:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, query in enumerate(queries, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query {i}/{len(queries)}: {query}\")\n",
    "    print('='*60)\n",
    "    answer = rag_with_vector_db(query, top_k=2, show_context=False)\n",
    "    print(f\"\\n‚úÖ Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226984c",
   "metadata": {},
   "source": [
    "## 7. Metadata Filtering\n",
    "\n",
    "Filter search results by metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search only in AI category\n",
    "print(\"üéØ Filtered Search Demo:\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "query = \"How does learning work?\"\n",
    "print(f\"\\n‚ùì Query: {query}\")\n",
    "print(\"üìã Filter: category='ai'\\n\")\n",
    "\n",
    "results = semantic_search(query, top_k=3, filter_metadata={\"category\": \"ai\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d411c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search only beginner-level content\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n‚ùì Query: Tell me about programming\")\n",
    "print(\"üìã Filter: level='beginner'\\n\")\n",
    "\n",
    "results = semantic_search(\n",
    "    \"Tell me about programming\",\n",
    "    top_k=3,\n",
    "    filter_metadata={\"level\": \"beginner\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG with filtering\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nü§ñ RAG with Metadata Filter\\n\")\n",
    "\n",
    "query = \"Explain AI concepts\"\n",
    "print(f\"‚ùì Query: {query}\")\n",
    "print(\"üìã Filter: category='ai', level='advanced'\\n\")\n",
    "\n",
    "answer = rag_with_vector_db(\n",
    "    query,\n",
    "    top_k=2,\n",
    "    filter_metadata={\"category\": \"ai\", \"level\": \"advanced\"},\n",
    "    show_context=True\n",
    ")\n",
    "print(f\"\\n‚úÖ Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5a0c62",
   "metadata": {},
   "source": [
    "## 8. Semantic Search Quality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6949ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic understanding\n",
    "test_queries = [\n",
    "    (\"How do I make websites interactive?\", \"JavaScript\"),\n",
    "    (\"What helps computers understand speech?\", \"NLP\"),\n",
    "    (\"How can I visualize my data?\", \"Data visualization\"),\n",
    "    (\"What's good for beginners in coding?\", \"Python or JavaScript\")\n",
    "]\n",
    "\n",
    "print(\"üß™ Semantic Understanding Test:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for query, expected in test_queries:\n",
    "    print(f\"\\n‚ùì Query: {query}\")\n",
    "    print(f\"üéØ Expected: {expected}\")\n",
    "    \n",
    "    query_embedding = get_query_embedding(query)\n",
    "    results = db.search(query_embedding, top_k=1)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"‚úÖ Found: [{results[0]['id']}] (score: {results[0]['score']:.4f})\")\n",
    "        print(f\"   {results[0]['text'][:80]}...\")\n",
    "    print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f3392",
   "metadata": {},
   "source": [
    "## 9. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5049db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_search(num_queries: int = 5):\n",
    "    \"\"\"Benchmark search performance\"\"\"\n",
    "    test_queries = [\n",
    "        \"What is Python?\",\n",
    "        \"How does machine learning work?\",\n",
    "        \"Explain neural networks\",\n",
    "        \"What is data visualization?\",\n",
    "        \"Tell me about web development\"\n",
    "    ]\n",
    "    \n",
    "    print(\"‚ö° Performance Benchmark:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    total_time = 0\n",
    "    for i, query in enumerate(test_queries[:num_queries], 1):\n",
    "        start = time.time()\n",
    "        query_embedding = get_query_embedding(query)\n",
    "        results = db.search(query_embedding, top_k=3)\n",
    "        elapsed = time.time() - start\n",
    "        total_time += elapsed\n",
    "        \n",
    "        print(f\"\\nQuery {i}: {elapsed*1000:.2f}ms\")\n",
    "        print(f\"  Found {len(results)} results\")\n",
    "    \n",
    "    avg_time = (total_time / num_queries) * 1000\n",
    "    print(f\"\\nüìä Average search time: {avg_time:.2f}ms\")\n",
    "    print(f\"üìä Total time: {total_time*1000:.2f}ms\")\n",
    "\n",
    "benchmark_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cc9fe",
   "metadata": {},
   "source": [
    "## 10. Complete RAG Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chatbot(queries: List[str], top_k: int = 2):\n",
    "    \"\"\"Interactive RAG chatbot\"\"\"\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "    chat = model.start_chat(history=[])\n",
    "    \n",
    "    print(\"ü§ñ RAG Chatbot Session:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Vector Database: {db.name}\")\n",
    "    print(f\"Documents loaded: {len(db.documents)}\\n\")\n",
    "    \n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n[Turn {i}]\")\n",
    "        print(f\"üë§ You: {query}\")\n",
    "        \n",
    "        # Retrieve context\n",
    "        query_embedding = get_query_embedding(query)\n",
    "        results = db.search(query_embedding, top_k=top_k)\n",
    "        context = \"\\n\".join([r['text'] for r in results])\n",
    "        \n",
    "        # Create contextualized query\n",
    "        context_query = f\"\"\"[Context from knowledge base: {context}]\n",
    "\n",
    "User question: {query}\n",
    "\n",
    "Answer based on the context:\"\"\"\n",
    "        \n",
    "        response = chat.send_message(context_query)\n",
    "        print(f\"ü§ñ Assistant: {response.text}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "# Demo chatbot\n",
    "chat_queries = [\n",
    "    \"What is Python?\",\n",
    "    \"Can you give me more details?\",\n",
    "    \"What about machine learning?\",\n",
    "    \"How are they related?\"\n",
    "]\n",
    "\n",
    "rag_chatbot(chat_queries, top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e7553",
   "metadata": {},
   "source": [
    "## 11. Real Pinecone Integration (Optional)\n",
    "\n",
    "For production use, integrate with real Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc242b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to use real Pinecone\n",
    "\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# def setup_pinecone():\n",
    "#     \"\"\"Setup real Pinecone vector database\"\"\"\n",
    "#     \n",
    "#     # Initialize Pinecone\n",
    "#     pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "#     \n",
    "#     # Create index\n",
    "#     index_name = \"tech-docs\"\n",
    "#     \n",
    "#     if index_name not in pc.list_indexes().names():\n",
    "#         pc.create_index(\n",
    "#             name=index_name,\n",
    "#             dimension=768,  # Google embedding dimension\n",
    "#             metric=\"cosine\",\n",
    "#             spec=ServerlessSpec(\n",
    "#                 cloud=\"aws\",\n",
    "#                 region=\"us-east-1\"\n",
    "#             )\n",
    "#         )\n",
    "#     \n",
    "#     # Connect to index\n",
    "#     index = pc.Index(index_name)\n",
    "#     print(f\"‚úÖ Connected to Pinecone index: {index_name}\")\n",
    "#     return index\n",
    "\n",
    "# def upsert_to_pinecone(index, documents):\n",
    "#     \"\"\"Upload documents to Pinecone\"\"\"\n",
    "#     vectors = []\n",
    "#     \n",
    "#     for doc in documents:\n",
    "#         embedding = get_embedding(doc['text'])\n",
    "#         vectors.append((\n",
    "#             doc['id'],\n",
    "#             embedding,\n",
    "#             {\"text\": doc['text'], **doc['metadata']}\n",
    "#         ))\n",
    "#     \n",
    "#     index.upsert(vectors=vectors)\n",
    "#     print(f\"‚úÖ Uploaded {len(vectors)} vectors to Pinecone\")\n",
    "\n",
    "# def search_pinecone(index, query, top_k=3):\n",
    "#     \"\"\"Search Pinecone index\"\"\"\n",
    "#     query_embedding = get_query_embedding(query)\n",
    "#     results = index.query(\n",
    "#         vector=query_embedding,\n",
    "#         top_k=top_k,\n",
    "#         include_metadata=True\n",
    "#     )\n",
    "#     return results['matches']\n",
    "\n",
    "print(\"‚ÑπÔ∏è To use real Pinecone:\")\n",
    "print(\"1. Get API key from pinecone.io\")\n",
    "print(\"2. Uncomment the code above\")\n",
    "print(\"3. Set PINECONE_API_KEY variable\")\n",
    "print(\"4. Run the setup functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba9c50",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "You've learned how to:\n",
    "- ‚úÖ Understand vector databases\n",
    "- ‚úÖ Create embeddings with Google API\n",
    "- ‚úÖ Build a simple vector database\n",
    "- ‚úÖ Perform semantic search\n",
    "- ‚úÖ Implement production RAG pipeline\n",
    "- ‚úÖ Use metadata filtering\n",
    "- ‚úÖ Build RAG chatbots\n",
    "- ‚úÖ Benchmark performance\n",
    "- ‚úÖ Integrate with real Pinecone\n",
    "\n",
    "## üí° Key Concepts:\n",
    "\n",
    "### Vector Database Architecture:\n",
    "```\n",
    "Documents ‚Üí Embeddings ‚Üí Vector Index\n",
    "                              ‚Üì\n",
    "Query ‚Üí Embedding ‚Üí Similarity Search ‚Üí Top K Results\n",
    "```\n",
    "\n",
    "### Embedding Model:\n",
    "- **Google's embedding-001**: 768 dimensions\n",
    "- **Task types**: retrieval_document, retrieval_query\n",
    "- **Captures semantic meaning**\n",
    "\n",
    "### Similarity Metrics:\n",
    "- **Cosine similarity**: Best for text\n",
    "- **Euclidean distance**: For numeric data\n",
    "- **Dot product**: Fast approximation\n",
    "\n",
    "## üìã Production RAG Template:\n",
    "\n",
    "```python\n",
    "# 1. Setup\n",
    "from pinecone import Pinecone\n",
    "pc = Pinecone(api_key=API_KEY)\n",
    "index = pc.Index(\"my-index\")\n",
    "\n",
    "# 2. Index documents\n",
    "for doc in documents:\n",
    "    embedding = get_embedding(doc['text'])\n",
    "    index.upsert([(doc['id'], embedding, doc['metadata'])])\n",
    "\n",
    "# 3. Search\n",
    "query_emb = get_query_embedding(query)\n",
    "results = index.query(vector=query_emb, top_k=3)\n",
    "\n",
    "# 4. Generate\n",
    "context = get_context_from_results(results)\n",
    "answer = model.generate_content(f\"Context: {context}\\nQ: {query}\")\n",
    "```\n",
    "\n",
    "## üéØ Best Practices:\n",
    "\n",
    "### 1. Chunking Strategy:\n",
    "- Break documents into 200-500 word chunks\n",
    "- Overlap chunks by 10-20%\n",
    "- Preserve context boundaries\n",
    "\n",
    "### 2. Metadata Design:\n",
    "```python\n",
    "metadata = {\n",
    "    \"category\": \"docs\",\n",
    "    \"source\": \"file.pdf\",\n",
    "    \"date\": \"2024-01-01\",\n",
    "    \"author\": \"name\",\n",
    "    \"page\": 5\n",
    "}\n",
    "```\n",
    "\n",
    "### 3. Search Optimization:\n",
    "- Use appropriate top_k (2-5 for most cases)\n",
    "- Filter by metadata when possible\n",
    "- Cache frequently searched queries\n",
    "- Use async for batch processing\n",
    "\n",
    "### 4. Quality Control:\n",
    "- Monitor retrieval accuracy\n",
    "- Track answer relevance\n",
    "- Log failed queries\n",
    "- A/B test retrieval strategies\n",
    "\n",
    "### 5. Cost Optimization:\n",
    "- Batch embed documents\n",
    "- Cache embeddings\n",
    "- Use appropriate index size\n",
    "- Monitor API usage\n",
    "\n",
    "## üìä Metrics to Track:\n",
    "\n",
    "### Retrieval Metrics:\n",
    "- **Precision@K**: Relevant docs in top K\n",
    "- **Recall@K**: Found relevant docs / total relevant\n",
    "- **MRR**: Mean reciprocal rank\n",
    "- **NDCG**: Normalized discounted cumulative gain\n",
    "\n",
    "### Generation Metrics:\n",
    "- **Faithfulness**: Based on retrieved context\n",
    "- **Answer relevance**: Addresses the query\n",
    "- **Context utilization**: Uses retrieved info\n",
    "- **Hallucination rate**: Made-up information\n",
    "\n",
    "### System Metrics:\n",
    "- **Latency**: End-to-end response time\n",
    "- **Throughput**: Queries per second\n",
    "- **Cost**: Per query cost\n",
    "- **Uptime**: System availability\n",
    "\n",
    "## üöÄ Advanced Techniques:\n",
    "\n",
    "### 1. Hybrid Search:\n",
    "Combine semantic + keyword search\n",
    "```python\n",
    "semantic_results = vector_search(query)\n",
    "keyword_results = keyword_search(query)\n",
    "final_results = rerank(semantic_results + keyword_results)\n",
    "```\n",
    "\n",
    "### 2. Reranking:\n",
    "Re-score results with cross-encoder\n",
    "```python\n",
    "results = initial_search(query, top_k=20)\n",
    "reranked = cross_encoder.rank(query, results)\n",
    "final = reranked[:5]\n",
    "```\n",
    "\n",
    "### 3. Query Expansion:\n",
    "Generate multiple query variants\n",
    "```python\n",
    "variants = [\n",
    "    original_query,\n",
    "    rephrase(original_query),\n",
    "    expand(original_query)\n",
    "]\n",
    "results = [search(v) for v in variants]\n",
    "combined = deduplicate(results)\n",
    "```\n",
    "\n",
    "### 4. Multi-hop Retrieval:\n",
    "Iterative retrieval for complex queries\n",
    "```python\n",
    "context1 = search(query)\n",
    "refined_query = generate_followup(query, context1)\n",
    "context2 = search(refined_query)\n",
    "final_answer = generate(query, context1 + context2)\n",
    "```\n",
    "\n",
    "## ‚ö†Ô∏è Common Pitfalls:\n",
    "\n",
    "1. **Too many results**: More isn't always better\n",
    "2. **Wrong embedding model**: Match to your domain\n",
    "3. **No metadata**: Miss filtering opportunities\n",
    "4. **Large chunks**: Lose precision\n",
    "5. **No monitoring**: Can't improve\n",
    "6. **Stale data**: Update your index\n",
    "7. **No fallback**: Handle no-result cases\n",
    "\n",
    "## üîß Troubleshooting:\n",
    "\n",
    "### Poor Retrieval Quality:\n",
    "- Try different embedding models\n",
    "- Adjust chunk size\n",
    "- Add more metadata\n",
    "- Use hybrid search\n",
    "\n",
    "### Slow Performance:\n",
    "- Reduce dimensionality\n",
    "- Use approximate search\n",
    "- Batch requests\n",
    "- Cache results\n",
    "\n",
    "### High Costs:\n",
    "- Optimize chunk size\n",
    "- Batch embeddings\n",
    "- Use cheaper tiers\n",
    "- Cache embeddings\n",
    "\n",
    "## üåü Production Checklist:\n",
    "\n",
    "- [ ] Use production embedding model\n",
    "- [ ] Set up real vector database (Pinecone, etc.)\n",
    "- [ ] Implement proper chunking\n",
    "- [ ] Add comprehensive metadata\n",
    "- [ ] Set up monitoring\n",
    "- [ ] Implement rate limiting\n",
    "- [ ] Add error handling\n",
    "- [ ] Cache frequent queries\n",
    "- [ ] Set up logging\n",
    "- [ ] Test retrieval quality\n",
    "- [ ] Document API\n",
    "- [ ] Plan for updates\n",
    "\n",
    "## üéì What's Next?\n",
    "\n",
    "Continue learning:\n",
    "1. Try with your own documents\n",
    "2. Experiment with different embedding models\n",
    "3. Build a domain-specific RAG system\n",
    "4. Implement advanced techniques (reranking, hybrid search)\n",
    "5. Deploy to production\n",
    "6. Monitor and iterate\n",
    "\n",
    "## üéä Congratulations!\n",
    "\n",
    "You've completed all 10 lessons! You now know:\n",
    "1. ‚úÖ Model preparation and API setup\n",
    "2. ‚úÖ Text chat and generation\n",
    "3. ‚úÖ Image understanding and VQA\n",
    "4. ‚úÖ Video frame analysis\n",
    "5. ‚úÖ Streaming responses\n",
    "6. ‚úÖ Conversation memory\n",
    "7. ‚úÖ Model configurations\n",
    "8. ‚úÖ System instructions and personas\n",
    "9. ‚úÖ Basic RAG\n",
    "10. ‚úÖ Production RAG with vector databases\n",
    "\n",
    "You're now ready to build production AI applications! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
